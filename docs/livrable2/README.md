# Livrable 2 : ModÃ¨le Physique et Optimisation

**Projet:** CHU Data Lakehouse
**Auteurs:** Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING
**Formation:** CESI FISA A4
**AnnÃ©e:** 2025-2026

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Scripts dÃ©veloppÃ©s](#scripts-dÃ©veloppÃ©s)
4. [Installation et dÃ©marrage](#installation-et-dÃ©marrage)
5. [ExÃ©cution des jobs](#exÃ©cution-des-jobs)
6. [Tests de performance](#tests-de-performance)
7. [RÃ©sultats attendus](#rÃ©sultats-attendus)

---

## ğŸ¯ Vue d'ensemble

Ce livrable implÃ©mente un **Data Lakehouse** complet avec architecture en 3 couches (Bronze, Silver, Gold) utilisant:
- **MinIO** (stockage objet S3-compatible)
- **Apache Spark** (traitement distribuÃ©)
- **Delta Lake** (format de stockage transactionnel)
- **PostgreSQL** (source de donnÃ©es opÃ©rationnelle)

### Objectifs du Livrable 2

âœ… Script pour la crÃ©ation et le chargement de donnÃ©es dans les tables
âœ… VÃ©rification des donnÃ©es prÃ©sentes et accÃ¨s aux donnÃ©es Ã  travers les tables
âœ… Script montrant le peuplement des tables
âœ… Script pour le partitionnement et les buckets
âœ… Graphes montrant les temps de rÃ©ponses pour Ã©valuer la performance
âœ… RequÃªtes faisant foi pour l'Ã©valuation de la performance

---

## ğŸ—ï¸ Architecture

### Architecture en 3 Couches (Medallion)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOURCES DE DONNÃ‰ES                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ PostgreSQL (100K patients, 1M+ consultations)            â”‚
â”‚  â€¢ CSV DÃ©cÃ¨s (~300K lignes)                                 â”‚
â”‚  â€¢ CSV Satisfaction (multiple annÃ©es)                       â”‚
â”‚  â€¢ CSV Ã‰tablissements de santÃ©                              â”‚
â”‚  â€¢ CSV Hospitalisations                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER (Raw Data)                                     â”‚
â”‚  ğŸ“¦ s3a://lakehouse/bronze/                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Format: Delta Lake                                        â”‚
â”‚  â€¢ DonnÃ©es brutes non transformÃ©es                           â”‚
â”‚  â€¢ Partitionnement par date d'ingestion                      â”‚
â”‚  â€¢ MÃ©tadonnÃ©es d'ingestion ajoutÃ©es                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Spark ETL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER (Cleaned Data)                                 â”‚
â”‚  ğŸ“¦ s3a://lakehouse/silver/                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Nettoyage des donnÃ©es                                     â”‚
â”‚  â€¢ Validation et typage correct                              â”‚
â”‚  â€¢ DÃ©doublonnage                                             â”‚
â”‚  â€¢ Jointures et enrichissements                              â”‚
â”‚  â€¢ Partitionnement par annÃ©e/mois                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“ Spark ETL
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER (Business Layer - Star Schema)                   â”‚
â”‚  ğŸ“¦ s3a://lakehouse/gold/                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ModÃ¨le dimensionnel (Ã©toile)                              â”‚
â”‚  â€¢ Tables de faits: Consultations, Hospitalisations, etc.    â”‚
â”‚  â€¢ Tables de dimensions: Temps, Patient, Diagnostic, etc.    â”‚
â”‚  â€¢ PARTITIONNEMENT optimisÃ©                                  â”‚
â”‚  â€¢ BUCKETING sur clÃ©s de jointure                            â”‚
â”‚  â€¢ Z-ORDERING pour performance                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Scripts DÃ©veloppÃ©s

### Structure des dossiers

```
spark/jobs/
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ spark_utils.py              # Utilitaires rÃ©utilisables
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ ingest_postgres_to_bronze.py # Ingestion PostgreSQL â†’ Bronze
â”‚   â””â”€â”€ ingest_csv_to_bronze.py      # Ingestion CSV â†’ Bronze
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ transform_to_silver.py       # Nettoyage et validation
â”‚   â””â”€â”€ enrich_silver.py             # Enrichissement
â”œâ”€â”€ gold/
â”‚   â”œâ”€â”€ create_dimensions.py         # CrÃ©ation des dimensions
â”‚   â”œâ”€â”€ create_facts.py              # CrÃ©ation des faits
â”‚   â””â”€â”€ optimize_gold.py             # Optimisations (partition, bucket)
â””â”€â”€ performance/
    â”œâ”€â”€ benchmark_queries.py         # RequÃªtes de benchmark
    â””â”€â”€ measure_performance.py       # Mesure et graphiques
```

### Scripts Principaux

#### 1. **utils/spark_utils.py**
Fonctions utilitaires pour:
- CrÃ©ation de sessions Spark configurÃ©es
- Connexion PostgreSQL
- Lecture/Ã‰criture Delta Lake
- Ajout de mÃ©tadonnÃ©es

#### 2. **bronze/ingest_postgres_to_bronze.py**
- Extrait 13 tables de PostgreSQL
- Sauvegarde en Delta Lake dans MinIO
- Partitionnement par date d'ingestion

#### 3. **bronze/ingest_csv_to_bronze.py**
- IngÃ¨re tous les CSV (dÃ©cÃ¨s, satisfaction, Ã©tablissements)
- Gestion des diffÃ©rents sÃ©parateurs (`,` et `;`)
- Encodage UTF-8

#### 4. **silver/** (Ã  venir)
- Nettoyage des donnÃ©es
- Validation des types
- DÃ©doublonnage

#### 5. **gold/** (Ã  venir)
- CrÃ©ation du modÃ¨le dimensionnel
- Partitionnement et bucketing
- Optimisations Z-ORDER

#### 6. **performance/** (Ã  venir)
- RequÃªtes de benchmark
- Mesure des temps de rÃ©ponse
- GÃ©nÃ©ration de graphiques

---

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- Docker et Docker Compose installÃ©s
- 8 GB RAM minimum
- 20 GB espace disque

### DÃ©marrage de l'infrastructure

```bash
# 1. DÃ©marrer tous les services
docker compose up -d

# 2. VÃ©rifier que tous les conteneurs sont dÃ©marrÃ©s
docker compose ps

# 3. VÃ©rifier les logs
docker logs chu_postgres
docker logs chu_minio
docker logs chu_spark_master
```

### Services disponibles

- **PostgreSQL:** localhost:5432
- **PgAdmin:** http://localhost:5050
- **MinIO Console:** http://localhost:9001 (minioadmin / minioadmin123)
- **Spark Master UI:** http://localhost:8081
- **Jupyter Lab:** http://localhost:8888 (token: admin123)
- **Airflow:** http://localhost:8080
- **Superset:** http://localhost:8088

---

## âš™ï¸ ExÃ©cution des Jobs

### MÃ©thode 1: Via Jupyter Lab

```python
# 1. Ouvrir Jupyter: http://localhost:8888 (token: admin123)
# 2. CrÃ©er un nouveau notebook
# 3. ExÃ©cuter:

!cd /opt/spark-apps && python jobs/bronze/ingest_postgres_to_bronze.py
```

### MÃ©thode 2: Via Docker Exec

```bash
# Job Bronze - PostgreSQL
docker exec chu_spark_master \
  /opt/spark/bin/spark-submit \
  --master local[*] \
  /opt/spark-apps/jobs/bronze/ingest_postgres_to_bronze.py

# Job Bronze - CSV
docker exec chu_spark_master \
  /opt/spark/bin/spark-submit \
  --master local[*] \
  /opt/spark-apps/jobs/bronze/ingest_csv_to_bronze.py
```

### MÃ©thode 3: Via Airflow DAG (recommandÃ© pour production)

Les DAGs Airflow seront crÃ©Ã©s pour orchestrer l'ensemble du pipeline.

---

## ğŸ“Š Tests de Performance

### RequÃªtes de Benchmark

Les requÃªtes suivantes seront testÃ©es:

1. **Consultations par Ã©tablissement sur pÃ©riode**
   - Filtrage par Ã©tablissement et date
   - AgrÃ©gation mensuelle

2. **Hospitalisations par diagnostic**
   - Jointure dimension diagnostic
   - Calcul de durÃ©e moyenne

3. **DÃ©cÃ¨s par rÃ©gion**
   - AgrÃ©gation gÃ©ographique
   - Filtrage temporel

4. **Satisfaction par rÃ©gion**
   - Scores moyens
   - Taux de recommandation

### MÃ©triques mesurÃ©es

- **Temps de rÃ©ponse** (ms)
- **Nombre de lignes scannÃ©es**
- **EfficacitÃ© du partitionnement**
- **Impact du bucketing**
- **Gains Z-ORDERING**

### Comparaisons

- Sans optimisation vs Avec partitionnement
- Avec partitionnement vs Avec bucketing
- Avant Z-ORDER vs AprÃ¨s Z-ORDER

---

## ğŸ“ˆ RÃ©sultats Attendus

### VolumÃ©trie

| Source | Nombre de lignes | Taille estimÃ©e |
|--------|------------------|----------------|
| Patients | 100,000 | ~10 MB |
| Consultations | 1,027,157 | ~100 MB |
| DÃ©cÃ¨s | ~300,000 | ~30 MB |
| Satisfaction | ~50,000 | ~20 MB |
| Ã‰tablissements | ~30,000 | ~5 MB |

### Objectifs de Performance

- RequÃªte simple: < 1 seconde
- RequÃªte avec jointure: < 3 secondes
- RequÃªte complexe (multiples jointures): < 10 secondes

---

## ğŸ” VÃ©rification des DonnÃ©es

### VÃ©rifier les donnÃ©es Bronze

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Verification").getOrCreate()

# Lire une table Bronze
df = spark.read.format("delta").load("s3a://lakehouse/bronze/postgres/Patient")
print(f"Nombre de patients: {df.count()}")
df.show(5)
```

### VÃ©rifier MinIO

1. Ouvrir http://localhost:9001
2. Login: minioadmin / minioadmin123
3. Naviguer dans le bucket `lakehouse`
4. VÃ©rifier la prÃ©sence des dossiers `bronze/`, `silver/`, `gold/`

---

## ğŸ“ Documentation ComplÃ©mentaire

- [ModÃ¨le Dimensionnel](./modele_dimensionnel.md)
- Architecture dÃ©taillÃ©e (Ã  venir)
- Guide des optimisations (Ã  venir)
- Rapport de performance (Ã  venir)

---

## ğŸ› Troubleshooting

### ProblÃ¨me: Conteneur Spark ne dÃ©marre pas
```bash
docker logs chu_spark_master
docker restart chu_spark_master
```

### ProblÃ¨me: Connexion MinIO Ã©choue
```bash
# VÃ©rifier que MinIO est dÃ©marrÃ©
docker logs chu_minio

# RecrÃ©er les buckets
docker restart chu_minio_setup
```

### ProblÃ¨me: PostgreSQL vide
```bash
# Restaurer le dump manuellement
docker exec chu_postgres pg_restore -U admin -d healthcare_data \
  -v --no-owner --no-acl \
  "/docker-entrypoint-initdb.d/DATA 2024/BDD PostgreSQL/DATA2023"
```

---

## ğŸ‘¥ Ã‰quipe

- **Nejma MOUALHI**
- **Brieuc OLIVIERI**
- **Nicolas TAING**

CESI FISA A4 - 2025-2026
