# ğŸ“Š SYNTHÃˆSE TECHNIQUE - CHU Data Lakehouse

**Projet**: CHU Data Lakehouse (Bronze â†’ Silver â†’ Gold)
**Ã‰quipe**: Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING
**Formation**: CESI FISA A4 - Big Data
**Date**: Octobre 2025

---

## ğŸ¯ Objectif du Projet

CrÃ©er un Data Lakehouse complet pour analyser les donnÃ©es de santÃ© d'un CHU, avec:
- Architecture mÃ©daillon (Bronze/Silver/Gold)
- ConformitÃ© RGPD (pseudonymisation/anonymisation)
- Star Schema pour analyse BI
- Visualisation via Apache Superset

---

## ğŸ—ï¸ Architecture Technique

### Stack Technologique

| Composant | Technologie | Version | RÃ´le |
|-----------|-------------|---------|------|
| **Processing** | Apache Spark | 3.5.0 | Traitement distribuÃ© PySpark |
| **Storage** | MinIO | latest | Object storage S3-compatible |
| **Database** | PostgreSQL | 14 | Source + Gold layer export |
| **Notebooks** | Jupyter Lab | latest | DÃ©veloppement interactif |
| **BI Tool** | Apache Superset | latest | Visualisation et dashboards |
| **Orchestration** | Docker Compose | - | Gestion containers |

### Infrastructure Docker

```yaml
Services:
- chu_postgres (port 5432) - Base de donnÃ©es relationnelle
- chu_spark_master (port 7077) - Spark cluster master
- chu_spark_worker - Spark worker node
- chu_jupyter (port 8888) - Interface notebooks
- chu_superset (port 8088) - Interface BI
- chu_minio (port 9000, 9001) - Object storage

Volumes:
- spark/data/ - Lakehouse (Bronze/Silver/Gold)
- postgres/data/ - Base PostgreSQL persistante
- jupyter/notebooks/ - Scripts PySpark
```

---

## ğŸ“Š Sources de DonnÃ©es

### 1. PostgreSQL (13 tables)

| Table | Lignes | Description |
|-------|--------|-------------|
| **Patient** | ~300K | Patients du CHU |
| **Consultation** | ~1M | Consultations mÃ©dicales 2015-2023 |
| **AAAA** | 82,216 | Hospitalisations (patient + diagnostic) |
| **date** | 82,216 | Dates entrÃ©e/sortie hospitalisations |
| **Diagnostic** | 15,442 | Codes CIM-10 diagnostics |
| **Professionnel_de_sante** | ~200K | MÃ©decins et personnel |
| **Prescription** | ~1M | Prescriptions mÃ©dicaments |
| **Medicaments** | ~3K | RÃ©fÃ©rentiel mÃ©dicaments |
| **Laboratoire** | ~500K | Analyses laboratoire |
| **Adher** | ~300K | AdhÃ©rents mutuelles |
| **Mutuelle** | ~150 | Organismes mutuelles |
| **Specialites** | ~50 | SpÃ©cialitÃ©s mÃ©dicales |
| **Salle** | ~500 | Salles et Ã©quipements |

### 2. Fichiers CSV (4 sources)

| Fichier | Lignes | Description |
|---------|--------|-------------|
| **deces.csv** | 25M (filtrÃ© 2019: 620K) | DÃ©cÃ¨s INSEE |
| **departements.csv** | 101 | RÃ©fÃ©rentiel dÃ©partements |
| **etablissement_sante.csv** | ~3K | Ã‰tablissements santÃ© France |
| **satisfaction_esatis48h_2019.csv** | 8 | Scores satisfaction E-Satis |

---

## ğŸ”„ Architecture MÃ©daillon

### Bronze Layer (DonnÃ©es Brutes)

**Objectif**: Ingestion sans transformation

**Format**: Parquet compressÃ© (snappy)

**Structure**:
```
spark/data/bronze/
â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ Patient/ingestion_date=2025-10-22/
â”‚   â”œâ”€â”€ Consultation/ingestion_date=2025-10-22/
â”‚   â”œâ”€â”€ AAAA/ingestion_date=2025-10-22/
â”‚   â”œâ”€â”€ date/ingestion_date=2025-10-22/
â”‚   â””â”€â”€ ... (10 autres tables)
â””â”€â”€ csv/
    â”œâ”€â”€ deces_2019/
    â”œâ”€â”€ departements/
    â”œâ”€â”€ etablissement_sante/
    â””â”€â”€ satisfaction_esatis48h_2019/
```

**MÃ©tadonnÃ©es ajoutÃ©es**:
- `ingestion_timestamp` (timestamp)
- `ingestion_date` (date YYYY-MM-DD)

**VolumÃ©trie**: ~4M lignes, ~2 minutes d'ingestion

---

### Silver Layer (DonnÃ©es NettoyÃ©es)

**Objectif**: Nettoyage + Pseudonymisation RGPD

**Transformations appliquÃ©es**:

1. **Pseudonymisation SHA-256** (rÃ©versible avec sel):
   ```python
   # Hash PII avec sel constant
   hash_udf = udf(lambda x: hashlib.sha256(
       (str(x) + SALT).encode()
   ).hexdigest(), StringType())

   df_clean = df_raw.withColumn(
       "nom_hash", hash_udf(col("nom"))
   ).drop("nom")
   ```

2. **Formatage dates**:
   - Unification format: `yyyy-MM-dd`
   - Conversion types: string â†’ date/timestamp

3. **Nettoyage valeurs**:
   - Suppression nulls critiques
   - Trim espaces
   - Normalisation casse

4. **Suppression colonnes sensibles**:
   - Adresses complÃ¨tes
   - NumÃ©ros sÃ©curitÃ© sociale
   - Emails personnels

**Structure**:
```
spark/data/silver/
â”œâ”€â”€ patient_clean/
â”œâ”€â”€ consultation_clean/
â”œâ”€â”€ hospitalisation_clean/
â”œâ”€â”€ diagnostic_clean/
â”œâ”€â”€ professionnel_clean/
â”œâ”€â”€ prescription_clean/
â”œâ”€â”€ laboratoire_clean/
â”œâ”€â”€ adher_clean/
â”œâ”€â”€ mutuelle_clean/
â”œâ”€â”€ deces_2019_clean/
â”œâ”€â”€ departements_clean/
â”œâ”€â”€ etablissement_sante_clean/
â””â”€â”€ satisfaction_clean/
```

**VolumÃ©trie**: ~3.5M lignes, ~3 minutes de traitement

---

### Gold Layer (Star Schema)

**Objectif**: ModÃ¨le dimensionnel pour analyse BI

**Architecture**: Star Schema avec 5 dimensions et 4 faits

#### Dimensions (5)

##### 1. dim_temps
```sql
Colonnes:
- id_temps (PK, format yyyyMMdd)
- date_complete
- annee, mois, jour
- nom_mois, nom_jour_semaine
- trimestre, semestre
- est_weekend (boolean)

Partitionnement: annee
VolumÃ©trie: ~4,500 dates (2013-2025)
```

##### 2. dim_patient
```sql
Colonnes:
- id_patient_hash (PK, SHA-256)
- age_groupe (tranches: 0-18, 19-30, etc.)
- sexe
- departement_residence
- statut_mutuelle (adhÃ©rent/non)

Anonymisation:
- Nom/prÃ©nom hashÃ©s
- Adresse supprimÃ©e
- Date naissance â†’ groupe d'Ã¢ge

VolumÃ©trie: ~300K patients
```

##### 3. dim_diagnostic
```sql
Colonnes:
- code_diag (PK, code CIM-10)
- libelle
- categorie
- gravite

VolumÃ©trie: 15,442 diagnostics
```

##### 4. dim_professionnel
```sql
Colonnes:
- id_professionnel_hash (PK, SHA-256)
- specialite
- type_professionnel (mÃ©decin, infirmier, etc.)
- annees_experience_groupe

Anonymisation:
- Nom/prÃ©nom hashÃ©s
- Identifiants RPPS pseudonymisÃ©s

VolumÃ©trie: ~200K professionnels
```

##### 5. dim_etablissement
```sql
Colonnes:
- code_etablissement (PK, FINESS)
- nom_etablissement
- type_etablissement (CHU, clinique, etc.)
- departement
- region
- capacite_lits

VolumÃ©trie: ~3K Ã©tablissements
```

---

#### Tables de Faits (4)

##### 1. fait_consultation
```sql
ClÃ©s Ã©trangÃ¨res:
- id_temps (FK â†’ dim_temps)
- id_patient (FK â†’ dim_patient)
- id_professionnel (FK â†’ dim_professionnel)
- code_diag (FK â†’ dim_diagnostic)

Mesures:
- nb_consultations (COUNT)
- duree_consultation_minutes

Partitionnement: annee, mois
VolumÃ©trie: 1,027,157 consultations (2015-2023)
```

##### 2. fait_hospitalisation (DÃ‰COUVERT!)
```sql
Source: Tables AAAA + date (jointure par position)

ClÃ©s Ã©trangÃ¨res:
- id_temps_entree (FK â†’ dim_temps)
- id_temps_sortie (FK â†’ dim_temps)
- id_patient (FK â†’ dim_patient)
- code_diag (FK â†’ dim_diagnostic)

Mesures:
- nb_hospitalisations (COUNT)
- duree_sejour_jours (date_sortie - date_entree)
- duree_moyenne_sejour (AVG)

Partitionnement: annee, mois (date entrÃ©e)
VolumÃ©trie: 82,216 hospitalisations (2013-2025)
DurÃ©e moyenne: ~1 jour
```

##### 3. fait_deces
```sql
Source: CSV INSEE filtrÃ© 2019

ClÃ©s Ã©trangÃ¨res:
- id_temps (FK â†’ dim_temps)
- id_patient_hash (hors dim_patient, anonyme)

Mesures:
- nb_deces (COUNT)
- age_deces
- age_moyen_deces (AVG)

Anonymisation complÃ¨te:
- Hashing irrÃ©versible
- Pas de lien direct avec dim_patient
- AgrÃ©gations uniquement

Partitionnement: annee, mois
VolumÃ©trie: 620,625 dÃ©cÃ¨s (2019 uniquement)
```

##### 4. fait_satisfaction
```sql
Source: E-Satis 2019

ClÃ©s Ã©trangÃ¨res:
- id_temps (FK â†’ dim_temps)
- code_etablissement (FK â†’ dim_etablissement)

Mesures:
- score_satisfaction (0-100)
- nb_repondants
- score_moyen (AVG)

Partitionnement: annee
VolumÃ©trie: 8 Ã©tablissements (2019)
```

---

## ğŸ” ConformitÃ© RGPD

### StratÃ©gie par Layer

| Layer | Approche | RÃ©versibilitÃ© | AccÃ¨s |
|-------|----------|---------------|-------|
| **Bronze** | DonnÃ©es brutes | Oui (originales) | Restreint |
| **Silver** | Pseudonymisation (SHA-256 + sel) | Oui (avec sel) | ContrÃ´lÃ© |
| **Gold** | Anonymisation (agrÃ©gation) | Non | Ouvert BI |

### Techniques AppliquÃ©es

#### Pseudonymisation (Silver)
```python
SALT = "CHU_SECRET_KEY_2025"

def pseudonymize(value):
    return hashlib.sha256(
        (str(value) + SALT).encode()
    ).hexdigest()

# Application
df = df.withColumn("nom_hash", pseudonymize_udf(col("nom")))
```

#### Anonymisation (Gold)
1. **Suppression identifiants directs**: Nom, prÃ©nom, adresse
2. **GÃ©nÃ©ralisation**: Date naissance â†’ groupe d'Ã¢ge
3. **AgrÃ©gations**: Statistiques au lieu de lignes individuelles
4. **K-anonymat**: Groupes de minimum K patients

---

## ğŸ“ˆ Performances

### Temps d'ExÃ©cution

| Notebook | Ã‰tape | Temps | VolumÃ©trie |
|----------|-------|-------|------------|
| **01 - Bronze** | Extraction PostgreSQL | ~1.5 min | 13 tables, ~3M lignes |
| **01 - Bronze** | Extraction CSV (2019) | ~0.5 min | 4 fichiers, 620K lignes |
| **02 - Silver** | Nettoyage | ~2 min | 13 tables, ~3.5M lignes |
| **02 - Silver** | Pseudonymisation | ~1 min | SHA-256 sur 500K PII |
| **03 - Gold** | Dimensions | ~1 min | 5 dimensions, ~525K lignes |
| **03 - Gold** | Faits | ~2 min | 4 faits, ~2.9M lignes |
| **06 - Export** | PostgreSQL export | ~3 min | 9 tables Gold |
| **TOTAL** | Pipeline complet | **~11 min** | Bronze â†’ PostgreSQL |

### Optimisations AppliquÃ©es

1. **Filtrage prÃ©coce**: DÃ©cÃ¨s 2019 uniquement (98% rÃ©duction: 25M â†’ 620K)
2. **Partitionnement**: Par annÃ©e/mois (pruning automatique)
3. **Compression**: Snappy parquet (ratio ~5:1)
4. **Repartitioning**: Ã‰quilibrage partitions Spark
5. **Broadcast joins**: Pour petites dimensions (<10MB)

---

## ğŸ“ ConformitÃ© Livrable 1

### Exigences Livrables

âœ… **4 Tables de Faits**:
1. fait_consultation
2. fait_hospitalisation
3. fait_deces
4. fait_satisfaction

âœ… **5 Dimensions minimum**:
1. dim_temps
2. dim_patient
3. dim_diagnostic
4. dim_professionnel
5. dim_etablissement

âœ… **Star Schema**: ModÃ¨le dimensionnel complet

âœ… **RGPD**: Pseudonymisation Silver + Anonymisation Gold

âœ… **Export PostgreSQL**: 9 tables dans schema `gold`

âœ… **Visualisation BI**: Apache Superset connectÃ©

---

## ğŸ” DÃ©couvertes Techniques

### 1. Tables AAAA + date (Hospitalisation)

**ProblÃ¨me initial**: Pas de table Ã©vidente d'hospitalisation

**Investigation**:
- Analyse systÃ©matique des 13 tables PostgreSQL
- DÃ©couverte de 2 tables avec exactement 82,216 lignes
- AAAA: Patient (Num) + Diagnostic (Code_diag)
- date: Dates entrÃ©e (date1) + sortie (date2)

**Solution**: Jointure par position (row_id)
```python
df_aaaa_idx = df_aaaa.withColumn("row_id", monotonically_increasing_id())
df_date_idx = df_date.withColumn("row_id", monotonically_increasing_id())
df_hospit = df_aaaa_idx.join(df_date_idx, "row_id", "inner")
```

âœ… Validation: 100% match sur Patient.Id_patient et Diagnostic.Code_diag

### 2. Filtrage DÃ©cÃ¨s 2019

**ProblÃ¨me initial**: 25M lignes causaient timeout (3+ minutes)

**Analyse**:
- Livrable 1 demande analyse 2019
- 98% des donnÃ©es inutiles (1919-2018, 2020-2023)

**Solution**: Filtrage prÃ©coce
```python
df_deces = df_deces_raw.filter(col("date_deces").startswith("2019"))
```

**RÃ©sultat**: 620K lignes, 40 secondes (-85% temps)

### 3. Erreur Colonne "datdec"

**Erreur**: `UNRESOLVED_COLUMN: datdec`

**Cause**: CSV utilise `date_deces` pas `datdec`

**Fix**: Lecture schÃ©ma CSV avant filtrage
```python
# Afficher colonnes disponibles
df_deces_raw.printSchema()
# Colonnes: [nom, prenom, sexe, date_naissance, date_deces, ...]

# Utiliser le bon nom
df_filtered = df_deces_raw.filter(col("date_deces").startswith("2019"))
```

---

## ğŸ“Š RequÃªtes SQL Utiles

### Statistiques Globales
```sql
-- Volume par table de fait
SELECT
    'fait_consultation' as table_name,
    COUNT(*) as total_lignes,
    MIN(annee) as annee_min,
    MAX(annee) as annee_max
FROM gold.fait_consultation
UNION ALL
SELECT 'fait_hospitalisation', COUNT(*), MIN(annee), MAX(annee)
FROM gold.fait_hospitalisation
UNION ALL
SELECT 'fait_deces', COUNT(*), MIN(annee), MAX(annee)
FROM gold.fait_deces
UNION ALL
SELECT 'fait_satisfaction', COUNT(*), MIN(annee), MAX(annee)
FROM gold.fait_satisfaction;
```

### Analyse Consultations
```sql
-- Consultations par spÃ©cialitÃ© et annÃ©e
SELECT
    p.specialite,
    t.annee,
    COUNT(*) as nb_consultations,
    COUNT(DISTINCT c.id_patient) as patients_uniques,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY t.annee), 2) as pct_annee
FROM gold.fait_consultation c
JOIN gold.dim_professionnel p ON c.id_professionnel = p.id_professionnel_hash
JOIN gold.dim_temps t ON c.id_temps = t.id_temps
GROUP BY p.specialite, t.annee
ORDER BY t.annee DESC, nb_consultations DESC;
```

### Analyse Hospitalisations
```sql
-- DurÃ©e moyenne sÃ©jour par diagnostic
SELECT
    d.categorie,
    d.libelle,
    COUNT(*) as nb_hospitalisations,
    ROUND(AVG(h.duree_sejour_jours), 2) as duree_moyenne_jours,
    MIN(h.duree_sejour_jours) as duree_min,
    MAX(h.duree_sejour_jours) as duree_max
FROM gold.fait_hospitalisation h
JOIN gold.dim_diagnostic d ON h.code_diag = d.code_diag
GROUP BY d.categorie, d.libelle
HAVING COUNT(*) >= 100
ORDER BY nb_hospitalisations DESC
LIMIT 20;
```

### Analyse DÃ©cÃ¨s 2019
```sql
-- DÃ©cÃ¨s par mois et tranche d'Ã¢ge
SELECT
    t.mois,
    t.nom_mois,
    CASE
        WHEN d.age_deces < 18 THEN '0-17 ans'
        WHEN d.age_deces < 65 THEN '18-64 ans'
        WHEN d.age_deces < 85 THEN '65-84 ans'
        ELSE '85+ ans'
    END as tranche_age,
    COUNT(*) as nb_deces,
    ROUND(AVG(d.age_deces), 1) as age_moyen
FROM gold.fait_deces d
JOIN gold.dim_temps t ON d.id_temps = t.id_temps
GROUP BY t.mois, t.nom_mois, tranche_age
ORDER BY t.mois, tranche_age;
```

---

## ğŸ¨ Dashboards Superset RecommandÃ©s

### Dashboard 1: Vue d'Ensemble CHU
- KPI: Total consultations, hospitalisations, taux occupation
- Graphique: Ã‰volution consultations par mois (2015-2023)
- Camembert: RÃ©partition consultations par spÃ©cialitÃ©
- Tableau: Top 10 diagnostics

### Dashboard 2: Analyse Hospitalisations
- KPI: DurÃ©e moyenne sÃ©jour, nb hospitalisations
- Graphique: DurÃ©e sÃ©jour par catÃ©gorie diagnostic
- Heatmap: Hospitalisations par mois/annÃ©e
- Tableau: Statistiques par diagnostic

### Dashboard 3: MortalitÃ© 2019
- KPI: Total dÃ©cÃ¨s, Ã¢ge moyen dÃ©cÃ¨s
- Graphique: Ã‰volution mensuelle dÃ©cÃ¨s 2019
- Pyramide: RÃ©partition par tranche d'Ã¢ge
- Carte: DÃ©cÃ¨s par dÃ©partement

### Dashboard 4: Satisfaction E-Satis
- KPI: Score moyen satisfaction
- Graphique: Comparaison Ã©tablissements
- Tableau: DÃ©tails par Ã©tablissement

---

## ğŸš€ AmÃ©liorations Futures

### Court Terme
1. Ajouter `dim_medicament` et `fait_prescription`
2. CrÃ©er `fait_laboratoire` pour analyses bio
3. ImplÃ©menter Quality Checks (Great Expectations)
4. Ajouter logging structurÃ© (JSON)

### Moyen Terme
1. Migration vers Databricks ou Snowflake
2. Orchestration Airflow pour pipeline automatique
3. CDC (Change Data Capture) pour Bronze incrÃ©mental
4. Data Catalog (Apache Atlas)

### Long Terme
1. Machine Learning: PrÃ©diction durÃ©e hospitalisation
2. Real-time streaming (Kafka + Spark Structured Streaming)
3. Data Mesh: Domaines mÃ©tier sÃ©parÃ©s
4. MLOps: ModÃ¨les en production

---

## ğŸ“š Ressources et RÃ©fÃ©rences

### Documentation Technique
- Apache Spark 3.5: https://spark.apache.org/docs/3.5.0/
- PostgreSQL 14: https://www.postgresql.org/docs/14/
- Apache Superset: https://superset.apache.org/docs/

### ConformitÃ© RGPD
- CNIL: Guide pseudonymisation
- RGPD Article 4(5): DÃ©finition pseudonymisation
- K-anonymat: Sweeney, L. (2002)

### Architecture Data
- Data Lakehouse (Databricks): https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html
- Medallion Architecture: Bronze/Silver/Gold layers
- Kimball Dimensional Modeling: The Data Warehouse Toolkit

---

## âœ… Checklist Validation Projet

**FonctionnalitÃ©s**:
- [x] Pipeline Bronze/Silver/Gold opÃ©rationnel
- [x] 4 tables de faits crÃ©Ã©es et peuplÃ©es
- [x] 5 dimensions crÃ©Ã©es et peuplÃ©es
- [x] Star Schema complet et cohÃ©rent
- [x] Pseudonymisation Silver (SHA-256)
- [x] Anonymisation Gold (agrÃ©gations)
- [x] Export PostgreSQL fonctionnel
- [x] Superset connectÃ© et requÃªtes testÃ©es

**Documentation**:
- [x] Guide d'utilisation complet
- [x] SynthÃ¨se technique (ce document)
- [x] Troubleshooting guide
- [x] RequÃªtes SQL exemples
- [x] Architecture diagrammes

**QualitÃ© Code**:
- [x] Notebooks commentÃ©s et structurÃ©s
- [x] Gestion erreurs (try/except)
- [x] Logs informatifs
- [x] Code reproductible

**Livrables**:
- [x] Docker Compose fonctionnel
- [x] Notebooks exÃ©cutables
- [x] DonnÃ©es Gold dans PostgreSQL
- [x] Documentation consolidÃ©e

---

**ğŸ‰ Projet Livrable 2 - COMPLET ET VALIDÃ‰**
