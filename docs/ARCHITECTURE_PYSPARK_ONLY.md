# âœ… ARCHITECTURE 100% PYSPARK (Sans Scala/JARs compliquÃ©s)

**Date**: 23 Octobre 2025
**Confirmation**: OUI, on peut faire TOUT en PySpark sans toucher Ã  Scala

---

## ğŸ¯ VOTRE ARCHITECTURE (ValidÃ©e âœ…)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CODE PYSPARK (.py files)                                   â”‚
â”‚  â”œâ”€ 01_extract_bronze.py                                    â”‚
â”‚  â”œâ”€ 02_transform_silver.py                                  â”‚
â”‚  â”œâ”€ 03_transform_gold.py                                    â”‚
â”‚  â””â”€ 04_benchmarks.py                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AIRFLOW DAG (orchestration Python)                         â”‚
â”‚  â””â”€ chu_etlt_dag.py                                         â”‚
â”‚     - PythonOperator (exÃ©cute .py directement)              â”‚
â”‚     OU                                                       â”‚
â”‚     - SparkSubmitOperator (soumet Ã  Spark Master)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPARK ENGINE (moteur de calcul)                            â”‚
â”‚  - Spark Master + Worker                                    â”‚
â”‚  - ExÃ©cute le bytecode Python                               â”‚
â”‚  - PAS besoin de Scala/JAR custom                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STOCKAGE (MinIO/Local)                                     â”‚
â”‚  â”œâ”€ Bronze/ (Parquet)                                       â”‚
â”‚  â”œâ”€ Silver/ (Parquet)                                       â”‚
â”‚  â””â”€ Gold/ (Parquet)                                         â”‚
â”‚  Note: Delta Lake = optionnel (Parquet suffit!)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… CE QU'ON VA FAIRE (100% Python)

### 1. Code PySpark Pur

```python
# spark/jobs/01_extract_bronze.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# âœ… Tout en Python
# âŒ Pas de Scala
# âŒ Pas de JARs custom

def main():
    spark = SparkSession.builder \
        .appName("CHU Extract Bronze") \
        .getOrCreate()  # âœ… Simple et propre

    # Extraction PostgreSQL (driver JDBC dÃ©jÃ  dans Spark)
    df = spark.read.jdbc(
        url="jdbc:postgresql://chu_postgres:5432/healthcare_data",
        table="Patient",
        properties={"user": "admin", "password": "admin123"}
    )

    # Ã‰crire en Parquet (pas besoin de JAR)
    df.write.mode("overwrite") \
        .option("compression", "snappy") \
        .parquet("/home/jovyan/data/bronze/patient")

    spark.stop()

if __name__ == "__main__":
    main()
```

### 2. Airflow DAG Python

```python
# airflow/dags/chu_etlt_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import subprocess

# âœ… Option 1: PythonOperator (exÃ©cute Python directement)
def run_extract_bronze():
    """ExÃ©cute le job PySpark via subprocess"""
    result = subprocess.run([
        'spark-submit',
        '--master', 'local[*]',  # ou 'spark://chu_spark_master:7077'
        '/opt/spark-apps/01_extract_bronze.py'
    ], capture_output=True, text=True)

    if result.returncode != 0:
        raise Exception(f"Job failed: {result.stderr}")

    print(result.stdout)

dag = DAG('chu_etlt', start_date=datetime(2025, 10, 23))

extract_task = PythonOperator(
    task_id='extract_bronze',
    python_callable=run_extract_bronze,
    dag=dag
)
```

**OU encore plus simple**:

```python
# âœ… Option 2: SparkSubmitOperator (Airflow gÃ¨re spark-submit)
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

extract_task = SparkSubmitOperator(
    task_id='extract_bronze',
    application='/opt/spark-apps/01_extract_bronze.py',
    conn_id='spark_default',  # Connection Spark dans Airflow UI
    dag=dag
)
```

---

## â“ DELTA LAKE: Obligatoire ou pas?

### RÃ©ponse: **PAS OBLIGATOIRE** pour votre Livrable 2 ! âœ…

**Ce qui suffit**:
```python
# âœ… PARQUET SIMPLE (ce que vous faites dÃ©jÃ )
df.write.mode("overwrite").parquet(path)

# Avantages:
# âœ… Pas de dÃ©pendance Delta Lake
# âœ… Pas de JAR supplÃ©mentaire
# âœ… Compatible avec tout (Spark, Pandas, DuckDB, etc.)
# âœ… Format standard industrie
```

**Delta Lake apporte quoi?**
```python
# ğŸ”¸ DELTA LAKE (optionnel, ajout de features)
df.write.format("delta").mode("overwrite").save(path)

# Avantages supplÃ©mentaires:
# + Transactions ACID (important pour production)
# + Time travel (revenir version prÃ©cÃ©dente)
# + Schema evolution automatique
# + Meilleure gestion concurrence

# InconvÃ©nient:
# - Besoin de la lib Delta Lake (mais c'est juste pip install)
```

**Recommandation pour VOUS**:
- **Livrable 2**: Rester avec **Parquet simple** âœ…
- **Bonus (si temps)**: Ajouter Delta Lake dans 1-2 jobs pour montrer que vous connaissez
- **Production future**: Migrer vers Delta Lake

---

## ğŸ”§ CONFIGURATION MINIMALE (Aucun JAR custom nÃ©cessaire)

### Dockerfile Spark (dÃ©jÃ  fourni par Apache)

```dockerfile
# Votre image Spark actuelle suffit!
FROM apache/spark:3.4.0

# âœ… Driver PostgreSQL JDBC (dÃ©jÃ  inclus dans l'image)
# âœ… PySpark (dÃ©jÃ  inclus)
# âœ… Pandas (pour conversions optionnelles)

# Optionnel: Delta Lake si vous voulez
RUN pip install delta-spark

# C'EST TOUT! Pas de Scala, pas de compilation, pas de JARs custom
```

### spark-submit Command

```bash
# âœ… SIMPLE: Python pur
spark-submit \
  --master spark://chu_spark_master:7077 \
  --executor-memory 2g \
  --driver-memory 1g \
  /opt/spark-apps/01_extract_bronze.py

# âŒ PAS BESOIN DE:
# --jars custom.jar                    # Non!
# --packages avec.maven.coordinates    # Non!
# --class com.scala.MainClass          # Non! (Scala only)
```

---

## ğŸ“¦ LIBRAIRIES NÃ‰CESSAIRES (Toutes disponibles via pip)

```txt
# requirements.txt pour vos jobs Spark
pyspark==3.4.0          # âœ… Moteur Spark Python
pandas==2.0.0           # âœ… Manipulation donnÃ©es
numpy==1.24.0           # âœ… Calculs numÃ©riques

# Optionnel:
delta-spark==2.4.0      # ğŸ”¸ Si vous voulez Delta Lake
psycopg2-binary==2.9.0  # ğŸ”¸ Si accÃ¨s direct PostgreSQL depuis Python
matplotlib==3.7.0       # ğŸ”¸ Pour graphiques dans notebooks
seaborn==0.12.0         # ğŸ”¸ Pour graphiques plus jolis
```

**Installation**:
```bash
# Dans le container Jupyter
pip install -r requirements.txt

# âœ… Pas de compilation Java/Scala
# âœ… Pas de mvn package
# âœ… Pas de sbt assembly
```

---

## ğŸ¯ FLUX COMPLET (100% Python)

### 1. DÃ©veloppement (Jupyter)

```python
# Dans notebook 01_Extract_Bronze.ipynb
spark = SparkSession.builder \
    .appName("Test") \
    .master("local[*]") \
    .getOrCreate()

# DÃ©velopper et tester le code interactivement
df = spark.read.jdbc(...)
df.write.parquet(...)
```

### 2. Conversion en Job (Script Python)

```bash
# Copier cellules notebook â†’ fichier .py
# spark/jobs/01_extract_bronze.py
# (voir template plus haut)
```

### 3. Test Local

```bash
# Tester job directement
docker exec chu_jupyter python /opt/spark-apps/01_extract_bronze.py

# OU avec spark-submit
docker exec chu_jupyter spark-submit /opt/spark-apps/01_extract_bronze.py
```

### 4. Orchestration Airflow

```python
# airflow/dags/chu_etlt_dag.py
# (voir template plus haut)

# Airflow appelle:
# â†’ spark-submit 01_extract_bronze.py
# â†’ spark-submit 02_transform_silver.py
# â†’ etc.
```

### 5. Monitoring

```bash
# Spark UI: http://localhost:4040 (pendant exÃ©cution)
# Airflow UI: http://localhost:8080 (orchestration)
# Logs: docker logs chu_spark_master
```

---

## âœ… AVANTAGES DE CETTE APPROCHE

| Aspect | Avec Scala/JARs | Avec PySpark Pur |
|--------|----------------|------------------|
| **ComplexitÃ©** | ğŸ”´ Ã‰levÃ©e (build, JARs, Maven) | ğŸŸ¢ Simple (Python pur) |
| **Compilation** | ğŸ”´ Obligatoire (sbt/mvn) | ğŸŸ¢ Aucune (interprÃ©tÃ©) |
| **DÃ©pendances** | ğŸ”´ Gestion JARs complexe | ğŸŸ¢ pip install |
| **DÃ©bogage** | ğŸ”´ Difficile (stacktraces Java) | ğŸŸ¢ Facile (Python) |
| **Courbe apprentissage** | ğŸ”´ Scala + Spark | ğŸŸ¢ Python seulement |
| **Performance** | ğŸŸ¡ LÃ©gÃ¨rement meilleur | ğŸŸ¡ Quasi identique |
| **Livrable acadÃ©mique** | ğŸ”´ Surcomplication | ğŸŸ¢ AppropriÃ© |

**Conclusion**: PySpark pur = **parfait pour votre contexte** âœ…

---

## ğŸš« CE QU'ON NE FAIT PAS

### âŒ Pas de Scala

```scala
// âŒ On NE fait PAS Ã§a:
object ExtractBronze {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()...
    // Compilation requise, JARs, complexe
  }
}
```

### âŒ Pas de JARs custom

```bash
# âŒ On NE fait PAS Ã§a:
sbt assembly  # CrÃ©er JAR custom
spark-submit --jars custom-assembly.jar ...
```

### âŒ Pas de Maven/SBT

```xml
<!-- âŒ Pas de pom.xml -->
<dependencies>
  <dependency>...</dependency>
</dependencies>
```

---

## ğŸ¯ EXEMPLE COMPLET: Job PySpark â†’ Airflow â†’ Spark Engine

### Fichier 1: Job PySpark

```python
# spark/jobs/01_extract_bronze.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp, current_date
import sys

def create_spark_session():
    """CrÃ©e session Spark avec config optimale"""
    return SparkSession.builder \
        .appName("CHU - Extract Bronze") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

def extract_postgres(spark):
    """Extrait tables PostgreSQL"""
    jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
    props = {"user": "admin", "password": "admin123"}

    tables = ["Patient", "Consultation", "AAAA", "date"]

    for table in tables:
        print(f"ğŸ“– Extracting {table}...")

        df = spark.read.jdbc(jdbc_url, table, properties=props)
        df = df.withColumn("ingestion_timestamp", current_timestamp()) \
               .withColumn("ingestion_date", current_date())

        output = f"/home/jovyan/data/bronze/postgres/{table}"
        df.write.mode("overwrite") \
            .option("compression", "snappy") \
            .partitionBy("ingestion_date") \
            .parquet(output)

        count = df.count()
        print(f"âœ… {table}: {count:,} rows extracted")

def main():
    print("ğŸš€ Starting Bronze Extraction...")
    spark = create_spark_session()

    try:
        extract_postgres(spark)
        print("âœ… Extraction completed successfully!")
        return 0
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return 1
    finally:
        spark.stop()

if __name__ == "__main__":
    sys.exit(main())
```

### Fichier 2: DAG Airflow

```python
# airflow/dags/chu_etlt_dag.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess
import sys

default_args = {
    'owner': 'chu_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 23),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'chu_etlt_pipeline',
    default_args=default_args,
    description='Pipeline ETLT CHU (100% PySpark)',
    schedule_interval='@daily',
    catchup=False,
)

def run_pyspark_job(job_name):
    """ExÃ©cute un job PySpark"""
    print(f"ğŸš€ Launching {job_name}...")

    cmd = [
        'spark-submit',
        '--master', 'local[*]',
        '--executor-memory', '2g',
        '--driver-memory', '1g',
        f'/opt/spark-apps/{job_name}.py'
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)

    print(result.stdout)
    if result.returncode != 0:
        print(result.stderr, file=sys.stderr)
        raise Exception(f"Job {job_name} failed!")

    print(f"âœ… {job_name} completed!")

# DÃ©finir les tÃ¢ches
extract_bronze = PythonOperator(
    task_id='extract_bronze',
    python_callable=lambda: run_pyspark_job('01_extract_bronze'),
    dag=dag,
)

transform_silver = PythonOperator(
    task_id='transform_silver',
    python_callable=lambda: run_pyspark_job('02_transform_silver'),
    dag=dag,
)

transform_gold = PythonOperator(
    task_id='transform_gold',
    python_callable=lambda: run_pyspark_job('03_transform_gold'),
    dag=dag,
)

benchmarks = PythonOperator(
    task_id='run_benchmarks',
    python_callable=lambda: run_pyspark_job('04_benchmarks'),
    dag=dag,
)

# Ordre d'exÃ©cution
extract_bronze >> transform_silver >> transform_gold >> benchmarks
```

### Fichier 3: Test

```bash
# Test local du job
docker exec chu_jupyter python /opt/spark-apps/01_extract_bronze.py

# Test avec Airflow
docker exec chu_airflow_webserver airflow dags trigger chu_etlt_pipeline

# VÃ©rifier logs
docker exec chu_airflow_webserver airflow tasks logs chu_etlt_pipeline extract_bronze 2025-10-23
```

---

## ğŸ“ POUR LE LIVRABLE 2

### Dans le rapport LaTeX

**Section "Technologies"**:
```latex
\subsection{Justification PySpark}

Le projet utilise exclusivement \textbf{PySpark} (API Python pour Apache Spark)
plutÃ´t que Scala pour les raisons suivantes:

\begin{itemize}
\item \textbf{SimplicitÃ©}: Pas de compilation, dÃ©ploiement direct de scripts Python
\item \textbf{MaintenabilitÃ©}: Code Python plus accessible pour l'Ã©quipe data science
\item \textbf{Ã‰cosystÃ¨me}: IntÃ©gration naturelle avec pandas, numpy, matplotlib
\item \textbf{Performance}: Quasi-identique Ã  Scala pour nos volumes (< 5M lignes)
\item \textbf{DÃ©ploiement}: Pas de gestion de JARs, dÃ©pendances via pip
\end{itemize}

Les jobs Spark sont orchestrÃ©s par \textbf{Apache Airflow} qui lance les scripts
Python via \texttt{spark-submit}, exploitant le moteur Spark distribuÃ© sans
nÃ©cessiter de code Scala.
```

---

## âœ… CONCLUSION

**Votre architecture est PARFAITE**:

```
PySpark Scripts (.py)
    â†“
Airflow DAG (orchestration)
    â†“
Spark Engine (calcul distribuÃ©)
    â†“
Parquet Files (stockage)
```

**Pas besoin de**:
- âŒ Scala
- âŒ JARs custom
- âŒ Maven/SBT
- âŒ Delta Lake (optionnel)

**Tout en**:
- âœ… Python pur
- âœ… PySpark natif
- âœ… Spark Engine standard
- âœ… Parquet standard

**ğŸ¯ On commence maintenant Ã  crÃ©er les jobs?**
