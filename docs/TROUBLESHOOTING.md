# üîß TROUBLESHOOTING - CHU Data Lakehouse

**Guide de r√©solution des probl√®mes courants**

---

## üìã Table des Mati√®res

1. [Docker et Containers](#-docker-et-containers)
2. [Jupyter et Notebooks](#-jupyter-et-notebooks)
3. [Apache Spark](#-apache-spark)
4. [PostgreSQL](#-postgresql)
5. [Apache Superset](#-apache-superset)
6. [Erreurs de Pipeline](#-erreurs-de-pipeline)
7. [Performances](#-performances)

---

## üê≥ Docker et Containers

### Container ne d√©marre pas

**Sympt√¥me**: `docker ps` ne montre pas le container

**Solutions**:

```bash
# 1. V√©rifier les logs
docker logs chu_jupyter --tail 50
docker logs chu_superset --tail 50
docker logs chu_postgres --tail 50

# 2. Red√©marrer le container
docker restart chu_jupyter

# 3. D√©marrer manuellement si arr√™t√©
docker start chu_jupyter

# 4. Recr√©er compl√®tement
docker-compose down
docker-compose up -d
```

### Port d√©j√† utilis√©

**Sympt√¥me**: `Error: bind: address already in use`

**Solutions**:

```bash
# Windows: Trouver le processus utilisant le port
netstat -ano | findstr :8888
netstat -ano | findstr :8088
netstat -ano | findstr :5432

# Tuer le processus (remplacer PID par le num√©ro trouv√©)
taskkill /PID <PID> /F

# OU modifier docker-compose.yml pour changer les ports
# Exemple: "8889:8888" au lieu de "8888:8888"
```

### M√©moire insuffisante

**Sympt√¥me**: Container red√©marre en boucle, logs montrent OOM

**Solutions**:

```bash
# 1. Augmenter RAM allou√©e √† Docker Desktop
# Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory: 8GB minimum

# 2. R√©duire nombre de workers Spark dans docker-compose.yml
# Supprimer ou commenter chu_spark_worker

# 3. Limiter m√©moire Java dans notebooks
spark = SparkSession.builder \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()
```

### Volumes non mont√©s

**Sympt√¥me**: Donn√©es disparaissent apr√®s restart

**Solutions**:

```bash
# V√©rifier volumes Docker
docker volume ls

# Inspecter un container
docker inspect chu_jupyter | grep -A 10 Mounts

# Recr√©er volumes si corrompus
docker-compose down -v  # ‚ö†Ô∏è ATTENTION: Supprime les donn√©es!
docker-compose up -d
```

---

## üìì Jupyter et Notebooks

### Token Jupyter invalide

**Sympt√¥me**: "Invalid credentials" lors de la connexion

**Solutions**:

```bash
# 1. R√©cup√©rer le token
docker logs chu_jupyter 2>&1 | grep token

# R√©sultat: http://127.0.0.1:8888/?token=abc123def456...

# 2. OU se connecter sans token (si configur√©)
# URL: http://localhost:8888
# Password: (laisser vide)

# 3. Red√©marrer Jupyter si probl√®me persiste
docker restart chu_jupyter
```

### Kernel Python crash

**Sympt√¥me**: "Kernel died, restarting" dans notebook

**Solutions**:

```python
# 1. R√©duire m√©moire Spark
spark = SparkSession.builder \
    .config("spark.driver.memory", "1g") \
    .config("spark.executor.memory", "1g") \
    .getOrCreate()

# 2. Limiter parall√©lisme
df = df.repartition(2)  # Au lieu de 10

# 3. Lib√©rer cache
spark.catalog.clearCache()

# 4. Arr√™ter session Spark proprement
spark.stop()
```

### Impossible de sauvegarder notebook

**Sympt√¥me**: "Permission denied" ou "Failed to save"

**Solutions**:

```bash
# 1. V√©rifier permissions du volume
docker exec chu_jupyter ls -la /home/jovyan/notebooks/

# 2. Corriger permissions si n√©cessaire
docker exec --user root chu_jupyter chown -R jovyan:users /home/jovyan/notebooks/

# 3. Red√©marrer Jupyter
docker restart chu_jupyter
```

### Package Python manquant

**Sympt√¥me**: `ModuleNotFoundError: No module named 'xxx'`

**Solutions**:

```bash
# 1. Installer dans le container
docker exec chu_jupyter pip install package_name

# 2. OU dans une cellule notebook
!pip install package_name

# 3. Pour installation permanente, ajouter au Dockerfile
RUN pip install package_name

# 4. Reconstruire image
docker-compose build jupyter
docker-compose up -d
```

---

## ‚ö° Apache Spark

### Spark session ne d√©marre pas

**Sympt√¥me**: `Exception: Java gateway process exited before sending its port number`

**Solutions**:

```bash
# 1. V√©rifier Java install√© dans container
docker exec chu_jupyter java -version

# 2. V√©rifier JAVA_HOME
docker exec chu_jupyter echo $JAVA_HOME

# 3. Red√©marrer Spark master
docker restart chu_spark_master

# 4. R√©duire configuration m√©moire
spark = SparkSession.builder \
    .master("local[2]") \
    .config("spark.driver.memory", "1g") \
    .getOrCreate()
```

### Erreur "Cannot connect to Spark master"

**Sympt√¥me**: `Exception: Could not connect to Spark master at spark://chu_spark_master:7077`

**Solutions**:

```bash
# 1. V√©rifier Spark master running
docker ps | grep spark_master

# 2. Tester connectivit√© r√©seau
docker exec chu_jupyter ping chu_spark_master

# 3. Utiliser mode local si master indisponible
spark = SparkSession.builder \
    .master("local[*]") \  # Au lieu de spark://...
    .getOrCreate()

# 4. Red√©marrer tous services Spark
docker restart chu_spark_master chu_spark_worker
```

### OutOfMemoryError Java

**Sympt√¥me**: `java.lang.OutOfMemoryError: Java heap space`

**Solutions**:

```python
# 1. Augmenter m√©moire driver
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .config("spark.driver.maxResultSize", "2g") \
    .getOrCreate()

# 2. Limiter taille cache
spark.conf.set("spark.sql.inMemoryColumnarStorage.batchSize", 10000)

# 3. D√©sactiver cache si non n√©cessaire
df.unpersist()

# 4. Repartitionner pour r√©duire taille par partition
df = df.repartition(20)
```

### Lecture Parquet √©choue

**Sympt√¥me**: `AnalysisException: Unable to infer schema for Parquet`

**Solutions**:

```python
# 1. V√©rifier chemin existe
import os
path = "/home/jovyan/data/bronze/postgres/Patient"
if os.path.exists(path):
    print(f"‚úÖ {path} existe")
else:
    print(f"‚ùå {path} n'existe pas")

# 2. Lire avec option mergeSchema
df = spark.read \
    .option("mergeSchema", "true") \
    .parquet(path)

# 3. V√©rifier fichiers parquet non vides
!ls -lh /home/jovyan/data/bronze/postgres/Patient/

# 4. Recr√©er Bronze si corrompu
# Relancer Notebook 01
```

---

## üêò PostgreSQL

### Connection refused

**Sympt√¥me**: `psycopg2.OperationalError: could not connect to server`

**Solutions**:

```bash
# 1. V√©rifier PostgreSQL running
docker ps | grep postgres

# 2. Tester connexion depuis Jupyter
docker exec chu_jupyter psql -h chu_postgres -U admin -d healthcare_data -c "SELECT 1;"

# 3. V√©rifier port expos√©
docker port chu_postgres

# 4. Red√©marrer PostgreSQL
docker restart chu_postgres

# 5. V√©rifier credentials dans docker-compose.yml
POSTGRES_USER: admin
POSTGRES_PASSWORD: admin123
POSTGRES_DB: healthcare_data
```

### Table n'existe pas

**Sympt√¥me**: `ERROR: relation "gold.fait_consultation" does not exist`

**Solutions**:

```bash
# 1. Lister les schemas
docker exec chu_postgres psql -U admin -d healthcare_data -c "\dn"

# 2. Lister tables dans schema gold
docker exec chu_postgres psql -U admin -d healthcare_data -c "\dt gold.*"

# 3. Cr√©er schema si manquant
docker exec chu_postgres psql -U admin -d healthcare_data -c "CREATE SCHEMA IF NOT EXISTS gold;"

# 4. Re-ex√©cuter Notebook 06 pour exporter Gold
# Jupyter ‚Üí 06_Export_Gold_to_PostgreSQL.ipynb ‚Üí Run All
```

### Donn√©es sources manquantes

**Sympt√¥me**: `SELECT` retourne 0 lignes sur tables sources

**Solutions**:

```bash
# 1. V√©rifier tables sources
docker exec chu_postgres psql -U admin -d healthcare_data -c "
SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename))
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY tablename;
"

# 2. R√©initialiser base si vide
docker exec chu_postgres psql -U admin -d healthcare_data -c "
DROP SCHEMA IF EXISTS public CASCADE;
CREATE SCHEMA public;
"

# 3. Reload donn√©es depuis dump SQL
docker cp /path/to/backup.sql chu_postgres:/tmp/
docker exec chu_postgres psql -U admin -d healthcare_data -f /tmp/backup.sql
```

### Erreur JDBC dans Spark

**Sympt√¥me**: `java.sql.SQLException: No suitable driver found for jdbc:postgresql`

**Solutions**:

```python
# 1. V√©rifier driver JDBC pr√©sent
import os
jdbc_path = "/usr/local/spark/jars/postgresql-42.6.0.jar"
print(f"JDBC driver exists: {os.path.exists(jdbc_path)}")

# 2. Sp√©cifier driver explicitement
jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
properties = {
    "user": "admin",
    "password": "admin123",
    "driver": "org.postgresql.Driver"
}
df = spark.read.jdbc(jdbc_url, "public.Patient", properties=properties)

# 3. T√©l√©charger driver si manquant
!wget https://jdbc.postgresql.org/download/postgresql-42.6.0.jar -P /usr/local/spark/jars/
```

---

## üé® Apache Superset

### "Could not load database driver"

**Sympt√¥me**: `ERROR: Could not load database driver: PostgresEngineSpec`

**Solution**:

```bash
# 1. Installer driver PostgreSQL
docker exec --user root chu_superset pip install psycopg2-binary

# 2. Red√©marrer Superset
docker restart chu_superset

# 3. Attendre 30 secondes pour red√©marrage complet

# 4. Tester connexion
# Superset UI ‚Üí Settings ‚Üí Database Connections ‚Üí Test Connection
```

### Connexion PostgreSQL √©choue

**Sympt√¥me**: "Connection failed" lors du test

**Solutions**:

```bash
# 1. V√©rifier URI SQLAlchemy
# Format: postgresql://user:password@host:port/database
postgresql://admin:admin123@chu_postgres:5432/healthcare_data

# 2. Tester connexion depuis container Superset
docker exec chu_superset psql -h chu_postgres -U admin -d healthcare_data -c "SELECT 1;"

# 3. V√©rifier r√©seau Docker
docker network inspect projet_git_default

# 4. Utiliser IP au lieu de nom si probl√®me DNS
docker inspect chu_postgres | grep IPAddress
# Remplacer chu_postgres par l'IP dans URI
```

### Schema "gold" invisible

**Sympt√¥me**: Pas de tables visibles dans SQL Lab

**Solutions**:

1. **V√©rifier schema existe**:
```bash
docker exec chu_postgres psql -U admin -d healthcare_data -c "\dt gold.*"
```

2. **Dans Superset UI**:
   - Data ‚Üí Databases ‚Üí CHU_Gold ‚Üí Edit
   - Advanced ‚Üí SQL Lab
   - Cocher: "Expose database in SQL Lab"
   - Cocher: "Allow data upload"
   - **Schema**: Taper manuellement `gold`
   - Save

3. **Refresh metadata**:
   - SQL Lab ‚Üí SQL Editor
   - Database: CHU_Gold
   - Schema: gold (s√©lectionner dans dropdown)
   - Cliquer sur refresh icon

### Dataset creation fails

**Sympt√¥me**: Erreur lors de cr√©ation dataset

**Solutions**:

```bash
# 1. V√©rifier permissions PostgreSQL
docker exec chu_postgres psql -U admin -d healthcare_data -c "
GRANT USAGE ON SCHEMA gold TO admin;
GRANT SELECT ON ALL TABLES IN SCHEMA gold TO admin;
"

# 2. Tester query manuellement
# SQL Lab ‚Üí Ex√©cuter:
SELECT COUNT(*) FROM gold.fait_consultation;

# 3. Cr√©er dataset via SQL Lab
# Apr√®s query r√©ussie: Save ‚Üí Save dataset

# 4. V√©rifier table existe et non vide
docker exec chu_postgres psql -U admin -d healthcare_data -c "
SELECT COUNT(*) FROM gold.fait_consultation;
"
```

### Login √©choue (admin/admin)

**Sympt√¥me**: "Invalid login" avec admin/admin123

**Solutions**:

```bash
# 1. R√©initialiser admin password
docker exec chu_superset superset fab create-admin \
    --username admin \
    --firstname Admin \
    --lastname User \
    --email admin@admin.com \
    --password admin123

# 2. Red√©marrer Superset
docker restart chu_superset

# 3. Attendre 30 secondes et r√©essayer

# 4. Si toujours bloqu√©, reset database Superset
docker exec chu_superset superset db upgrade
docker exec chu_superset superset init
```

---

## üîß Erreurs de Pipeline

### Erreur "Column not found"

**Sympt√¥me**: `[UNRESOLVED_COLUMN] A column or function parameter with name 'xxx' cannot be resolved`

**Solutions**:

```python
# 1. Afficher schema du DataFrame
df.printSchema()
df.columns  # Liste tous les noms de colonnes

# 2. V√©rifier casse (sensible!)
# "date_deces" ‚â† "Date_Deces" ‚â† "DATE_DECES"

# 3. V√©rifier espaces dans nom
# "date_deces" ‚â† "date_deces " (espace √† la fin)

# 4. Utiliser backticks pour noms avec espaces
df.select(col("`Date Deces`"))

# 5. Renommer colonne si n√©cessaire
df = df.withColumnRenamed("Date_Deces", "date_deces")
```

**Exemple r√©el** (erreur colonne d√©c√®s):
```python
# ‚ùå AVANT (incorrect):
df = df.filter(col("datdec").startswith("2019"))

# ‚úÖ APR√àS (correct):
df.printSchema()  # Montre: date_deces (pas datdec!)
df = df.filter(col("date_deces").startswith("2019"))
```

### Erreur "File not found"

**Sympt√¥me**: `FileNotFoundError: Path does not exist: /path/to/file`

**Solutions**:

```python
# 1. V√©rifier chemin existe
import os
path = "/home/jovyan/data/bronze/postgres/Patient"
print(f"Exists: {os.path.exists(path)}")

# 2. Lister contenu dossier parent
!ls -la /home/jovyan/data/bronze/postgres/

# 3. V√©rifier variable path
print(f"SILVER_BASE = {SILVER_BASE}")
print(f"GOLD_OUTPUT = {GOLD_OUTPUT}")

# 4. Utiliser chemin absolu
df = spark.read.parquet("/home/jovyan/data/bronze/postgres/Patient")
```

### Jointure retourne 0 lignes

**Sympt√¥me**: Join DataFrame result est vide

**Solutions**:

```python
# 1. V√©rifier donn√©es dans chaque DF avant join
print(f"DF1 count: {df1.count()}")
print(f"DF2 count: {df2.count()}")
df1.show(5)
df2.show(5)

# 2. V√©rifier types colonnes join
df1.select("id_column").dtypes
df2.select("id_column").dtypes

# 3. Tester join avec left pour voir non-matches
df_left = df1.join(df2, "id_column", "left")
df_left.filter(col("df2_column").isNull()).show()

# 4. Convertir types si n√©cessaire
df1 = df1.withColumn("id_column", col("id_column").cast("integer"))
df2 = df2.withColumn("id_column", col("id_column").cast("integer"))
```

### Dates invalides

**Sympt√¥me**: `DateTimeException: Text '...' could not be parsed`

**Solutions**:

```python
# 1. V√©rifier format date dans donn√©es
df.select("date_column").show(10, truncate=False)

# 2. Utiliser to_date avec format explicite
from pyspark.sql.functions import to_date

# Format dd/MM/yyyy
df = df.withColumn("date_clean", to_date(col("date_string"), "dd/MM/yyyy"))

# Format yyyy-MM-dd
df = df.withColumn("date_clean", to_date(col("date_string"), "yyyy-MM-dd"))

# 3. Filtrer dates nulles apr√®s conversion
df = df.filter(col("date_clean").isNotNull())

# 4. Gestion erreurs avec coalesce
from pyspark.sql.functions import coalesce, lit
df = df.withColumn("date_safe",
    coalesce(
        to_date(col("date_string"), "dd/MM/yyyy"),
        to_date(col("date_string"), "yyyy-MM-dd"),
        lit(None)
    )
)
```

### Hash SHA-256 √©choue

**Sympt√¥me**: `TypeError: 'NoneType' object is not iterable` dans hash

**Solutions**:

```python
# 1. G√©rer valeurs NULL
from pyspark.sql.functions import when, col
import hashlib

def hash_with_null_check(value, salt):
    if value is None:
        return None
    return hashlib.sha256((str(value) + salt).encode()).hexdigest()

hash_udf = udf(hash_with_null_check, StringType())

# 2. OU filtrer NULL avant hash
df = df.filter(col("nom").isNotNull())
df = df.withColumn("nom_hash", hash_udf(col("nom"), lit(SALT)))

# 3. OU utiliser fonction Spark native (plus rapide)
from pyspark.sql.functions import sha2, concat
df = df.withColumn("nom_hash",
    sha2(concat(col("nom"), lit(SALT)), 256)
)
```

---

## üöÄ Performances

### Pipeline trop lent

**Sympt√¥me**: Notebook prend 10+ minutes

**Solutions**:

```python
# 1. Filtrer t√¥t dans pipeline
df = spark.read.parquet(path) \
    .filter(col("annee") >= 2019)  # Avant transformations

# 2. Limiter partitions
df = df.coalesce(4)  # R√©duire de 200 √† 4 partitions

# 3. Utiliser broadcast pour petites tables (<10MB)
from pyspark.sql.functions import broadcast
df_result = df_large.join(broadcast(df_small), "id")

# 4. Persister DataFrames r√©utilis√©s
df.cache()
df.count()  # Trigger cache
# ... utiliser df plusieurs fois ...
df.unpersist()  # Lib√©rer cache apr√®s

# 5. D√©sactiver statistics si non n√©cessaires
spark.conf.set("spark.sql.statistics.histogram.enabled", "false")
```

### √âcriture Parquet lente

**Sympt√¥me**: `.write.parquet()` prend plusieurs minutes

**Solutions**:

```python
# 1. R√©duire nombre de partitions avant √©criture
df = df.coalesce(5)  # 5 fichiers parquet au lieu de 200

# 2. Utiliser compression efficace
df.write \
    .mode("overwrite") \
    .option("compression", "snappy") \  # Plus rapide que gzip
    .parquet(path)

# 3. Partitionner seulement si n√©cessaire
# ‚ùå Trop de partitions:
df.write.partitionBy("annee", "mois", "jour").parquet(path)  # 365+ dossiers

# ‚úÖ Partitions raisonnables:
df.write.partitionBy("annee").parquet(path)  # ~10 dossiers

# 4. Repartitionner avant write
df = df.repartition("annee")  # Groupe par partition key
df.write.partitionBy("annee").parquet(path)
```

### PostgreSQL export timeout

**Sympt√¥me**: `JDBC write` timeout apr√®s 10+ minutes

**Solutions**:

```python
# 1. R√©duire batchsize
jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
properties = {
    "user": "admin",
    "password": "admin123",
    "batchsize": "1000"  # Default: 10000
}
df.write.jdbc(jdbc_url, "gold.table_name", mode="overwrite", properties=properties)

# 2. Augmenter timeout
properties = {
    "user": "admin",
    "password": "admin123",
    "socketTimeout": "600"  # 10 minutes
}

# 3. √âcrire en chunks
def write_chunks(df, table_name, chunk_size=100000):
    total = df.count()
    for offset in range(0, total, chunk_size):
        chunk = df.limit(chunk_size).offset(offset)
        mode = "overwrite" if offset == 0 else "append"
        chunk.write.jdbc(jdbc_url, table_name, mode=mode, properties=properties)

# 4. Utiliser mode append avec truncate
df.write \
    .mode("overwrite") \
    .option("truncate", "true") \
    .jdbc(jdbc_url, table_name, properties=properties)
```

### M√©moire RAM satur√©e

**Sympt√¥me**: Docker Desktop consomme 100% RAM, syst√®me lent

**Solutions**:

```bash
# 1. R√©duire m√©moire allou√©e √† Docker
# Docker Desktop ‚Üí Settings ‚Üí Resources
# RAM: 6GB au lieu de 12GB

# 2. Limiter workers Spark
# docker-compose.yml: Supprimer chu_spark_worker

# 3. R√©duire cache Spark dans notebooks
spark = SparkSession.builder \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .config("spark.sql.shuffle.partitions", "20") \
    .getOrCreate()

# 4. Clear cache r√©guli√®rement
spark.catalog.clearCache()

# 5. Utiliser mode local[2] au lieu cluster
spark = SparkSession.builder.master("local[2]").getOrCreate()
```

---

## üÜò Dernier Recours

### R√©initialisation compl√®te

**‚ö†Ô∏è ATTENTION: Supprime toutes les donn√©es!**

```bash
# 1. Arr√™ter tous containers
docker-compose down

# 2. Supprimer volumes
docker-compose down -v

# 3. Supprimer images (optionnel)
docker rmi $(docker images -q)

# 4. Nettoyer syst√®me Docker
docker system prune -a --volumes

# 5. Red√©marrer from scratch
docker-compose up -d

# 6. Re-ex√©cuter notebooks 01 ‚Üí 02 ‚Üí 03 ‚Üí 06
```

### Logs d√©taill√©s

```bash
# Tous logs d'un container
docker logs chu_jupyter > jupyter.log 2>&1

# Suivre logs en temps r√©el
docker logs -f chu_superset

# Logs avec timestamps
docker logs --timestamps chu_postgres

# Derni√®res 100 lignes
docker logs --tail 100 chu_spark_master
```

### Support et Ressources

- **Docker**: https://docs.docker.com/
- **Spark**: https://spark.apache.org/docs/latest/
- **PostgreSQL**: https://www.postgresql.org/docs/
- **Superset**: https://superset.apache.org/docs/
- **Stack Overflow**: Rechercher erreurs sp√©cifiques

---

**üîß Guide Troubleshooting Complet - Bonne chance !**
