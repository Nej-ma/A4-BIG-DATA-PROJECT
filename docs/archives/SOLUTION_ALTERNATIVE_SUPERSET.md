# üéØ SOLUTION ALTERNATIVE - Superset sans Hive

**Probl√®me** : Les tables Hive avec `PARTITIONED BY` + `convertMetastoreParquet=false` causent des erreurs de lecture.

**Solution** : Utiliser **PostgreSQL** ou **Trino** comme passerelle vers les fichiers Parquet.

---

## ‚úÖ OPTION 1 : PostgreSQL + Foreign Data Wrapper (RECOMMAND√â)

### Pourquoi ?

- ‚úÖ PostgreSQL est d√©j√† dans le stack
- ‚úÖ Superset a un driver natif PostgreSQL
- ‚úÖ Pas besoin d'installer de drivers
- ‚úÖ Performant avec `parquet_fdw`

### √âtape 1 : Cr√©er des vues mat√©rialis√©es dans PostgreSQL

Depuis Jupyter, ex√©cuter ce code dans une nouvelle cellule :

```python
from sqlalchemy import create_engine
import pandas as pd

# Connexion PostgreSQL
engine = create_engine('postgresql://postgres:postgres@postgres:5432/hopital')

# Lire les fichiers Parquet
dim_temps = spark.read.parquet("/home/jovyan/data/gold/dim_temps").toPandas()
dim_patient = spark.read.parquet("/home/jovyan/data/gold/dim_patient").toPandas()
dim_diagnostic = spark.read.parquet("/home/jovyan/data/gold/dim_diagnostic").toPandas()
dim_professionnel = spark.read.parquet("/home/jovyan/data/gold/dim_professionnel").toPandas()
dim_etablissement = spark.read.parquet("/home/jovyan/data/gold/dim_etablissement").toPandas()

# √âchantillon de consultations (ou toutes si assez de RAM)
fait_consultation = spark.read.parquet("/home/jovyan/data/gold/fait_consultation") \
    .limit(100000).toPandas()  # Limiter √† 100K pour test

# √âcrire dans PostgreSQL
dim_temps.to_sql('gold_dim_temps', engine, schema='public', if_exists='replace', index=False)
dim_patient.to_sql('gold_dim_patient', engine, schema='public', if_exists='replace', index=False)
dim_diagnostic.to_sql('gold_dim_diagnostic', engine, schema='public', if_exists='replace', index=False)
dim_professionnel.to_sql('gold_dim_professionnel', engine, schema='public', if_exists='replace', index=False)
dim_etablissement.to_sql('gold_dim_etablissement', engine, schema='public', if_exists='replace', index=False)
fait_consultation.to_sql('gold_fait_consultation', engine, schema='public', if_exists='replace', index=False)

print("‚úÖ Tables Gold export√©es vers PostgreSQL")
```

### √âtape 2 : Connecter Superset √† PostgreSQL

Dans Superset :

1. **Settings** ‚Üí **Database Connections** ‚Üí **+ DATABASE**
2. **SUPPORTED DATABASES** ‚Üí **PostgreSQL**
3. **SQLALCHEMY URI** :
   ```
   postgresql://postgres:postgres@postgres:5432/hopital
   ```
4. **TEST CONNECTION** ‚Üí **CONNECT**

### √âtape 3 : Utiliser SQL Lab

Les tables `gold_*` sont maintenant disponibles dans Superset via PostgreSQL !

```sql
SELECT
    t.annee,
    COUNT(*) as nb_consultations
FROM gold_fait_consultation f
JOIN gold_dim_temps t ON f.id_temps = t.id_temps
GROUP BY t.annee
ORDER BY t.annee
```

---

## ‚úÖ OPTION 2 : Utiliser DuckDB (ULTRA RAPIDE)

### Pourquoi ?

- ‚úÖ DuckDB peut lire Parquet directement
- ‚úÖ Performant (moteur OLAP)
- ‚úÖ Support SQLAlchemy

### √âtape 1 : Installer DuckDB

```bash
docker exec chu_superset pip install duckdb duckdb-engine
docker restart chu_superset
```

### √âtape 2 : Cr√©er fichier DuckDB avec vues

Dans Jupyter, cr√©er une nouvelle cellule :

```python
import duckdb

# Cr√©er base DuckDB
conn = duckdb.connect('/home/jovyan/data/chu_gold.duckdb')

# Cr√©er vues sur les fichiers Parquet
conn.execute("""
    CREATE OR REPLACE VIEW dim_temps AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/dim_temps/**/*.parquet', hive_partitioning=true)
""")

conn.execute("""
    CREATE OR REPLACE VIEW dim_patient AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/dim_patient/*.parquet')
""")

conn.execute("""
    CREATE OR REPLACE VIEW dim_diagnostic AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/dim_diagnostic/*.parquet')
""")

conn.execute("""
    CREATE OR REPLACE VIEW dim_professionnel AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/dim_professionnel/*.parquet')
""")

conn.execute("""
    CREATE OR REPLACE VIEW dim_etablissement AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/dim_etablissement/*.parquet')
""")

conn.execute("""
    CREATE OR REPLACE VIEW fait_consultation AS
    SELECT * FROM read_parquet('/home/jovyan/data/gold/fait_consultation/**/*.parquet', hive_partitioning=true)
""")

conn.close()
print("‚úÖ Vues DuckDB cr√©√©es")
```

### √âtape 3 : Connecter Superset √† DuckDB

Dans Superset :

1. **Settings** ‚Üí **Database Connections** ‚Üí **+ DATABASE**
2. **Other** (DuckDB pas dans la liste)
3. **SQLALCHEMY URI** :
   ```
   duckdb:////home/jovyan/data/chu_gold.duckdb
   ```
4. **TEST CONNECTION** ‚Üí **CONNECT**

---

## ‚úÖ OPTION 3 : Simplifier les tables Hive (QUICK FIX)

### Recr√©er les tables SANS partitionnement

Dans Notebook 05, modifier la cellule 4 :

```python
tables_definitions = {
    "dim_temps": """
        CREATE EXTERNAL TABLE dim_temps (
            id_temps STRING,
            date_complete DATE,
            annee INT,
            mois INT,
            jour INT,
            jour_semaine INT,
            nom_jour STRING,
            trimestre INT,
            semaine_annee INT
        )
        STORED AS PARQUET
        LOCATION '/home/jovyan/data/gold/dim_temps'
        TBLPROPERTIES('parquet.compression'='SNAPPY')
    """,

    "fait_consultation": """
        CREATE EXTERNAL TABLE fait_consultation (
            id_consultation STRING,
            id_temps STRING,
            id_patient STRING,
            code_diag STRING,
            id_prof STRING,
            cout DECIMAL(10,2),
            duree_minutes INT,
            urgence BOOLEAN,
            annee INT,
            mois INT
        )
        STORED AS PARQUET
        LOCATION '/home/jovyan/data/gold/fait_consultation'
        TBLPROPERTIES('parquet.compression'='SNAPPY')
    """,
    # ... autres tables inchang√©es
}
```

**MAIS** : Cela n√©cessite de **r√©√©crire les fichiers Parquet** dans Notebook 03 sans partitionnement.

---

## üéØ RECOMMANDATION

**Pour ta soutenance MAINTENANT** : **OPTION 1 (PostgreSQL)**

### Pourquoi ?

1. ‚úÖ **Rapide** : 5 minutes pour exporter vers PostgreSQL
2. ‚úÖ **Fonctionne √† coup s√ªr** : Driver natif Superset
3. ‚úÖ **Pas de config Docker** : Tout dans Jupyter
4. ‚úÖ **D√©mo fonctionnelle** : Workflow E2E valid√©

### Code complet Option 1

```python
# DANS JUPYTER - Nouvelle cellule apr√®s le Notebook 05

from sqlalchemy import create_engine

# Connexion PostgreSQL
engine = create_engine('postgresql://postgres:postgres@postgres:5432/hopital')

print("üìä Export Gold Layer vers PostgreSQL pour Superset...\n")

# Lire et exporter chaque table
tables = {
    "gold_dim_temps": "/home/jovyan/data/gold/dim_temps",
    "gold_dim_patient": "/home/jovyan/data/gold/dim_patient",
    "gold_dim_diagnostic": "/home/jovyan/data/gold/dim_diagnostic",
    "gold_dim_professionnel": "/home/jovyan/data/gold/dim_professionnel",
    "gold_dim_etablissement": "/home/jovyan/data/gold/dim_etablissement",
    "gold_fait_consultation": "/home/jovyan/data/gold/fait_consultation"
}

for table_name, parquet_path in tables.items():
    print(f"  Exportation {table_name}...", end="")

    # Lire Parquet
    df = spark.read.parquet(parquet_path)

    # Limiter fait_consultation si trop gros (optionnel)
    if "consultation" in table_name:
        df = df.limit(500000)  # 500K lignes suffisent pour demo

    # Convertir en Pandas
    pdf = df.toPandas()

    # √âcrire dans PostgreSQL
    pdf.to_sql(table_name, engine, schema='public', if_exists='replace', index=False)

    print(f" ‚úÖ {len(pdf):,} lignes")

print("\n‚úÖ Export termin√© ! Connecter Superset √† PostgreSQL maintenant.")
```

### Puis dans Superset

```
Settings ‚Üí + DATABASE ‚Üí PostgreSQL
URI: postgresql://postgres:postgres@postgres:5432/hopital
TEST CONNECTION ‚Üí CONNECT
```

### SQL Lab

```sql
SELECT
    t.annee,
    COUNT(*) as nb_consultations,
    COUNT(DISTINCT f.id_patient) as patients_uniques
FROM gold_fait_consultation f
JOIN gold_dim_temps t ON f.id_temps = t.id_temps
GROUP BY t.annee
ORDER BY t.annee
```

---

## üìä COMPARAISON OPTIONS

| Option | Temps setup | Performance | Complexit√© | Pour soutenance |
|--------|-------------|-------------|------------|-----------------|
| **PostgreSQL** | 5 min | Moyenne | Facile | ‚úÖ OUI |
| DuckDB | 10 min | Excellente | Moyenne | ‚úÖ OK |
| Hive sans partition | 20 min | Moyenne | Difficile | ‚ùå Non |
| Trino | 30 min | Excellente | Difficile | ‚ùå Non |

---

## üöÄ ACTION IMM√âDIATE

**Copie-colle ce code dans Jupyter** (nouvelle cellule) :

```python
from sqlalchemy import create_engine

engine = create_engine('postgresql://postgres:postgres@postgres:5432/hopital')

print("üìä Export rapide Gold Layer ‚Üí PostgreSQL\n")

# Dimensions (petites, on charge tout)
for name, path in [
    ("gold_dim_temps", "/home/jovyan/data/gold/dim_temps"),
    ("gold_dim_patient", "/home/jovyan/data/gold/dim_patient"),
    ("gold_dim_diagnostic", "/home/jovyan/data/gold/dim_diagnostic"),
    ("gold_dim_professionnel", "/home/jovyan/data/gold/dim_professionnel"),
    ("gold_dim_etablissement", "/home/jovyan/data/gold/dim_etablissement"),
]:
    df = spark.read.parquet(path).toPandas()
    df.to_sql(name, engine, if_exists='replace', index=False)
    print(f"‚úÖ {name}: {len(df):,} lignes")

# Fait (on limite √† 200K pour la d√©mo)
df_fait = spark.read.parquet("/home/jovyan/data/gold/fait_consultation").limit(200000).toPandas()
df_fait.to_sql("gold_fait_consultation", engine, if_exists='replace', index=False)
print(f"‚úÖ gold_fait_consultation: {len(df_fait):,} lignes")

print("\nüéâ TERMIN√â ! Connecter Superset √† PostgreSQL maintenant.")
```

**Ex√©cute** ‚Üí Attends 2-3 minutes ‚Üí Superset !

---

**üéØ C'est la solution la plus rapide pour avoir Superset fonctionnel MAINTENANT !**
