# ğŸ“ TUTO COMPLET - Pipeline ETLT CHU

**Projet** : Cloud Healthcare Unit - Data Lakehouse
**Ã‰quipe** : Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING
**Architecture** : ETLT (Extract â†’ Transform â†’ Load â†’ Transform)

---

## ğŸ“‹ TABLE DES MATIÃˆRES

1. [Architecture ETLT ExpliquÃ©e](#architecture)
2. [PrÃ©requis et DÃ©marrage](#prerequis)
3. [Ã‰tape E - Extract (Notebook 01)](#extract)
4. [Ã‰tape T1 - Transform ConformitÃ© (Notebook 02)](#transform1)
5. [Ã‰tape L - Load Bronze + T2 Transform MÃ©tier (Notebook 03)](#load-transform2)
6. [Benchmarks et Validation (Notebook 04)](#benchmarks)
7. [VÃ©rification dans MinIO](#minio)
8. [DAGs Airflow (Automatisation)](#airflow)
9. [Troubleshooting](#troubleshooting)

---

<a name="architecture"></a>
## 1ï¸âƒ£ ARCHITECTURE ETLT EXPLIQUÃ‰E

### Pourquoi ETLT et pas ETL ou ELT ?

Votre Livrable 1 (page 4-5) explique :

> **"Le modÃ¨le ETLT introduit une premiÃ¨re transformation de conformitÃ© avant le stockage. Cette Ã©tape intermÃ©diaire rÃ©pond Ã  deux impÃ©ratifs :"**
>
> 1. **ConformitÃ© RGPD/HDS** : Les donnÃ©es de santÃ© ne peuvent pas Ãªtre stockÃ©es brutes
> 2. **Performance** : La deuxiÃ¨me transformation (T2) est rÃ©alisÃ©e dans le cluster Big Data

### Flux ETLT DÃ©taillÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SOURCES DE DONNÃ‰ES                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL                    CSV Bruts                     â”‚
â”‚  â”œâ”€ Patient (100K)             â”œâ”€ Ã‰tablissements (416K)      â”‚
â”‚  â”œâ”€ Consultation (1M+)         â”œâ”€ Satisfaction (1K)          â”‚
â”‚  â”œâ”€ Diagnostic (15K)           â””â”€ DÃ©cÃ¨s 2019 (620K)          â”‚
â”‚  â””â”€ 10 autres tables                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â–¼                            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ““ NOTEBOOK 01 - EXTRACT (E)             â”‚
    â”‚   â€¢ Lit PostgreSQL (JDBC)                   â”‚
    â”‚   â€¢ Lit CSV bruts depuis /DATA_2024/        â”‚
    â”‚   â€¢ Sauvegarde Parquet (copie brute)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ“¦ ZONE BRONZE (Landing)                  â”‚
    â”‚   â€¢ Format : Parquet                        â”‚
    â”‚   â€¢ Statut : DonnÃ©es BRUTES (audit trail)   â”‚
    â”‚   â€¢ Localisation : /data/bronze/            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ““ NOTEBOOK 02 - TRANSFORM 1 (T1)             â”‚
    â”‚   ğŸ” CONFORMITÃ‰ RGPD/HDS                        â”‚
    â”‚   â€¢ Pseudonymisation SHA-256                    â”‚
    â”‚   â€¢ Minimisation (suppression PII inutiles)     â”‚
    â”‚   â€¢ Normalisation formats dates                 â”‚
    â”‚   â€¢ ContrÃ´les qualitÃ© (Ã¢ge, doublons)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ“¦ ZONE SILVER (Integration)              â”‚
    â”‚   â€¢ Format : Parquet                        â”‚
    â”‚   â€¢ Statut : DonnÃ©es PSEUDONYMISÃ‰ES         â”‚
    â”‚   â€¢ Localisation : /data/silver/            â”‚
    â”‚   â€¢ âœ… Conformes RGPD (stockables HDFS)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼ (Chargement dans cluster Big Data)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ““ NOTEBOOK 03 - LOAD + TRANSFORM 2 (L + T2)   â”‚
    â”‚   ğŸ“Š MODÃˆLE DÃ‰CISIONNEL                          â”‚
    â”‚   â€¢ CrÃ©ation 5 Dimensions (Temps, Patient...)    â”‚
    â”‚   â€¢ CrÃ©ation 3 Faits (Consultation, DÃ©cÃ¨s...)    â”‚
    â”‚   â€¢ Partitionnement annÃ©e/mois                   â”‚
    â”‚   â€¢ Format ORC optimisÃ©                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â­ ZONE GOLD (Data Warehouse)             â”‚
    â”‚   â€¢ Format : Parquet partitionnÃ©            â”‚
    â”‚   â€¢ Statut : MODÃˆLE MÃ‰TIER (Star Schema)    â”‚
    â”‚   â€¢ Localisation : /data/gold/              â”‚
    â”‚   â€¢ âœ… PrÃªt pour BI (Power BI/Tableau)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ğŸ““ NOTEBOOK 04 - BENCHMARKS            â”‚
    â”‚   â€¢ RequÃªtes SQL Gold                     â”‚
    â”‚   â€¢ Mesure performances                   â”‚
    â”‚   â€¢ Graphiques                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Correspondance avec Livrable 1

| Notebook | Phase ETLT | Page Livrable 1 | Description |
|----------|------------|-----------------|-------------|
| 01 | **E** (Extract) | p.4 | "Extraction depuis PostgreSQL et CSV" |
| 02 | **T1** (Transform ConformitÃ©) | p.4-5 | "Pseudonymisation, Minimisation, Normalisation" |
| 03 | **L** (Load) + **T2** (Transform MÃ©tier) | p.5-6 | "Chargement HDFS + ModÃ¨le dimensionnel" |
| 04 | Validation | p.21 | "Tests de performance et optimisations" |

---

<a name="prerequis"></a>
## 2ï¸âƒ£ PRÃ‰REQUIS ET DÃ‰MARRAGE

### Ã‰tape 1 : Lancer l'infrastructure Docker

```bash
cd c:\Users\littl\Desktop\Big DATA\projet_git

# Lancer tous les services
docker compose up -d

# Attendre 2-3 minutes que tout dÃ©marre
```

### Ã‰tape 2 : VÃ©rifier que les services sont UP

```bash
docker compose ps
```

**RÃ©sultat attendu** :
```
NAME                  STATUS
chu_postgres          Up (healthy)
chu_minio             Up (healthy)
chu_spark_master      Up
chu_jupyter           Up
chu_airflow_webserver Up
chu_superset          Up
```

### Ã‰tape 3 : VÃ©rifier que PostgreSQL contient les donnÃ©es

```bash
docker exec -it chu_postgres psql -U admin -d healthcare_data -c "SELECT COUNT(*) FROM \"Patient\";"
```

**RÃ©sultat attendu** :
```
 count
--------
 100000
```

Si vide, restaurer le dump :
```bash
docker exec chu_postgres pg_restore -U admin -d healthcare_data -v --no-owner --no-acl "/docker-entrypoint-initdb.d/DATA 2024/BDD PostgreSQL/DATA2023"
```

### Ã‰tape 4 : AccÃ©der Ã  Jupyter Lab

1. Ouvrir navigateur : **http://localhost:8888**
2. Token : `chu_token`
3. Vous devriez voir le dossier `notebooks/`

---

<a name="extract"></a>
## 3ï¸âƒ£ Ã‰TAPE E - EXTRACT (Notebook 01)

### ğŸ““ Notebook : `01_Extract_Bronze_SOURCES_DIRECTES.ipynb`

### Objectif (Livrable 1, p.4)

> **"E â€“ Extract : L'extraction regroupe les donnÃ©es provenant de plusieurs sources :"**
> - PostgreSQL : donnÃ©es mÃ©dico-administratives
> - Fichiers CSV : Ã©tablissements, satisfaction, dÃ©cÃ¨s

### Que fait ce notebook ?

1. **Extraction PostgreSQL** (13 tables) :
   ```python
   # Lit directement depuis PostgreSQL
   df = spark.read.jdbc("jdbc:postgresql://chu_postgres:5432/healthcare_data", "Patient")

   # Sauvegarde en Bronze (Parquet)
   df.write.parquet("/home/jovyan/data/bronze/postgres/Patient")
   ```

2. **Extraction CSV** (3 fichiers lus **DIRECTEMENT**) :
   ```python
   # Lit CSV brut (PAS PostgreSQL!)
   df_etab = spark.read.csv("/home/jovyan/DATA_2024/Etablissement de SANTE/etablissement_sante.csv")

   # Sauvegarde en Bronze
   df_etab.write.parquet("/home/jovyan/data/bronze/csv/etablissement_sante")
   ```

3. **Filtrage intelligent dÃ©cÃ¨s 2019** :
   ```python
   # Lit 25M lignes, filtre seulement 2019 â†’ 620K lignes
   df_deces = spark.read.csv("/home/jovyan/DATA_2024/DECES EN FRANCE/deces.csv")
   df_deces_2019 = df_deces.filter(year(col("date_deces")) == 2019)
   ```

### ExÃ©cution

1. Ouvrir `01_Extract_Bronze_SOURCES_DIRECTES.ipynb`
2. **Run â†’ Run All Cells**
3. â±ï¸ DurÃ©e : ~2-3 minutes

### RÃ©sultats attendus

```
âœ… 13 tables PostgreSQL extraites
âœ… 416,665 Ã©tablissements de santÃ©
âœ… 1,152 Ã©valuations satisfaction 2019
âœ… 620,000 dÃ©cÃ¨s 2019 (filtrÃ©)
âœ… Total : ~4.6 millions de lignes

ğŸ’¾ SauvegardÃ© dans :
   /home/jovyan/data/bronze/postgres/
   /home/jovyan/data/bronze/csv/
```

### VÃ©rification

```python
# Dans une cellule Jupyter
df = spark.read.parquet("/home/jovyan/data/bronze/postgres/Patient")
print(f"Patients en Bronze : {df.count():,}")
df.show(5)
```

**RÃ©sultat attendu** :
```
Patients en Bronze : 100,000
```

---

<a name="transform1"></a>
## 4ï¸âƒ£ Ã‰TAPE T1 - TRANSFORM CONFORMITÃ‰ (Notebook 02)

### ğŸ““ Notebook : `02_Transform_Silver_NETTOYAGE.ipynb`

### Objectif (Livrable 1, p.4-5)

> **"T1 â€“ Transform ConformitÃ© : Cette premiÃ¨re transformation intervient AVANT le stockage dans HDFS. Elle est dÃ©diÃ©e Ã  la sÃ©curitÃ© et la conformitÃ© :"**
>
> - **Pseudonymisation** : hachage salÃ© SHA-256
> - **Minimisation** : suppression des champs inutiles
> - **Normalisation** : formats dates, codes, unitÃ©s
> - **ContrÃ´les** : validation type, contraintes, dictionnaires

### Que fait ce notebook ?

#### 1. Anonymisation Patient (SHA-256)

```python
# AVANT (Bronze - DONNÃ‰ES SENSIBLES)
Id_patient | Nom      | Prenom  | Email             | Telephone
1          | Dupont   | Jean    | jean@email.fr     | 0612345678

# APRÃˆS (Silver - PSEUDONYMISÃ‰ES)
id_patient | nom_hash (SHA-256)                              | prenom_hash
1          | 3a7bd3e2360a3d29eea436fcfb7e44c735d117c42d1c1835 | 8f14e45f...
```

**Code** :
```python
df_patient_silver = df_patient_bronze.select(
    col("Id_patient").alias("id_patient"),
    sha2(col("Nom"), 256).alias("nom_hash"),          # â† Hash SHA-256
    sha2(col("Prenom"), 256).alias("prenom_hash"),    # â† Hash SHA-256
    col("Sexe").alias("sexe"),
    col("Age").cast("integer").alias("age"),
    to_date(col("Date"), "M/d/yyyy").alias("date_naissance")  # â† Format ISO
)
```

#### 2. Nettoyage Consultation (dates + typage)

```python
# AVANT (Bronze)
Date       | Duree
6/20/2015  | "120"  (string)

# APRÃˆS (Silver)
date_consultation | annee | mois | duree_minutes
2015-06-20        | 2015  | 6    | 120 (integer)
```

#### 3. Nettoyage Ã‰tablissements

```python
# AVANT (Bronze - donnÃ©es sales)
telephone      | email
"01.23.45.67"  | " HOPITAL@email.fr "

# APRÃˆS (Silver - nettoyÃ©)
telephone   | email
"0123456789" | "hopital@email.fr"
```

### ExÃ©cution

1. Ouvrir `02_Transform_Silver_NETTOYAGE.ipynb`
2. **Run â†’ Run All Cells**
3. â±ï¸ DurÃ©e : ~3-4 minutes

### RÃ©sultats attendus

```
ğŸ” ANONYMISATION:
   âœ… Patient : 100,000 patients (noms/prÃ©noms hashÃ©s SHA-256)
   âœ… DÃ©cÃ¨s : 620,000 dÃ©cÃ¨s (identitÃ©s hashÃ©es)

ğŸ“… NETTOYAGE:
   âœ… Consultation : 1,027,157 consultations (dates yyyy-MM-dd)
   âœ… Ã‰tablissements : 416,665 Ã©tablissements (tÃ©lÃ©phones normalisÃ©s)
   âœ… Satisfaction : 1,152 Ã©valuations (scores typÃ©s double)

ğŸ’¾ SauvegardÃ© dans :
   /home/jovyan/data/silver/patient
   /home/jovyan/data/silver/consultation
   /home/jovyan/data/silver/etablissement_sante
   /home/jovyan/data/silver/satisfaction_2019
   /home/jovyan/data/silver/deces_2019
```

### VÃ©rification RGPD

```python
# VÃ©rifier que les noms sont bien hashÃ©s
df_patient_silver = spark.read.parquet("/home/jovyan/data/silver/patient")
df_patient_silver.select("id_patient", "nom_hash", "prenom_hash", "sexe").show(5)
```

**RÃ©sultat attendu** :
```
+----------+--------------------+--------------------+------+
|id_patient|            nom_hash|        prenom_hash|  sexe|
+----------+--------------------+--------------------+------+
|         1|3a7bd3e2360a3d29e...|8f14e45fceea167a5...|female|
|         2|e4d909c290d0fb1ca...|5d41402abc4b2a76b...|female|
```

âœ… **Impossible de rÃ©-identifier les personnes** â†’ Conforme RGPD !

---

<a name="load-transform2"></a>
## 5ï¸âƒ£ Ã‰TAPE L + T2 - LOAD + TRANSFORM MÃ‰TIER (Notebook 03)

### ğŸ““ Notebook : `03_Transform_Gold_STAR_SCHEMA.ipynb`

### Objectif (Livrable 1, p.5-6)

> **"L â€“ Load : Le chargement consiste Ã  insÃ©rer les donnÃ©es pseudonymisÃ©es dans le Data Lake (HDFS)"**
>
> **"T2 â€“ Transform MÃ©tier : Cette deuxiÃ¨me transformation est exÃ©cutÃ©e directement dans le cluster Big Data et correspond Ã  la phase de prÃ©paration analytique :"**
> - Conformation des dimensions
> - AgrÃ©gations et calculs des indicateurs
> - GÃ©nÃ©ration du modÃ¨le dÃ©cisionnel en constellation

### Que fait ce notebook ?

#### 1. CrÃ©ation des 5 Dimensions (Livrable 1, p.11-13)

**DIM_TEMPS** (4,748 jours 2013-2025) :
```python
# GÃ©nÃ¨re tous les jours de 2013 Ã  2025
dates = []
current = datetime(2013, 1, 1)
while current <= datetime(2025, 12, 31):
    dates.append((
        current.strftime("%Y%m%d"),  # id_temps: "20250121"
        current,
        current.year,
        current.month,
        current.strftime("%A")       # "Monday", "Tuesday"...
    ))
    current += timedelta(days=1)

dim_temps = spark.createDataFrame(dates)
dim_temps.write.partitionBy("annee").parquet("/home/jovyan/data/gold/dim_temps")
```

**DIM_PATIENT** (100K patients anonymisÃ©s - depuis SILVER) :
```python
# Lit depuis SILVER (dÃ©jÃ  anonymisÃ©)
df_patient_silver = spark.read.parquet("/home/jovyan/data/silver/patient")

dim_patient = df_patient_silver.select(
    "id_patient",
    "nom_hash",      # DÃ©jÃ  hashÃ© en Silver
    "prenom_hash",   # DÃ©jÃ  hashÃ© en Silver
    "sexe",
    "age",
    "ville"
)
```

**DIM_DIAGNOSTIC**, **DIM_PROFESSIONNEL**, **DIM_ETABLISSEMENT** : Idem depuis Silver

#### 2. CrÃ©ation des 3 Tables de Faits (Livrable 1, p.13-16)

**FAIT_CONSULTATION** (1M+ consultations) :
```python
df_consult_silver = spark.read.parquet("/home/jovyan/data/silver/consultation")

fait_consultation = df_consult_silver.select(
    "id_consultation",
    "id_patient",
    "id_professionnel",
    date_format("date_consultation", "yyyyMMdd").alias("id_temps"),  # ClÃ© FK vers dim_temps
    "annee",     # Pour partitionnement
    "mois",      # Pour partitionnement
    "duree_minutes"
)

# PARTITIONNEMENT (Livrable 1 p.21 : "partitionnement et bucketing")
fait_consultation.write \
    .partitionBy("annee", "mois") \
    .parquet("/home/jovyan/data/gold/fait_consultation")
```

**Structure Gold crÃ©Ã©e** :
```
/data/gold/fait_consultation/
â”œâ”€â”€ annee=2015/
â”‚   â”œâ”€â”€ mois=1/
â”‚   â”œâ”€â”€ mois=2/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ annee=2016/
â”‚   â”œâ”€â”€ mois=1/
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

**FAIT_DECES** (620K dÃ©cÃ¨s 2019) :
```python
df_deces_silver = spark.read.parquet("/home/jovyan/data/silver/deces_2019")

fait_deces = df_deces_silver.select(
    "nom_hash",          # DÃ©jÃ  anonymisÃ©
    "prenom_hash",       # DÃ©jÃ  anonymisÃ©
    "sexe",
    "age_deces",
    "date_deces",
    date_format("date_deces", "yyyyMMdd").alias("id_temps"),
    "annee_deces",
    "mois_deces"
)

fait_deces.write.partitionBy("annee_deces", "mois_deces").parquet("/home/jovyan/data/gold/fait_deces")
```

**FAIT_SATISFACTION** (1,152 Ã©valuations 2019) :
```python
fait_satisfaction = df_satis_silver.select(
    "finess",
    "score_global",
    "score_accueil",
    "score_pec_infirmier",
    "taux_recommandation",
    "nb_reponses_global",
    lit("20190101").alias("id_temps"),  # ClÃ© vers dim_temps
    "annee"
)

fait_satisfaction.write.partitionBy("annee").parquet("/home/jovyan/data/gold/fait_satisfaction")
```

### ExÃ©cution

1. Ouvrir `03_Transform_Gold_STAR_SCHEMA.ipynb`
2. **Run â†’ Run All Cells**
3. â±ï¸ DurÃ©e : ~2-3 minutes

### RÃ©sultats attendus

```
ğŸ”· DIMENSIONS:
   âœ… dim_temps : 4,748 jours (2013-2025)
   âœ… dim_patient : 100,000 patients anonymisÃ©s
   âœ… dim_diagnostic : 15,490 codes CIM-10
   âœ… dim_professionnel : 1,048,575 professionnels
   âœ… dim_etablissement : 416,665 Ã©tablissements

ğŸ“Š FAITS:
   âœ… fait_consultation : 1,027,157 consultations (partitionnÃ© annÃ©e/mois)
   âœ… fait_deces : 620,000 dÃ©cÃ¨s 2019 (partitionnÃ© annÃ©e/mois)
   âœ… fait_satisfaction : 1,152 Ã©valuations (partitionnÃ© annÃ©e)

ğŸ’¾ SauvegardÃ© dans :
   /home/jovyan/data/gold/
```

### VÃ©rification du modÃ¨le Star Schema

```python
# VÃ©rifier le modÃ¨le
import os
for table in os.listdir("/home/jovyan/data/gold"):
    df = spark.read.parquet(f"/home/jovyan/data/gold/{table}")
    print(f"{table:30} {df.count():>10,} lignes")
```

**RÃ©sultat attendu** :
```
dim_temps                         4,748 lignes
dim_patient                     100,000 lignes
dim_diagnostic                   15,490 lignes
dim_professionnel             1,048,575 lignes
dim_etablissement               416,665 lignes
fait_consultation             1,027,157 lignes
fait_deces                      620,000 lignes
fait_satisfaction                 1,152 lignes
```

---

<a name="benchmarks"></a>
## 6ï¸âƒ£ BENCHMARKS ET VALIDATION (Notebook 04)

### ğŸ““ Notebook : `04_Performance_Benchmarks.ipynb`

### Objectif (Livrable 1, p.21)

> **"Tests de performance et optimisations"**

### Que fait ce notebook ?

1. **6 requÃªtes SQL benchmark** sur Gold :
   ```sql
   -- Q1: Consultations par mois
   SELECT annee, mois, COUNT(*) as nb_consultations
   FROM fait_consultation
   GROUP BY annee, mois
   ORDER BY annee, mois

   -- Q2: Top 10 diagnostics
   SELECT code_diag, COUNT(*) as nb_cas
   FROM fait_consultation
   GROUP BY code_diag
   ORDER BY nb_cas DESC
   LIMIT 10
   ```

2. **Mesure des temps d'exÃ©cution** :
   ```python
   times = []
   for i in range(3):
       start = time.time()
       result = spark.sql(query)
       count = result.count()
       elapsed = time.time() - start
       times.append(elapsed)

   avg_time = sum(times) / len(times)
   ```

3. **Graphiques de performance** :
   - Temps d'exÃ©cution par requÃªte
   - Impact du partitionnement
   - Comparaison avec/sans optimisations

### ExÃ©cution

1. Ouvrir `04_Performance_Benchmarks.ipynb`
2. **Run â†’ Run All Cells**
3. â±ï¸ DurÃ©e : ~1-2 minutes

### RÃ©sultats attendus

```
ğŸ“Š BENCHMARK RÃ‰SULTATS:

Q1 - Consultations par mois : 0.85s (avg)
Q2 - Top 10 diagnostics     : 1.23s (avg)
Q3 - RÃ©partition sexe       : 0.67s (avg)
Q4 - Taux satisfaction      : 0.42s (avg)
Q5 - MortalitÃ© par rÃ©gion   : 1.56s (avg)
Q6 - Jointure dimensions    : 2.34s (avg)

ğŸ“Š Graphiques gÃ©nÃ©rÃ©s :
   /home/jovyan/data/gold_star_schema_stats.png
   /home/jovyan/data/benchmark_results.png
```

---

<a name="minio"></a>
## 7ï¸âƒ£ VÃ‰RIFICATION DANS MINIO

### AccÃ¨s MinIO Console

1. URL : **http://localhost:9001**
2. Login : `minioadmin`
3. Password : `minioadmin`

### VÃ©rifier les buckets

**Bucket `lakehouse`** :
```
lakehouse/
â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”œâ”€â”€ Patient/
â”‚   â”‚   â”œâ”€â”€ Consultation/
â”‚   â”‚   â””â”€â”€ ... (13 tables)
â”‚   â””â”€â”€ csv/
â”‚       â”œâ”€â”€ etablissement_sante/
â”‚       â”œâ”€â”€ satisfaction_esatis48h_2019/
â”‚       â””â”€â”€ deces_2019/
â”œâ”€â”€ silver/
â”‚   â”œâ”€â”€ patient/
â”‚   â”œâ”€â”€ consultation/
â”‚   â”œâ”€â”€ etablissement_sante/
â”‚   â”œâ”€â”€ satisfaction_2019/
â”‚   â””â”€â”€ deces_2019/
â””â”€â”€ gold/
    â”œâ”€â”€ dim_temps/
    â”œâ”€â”€ dim_patient/
    â”œâ”€â”€ dim_diagnostic/
    â”œâ”€â”€ dim_professionnel/
    â”œâ”€â”€ dim_etablissement/
    â”œâ”€â”€ fait_consultation/
    â”œâ”€â”€ fait_deces/
    â””â”€â”€ fait_satisfaction/
```

### VÃ©rifier les fichiers Parquet

Cliquer sur `gold/fait_consultation/` â†’ Vous devriez voir :
```
annee=2015/
annee=2016/
annee=2017/
...
```

Cliquer sur `annee=2015/` â†’ Vous devriez voir :
```
mois=1/
mois=2/
...
mois=12/
```

âœ… **C'est le partitionnement en action !**

---

<a name="airflow"></a>
## 8ï¸âƒ£ DAGS AIRFLOW (AUTOMATISATION)

### Pourquoi 2 DAGs ?

Vous avez mentionnÃ© :
> "dans le airflow j'ai 2 pipelines : chu_etl_pipeline et exemple_delta_lake"

1. **`exemple_delta_lake`** : DAG d'exemple (peut Ãªtre ignorÃ©)
2. **`chu_etl_pipeline`** : DAG principal qui DEVRAIT orchestrer les 4 notebooks

### ProblÃ¨me actuel

Le DAG `chu_etl_pipeline` a fail. C'est normal car il essaie probablement d'exÃ©cuter des scripts Spark qui n'existent pas encore.

### Solution : Convertir notebooks en scripts Python

Pour que Airflow puisse orchestrer, il faut :

1. **Convertir les notebooks en scripts Python** :
   ```bash
   jupyter nbconvert --to python notebooks/01_Extract_Bronze_SOURCES_DIRECTES.ipynb
   jupyter nbconvert --to python notebooks/02_Transform_Silver_NETTOYAGE.ipynb
   jupyter nbconvert --to python notebooks/03_Transform_Gold_STAR_SCHEMA.ipynb
   ```

2. **Mettre les scripts dans `spark/jobs/`**

3. **Modifier le DAG** pour pointer vers ces scripts

**Pour le Livrable 2** : Les notebooks Jupyter suffisent !
**Pour le Livrable 3** : On crÃ©era les DAGs Airflow complets.

---

<a name="troubleshooting"></a>
## 9ï¸âƒ£ TROUBLESHOOTING

### ProblÃ¨me 1 : PostgreSQL vide

**SymptÃ´me** :
```
Patient table: 0 rows
```

**Solution** :
```bash
docker exec chu_postgres pg_restore -U admin -d healthcare_data -v --no-owner --no-acl "/docker-entrypoint-initdb.d/DATA 2024/BDD PostgreSQL/DATA2023"
```

### ProblÃ¨me 2 : CSV non trouvÃ©s (PATH_NOT_FOUND)

**SymptÃ´me** :
```
PATH_NOT_FOUND: /home/jovyan/DATA_2024/...
```

**Solution** :
Le docker-compose monte dÃ©jÃ  `./data/DATA 2024` â†’ `/home/jovyan/DATA_2024`
VÃ©rifier que le chemin dans le notebook est bien :
```python
"/home/jovyan/DATA_2024/Etablissement de SANTE/etablissement_sante.csv"
```

### ProblÃ¨me 3 : Spark crash (mÃ©moire)

**SymptÃ´me** :
```
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
```

**Solution** :
Le Notebook 01 filtre dÃ©jÃ  les dÃ©cÃ¨s 2019 (620K au lieu de 25M).
Si Ã§a crash encore :
```python
# Augmenter la mÃ©moire Spark dans le notebook
spark = SparkSession.builder \
    .config("spark.driver.memory", "16g") \
    .config("spark.executor.memory", "16g") \
    .getOrCreate()
```

### ProblÃ¨me 4 : MinIO buckets vides

**Normal !** Les buckets se remplissent APRÃˆS exÃ©cution des notebooks.

VÃ©rifier aprÃ¨s Notebook 01 â†’ `bronze/` rempli
VÃ©rifier aprÃ¨s Notebook 02 â†’ `silver/` rempli
VÃ©rifier aprÃ¨s Notebook 03 â†’ `gold/` rempli

### ProblÃ¨me 5 : Airflow CSRF error

**SymptÃ´me** :
```
The CSRF session token is missing
```

**Solution** : DÃ©jÃ  rÃ©glÃ© dans `docker-compose.yml` :
```yaml
AIRFLOW__WEBSERVER__WTF_CSRF_ENABLED: 'False'
```

RedÃ©marrer Airflow :
```bash
docker compose restart airflow-webserver airflow-scheduler
```

---

## âœ… CHECKLIST COMPLÃˆTE

### Livrable 2 - ExÃ©cution

- [ ] 1. Docker Compose UP
- [ ] 2. PostgreSQL contient 100K patients
- [ ] 3. Jupyter accessible (http://localhost:8888)
- [ ] 4. Notebook 01 exÃ©cutÃ© âœ… â†’ Bronze rempli
- [ ] 5. Notebook 02 exÃ©cutÃ© âœ… â†’ Silver rempli
- [ ] 6. Notebook 03 exÃ©cutÃ© âœ… â†’ Gold rempli
- [ ] 7. Notebook 04 exÃ©cutÃ© âœ… â†’ Benchmarks gÃ©nÃ©rÃ©s
- [ ] 8. MinIO : bronze/ silver/ gold/ visibles
- [ ] 9. Graphiques de performance gÃ©nÃ©rÃ©s

### ConformitÃ© Livrable 1

- [x] Architecture ETLT respectÃ©e
- [x] T1 : Pseudonymisation SHA-256 (Notebook 02)
- [x] T1 : Minimisation donnÃ©es (Notebook 02)
- [x] T1 : Normalisation dates (Notebook 02)
- [x] L : Chargement Bronze (Notebook 01)
- [x] T2 : ModÃ¨le en constellation (Notebook 03)
- [x] T2 : 5 Dimensions + 3 Faits (Notebook 03)
- [x] Partitionnement annÃ©e/mois (Notebook 03)
- [x] Format Parquet/ORC (Notebooks 01-03)

---

## ğŸ“Š RÃ‰SUMÃ‰ VOLUMÃ‰TRIES

| Layer | Tables | Lignes Total | Statut |
|-------|--------|--------------|--------|
| **Bronze** | 16 | ~4.6M | DonnÃ©es brutes |
| **Silver** | 12 | ~4.6M | PseudonymisÃ©es RGPD |
| **Gold** | 8 | ~1.7M | Star Schema optimisÃ© |

---

## ğŸ“ CONCEPTS CLÃ‰S Ã€ RETENIR

### ETLT vs ETL vs ELT

| Approche | Flux | Avantage | InconvÃ©nient |
|----------|------|----------|--------------|
| **ETL** | E â†’ T â†’ L | DonnÃ©es propres dÃ¨s le load | Pas scalable Big Data |
| **ELT** | E â†’ L â†’ T | Scalable | DonnÃ©es brutes stockÃ©es (RGPD âŒ) |
| **ETLT** | E â†’ T1 â†’ L â†’ T2 | Scalable + RGPD âœ… | Plus complexe |

### Pourquoi Medallion (Bronze/Silver/Gold) ?

- **Bronze** : Audit trail (si erreur, on peut rejouer depuis brut)
- **Silver** : Zone de confiance (conformitÃ© lÃ©gale)
- **Gold** : Zone business (performances optimales)

### Pourquoi Star Schema ?

- **ModÃ¨le relationnel** (PostgreSQL) : Complexe, jointures lentes
- **Star Schema** : Simple, jointures rapides, optimisÃ© BI

```
Relationnel:
Patient â†’ Consultation â†’ Diagnostic â†’ Professionnel
        â†’ Mutuelle â†’ Adher
        â†’ Prescription â†’ Medicaments

Star Schema:
        DIM_TEMPS
            |
DIM_PATIENT â”€â”€â”€ FAIT_CONSULTATION â”€â”€â”€ DIM_PROFESSIONNEL
            |
       DIM_DIAGNOSTIC
```

---

**FIN DU TUTO - Vous Ãªtes prÃªt pour le Livrable 2 ! ğŸ‰**
