# ğŸ“ COMPRENDRE LE PROJET - Architecture et Outils

**Projet** : CHU Data Lakehouse
**Auteurs** : Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING

---

## ğŸ—ï¸ ARCHITECTURE COMPLÃˆTE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SOURCES DE DONNÃ‰ES                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL : Base opÃ©rationnelle (patients, consultations) â”‚
â”‚  CSV : Fichiers plats (dÃ©cÃ¨s, satisfaction, Ã©tablissements) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRAITEMENT (ETL)                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Spark : Moteur de calcul distribuÃ© Big Data                â”‚
â”‚  (lecture, transformation, agrÃ©gation des millions de lignes)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STOCKAGE (Data Lakehouse)                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MinIO : Stockage objet (comme AWS S3)                       â”‚
â”‚    â€¢ lakehouse/bronze/  : DonnÃ©es brutes                     â”‚
â”‚    â€¢ lakehouse/silver/  : DonnÃ©es nettoyÃ©es                  â”‚
â”‚    â€¢ lakehouse/gold/    : ModÃ¨le dimensionnel                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ORCHESTRATION                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Airflow : Planification et automatisation des pipelines    â”‚
â”‚  (lance les jobs Spark de maniÃ¨re automatique/programmÃ©e)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYSE & VISUALISATION                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Jupyter : DÃ©veloppement interactif (notebooks)              â”‚
â”‚  Superset : Dashboards BI pour les dÃ©cideurs                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ RÃ”LE DE CHAQUE OUTIL

### 1. **PostgreSQL** ğŸ—„ï¸
**RÃ´le** : Base de donnÃ©es opÃ©rationnelle (OLTP)
**Contient** :
- 100,000 patients
- 1,027,157 consultations
- 13 tables relationnelles
- + Les CSV qu'on a chargÃ©s

**Pourquoi ?** : C'est la source de vÃ©ritÃ©, les donnÃ©es "en production"

---

### 2. **Spark** âš¡
**RÃ´le** : Moteur de calcul Big Data (traitement distribuÃ©)

**Fait quoi concrÃ¨tement ?**
- Lit des millions de lignes rapidement
- Transforme les donnÃ©es (nettoyage, agrÃ©gations)
- Joint plusieurs tables ensemble
- Calcule des statistiques complexes
- Ã‰crit des rÃ©sultats en Parquet/Delta Lake

**Exemple concret** :
```python
# Spark lit 1 million de consultations
df = spark.read.jdbc("postgresql://...", "Consultation")

# Calcule le nombre de consultations par annÃ©e
result = df.groupBy("annee").count()

# Sauvegarde en Parquet dans MinIO
result.write.parquet("s3a://lakehouse/gold/stats")
```

**Pourquoi Spark ?**
- PostgreSQL galÃ¨re avec 1M+ lignes en analyse
- Spark parallÃ©lise le travail â†’ plus rapide
- Spark peut lire Parquet, CSV, JSON, etc.

---

### 3. **Jupyter** ğŸ““
**RÃ´le** : Interface de dÃ©veloppement interactive

**Fait quoi ?**
- Permet d'Ã©crire du code Python/Spark cellule par cellule
- Visualise les rÃ©sultats immÃ©diatement
- CrÃ©e des graphiques (matplotlib, seaborn)
- Documente le code avec du Markdown

**Pourquoi Jupyter ?**
- **Pour le dÃ©veloppement** : tester rapidement les transformations
- **Pour le Livrable 2** : notebooks = code + rÃ©sultats + explications
- **Pour la dÃ©mo** : exÃ©cuter devant le jury

**Alternatives** :
- Script Python classique (moins visuel)
- PySpark en ligne de commande (moins pratique)

---

### 4. **MinIO** ğŸª£
**RÃ´le** : Stockage objet (comme AWS S3, mais local)

**Contient** :
- **lakehouse/** : Votre Data Lakehouse
  - `bronze/` : DonnÃ©es brutes (copie de PostgreSQL + CSV)
  - `silver/` : DonnÃ©es nettoyÃ©es
  - `gold/` : ModÃ¨le dimensionnel (Star Schema)
- **warehouse/** : RÃ©servÃ© pour Spark SQL (tables Hive)

**Pourquoi MinIO ?**
- Stockage illimitÃ© (vs PostgreSQL limitÃ©)
- Format optimisÃ© (Parquet = compression 10x)
- Versioning (Delta Lake garde l'historique)
- Compatible S3 (on peut migrer vers AWS aprÃ¨s)

**Pourquoi vide ?**
- Normal ! Vous n'avez pas encore exÃ©cutÃ© les notebooks
- Les notebooks Ã©crivent dedans quand vous les lancez

---

### 5. **Airflow** ğŸ”„
**RÃ´le** : Orchestrateur de workflows (ETL automatisÃ©)

**Fait quoi ?**
```
Airflow DAG (pipeline) :
  1. Tous les jours Ã  2h du matin
  2. Lance le job Bronze (ingestion PostgreSQL)
  3. Puis lance le job Silver (nettoyage)
  4. Puis lance le job Gold (modÃ¨le dimensionnel)
  5. Envoie un email si erreur
```

**Pourquoi Airflow ?**
- Automatise les pipelines (au lieu de lancer manuellement)
- GÃ¨re les dÃ©pendances (Silver attend Bronze)
- Retry automatique si Ã©chec
- Monitoring visuel

**Pour le Livrable 2** : Pas obligatoire (mais bonus++)
**Pour le Livrable 3** : Probablement demandÃ©

---

### 6. **Superset** ğŸ“Š
**RÃ´le** : Outil de BI (Business Intelligence) / Dashboards

**Fait quoi ?**
- Se connecte aux donnÃ©es (PostgreSQL, Spark SQL)
- CrÃ©e des graphiques interactifs (barres, lignes, cartes)
- Construit des tableaux de bord
- Partage avec les dÃ©cideurs (mÃ©decins, direction)

**Exemple dashboard** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHU Dashboard - Vue d'ensemble     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ˆ Consultations par mois          â”‚
â”‚  ğŸ“Š Top 10 diagnostics              â”‚
â”‚  ğŸ—ºï¸  Carte des dÃ©cÃ¨s par rÃ©gion     â”‚
â”‚  â­ Satisfaction moyenne : 72%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pour le Livrable 2** : Pas obligatoire
**Pour le Livrable 3** : SÃ»rement demandÃ© (storytelling)

---

## ğŸ¯ POUR LE LIVRABLE 2 (CE QU'ON FAIT LÃ€)

### Objectif du Livrable 2 :
**"ModÃ¨le physique et optimisation"**

### Ce qu'il faut livrer :
1. âœ… **Script de crÃ©ation et chargement de donnÃ©es**
   - â†’ Notebooks 01 et 03 (fait !)

2. âœ… **VÃ©rification des donnÃ©es**
   - â†’ Chaque notebook affiche les comptages

3. âœ… **Script de peuplement des tables**
   - â†’ Notebook 01 (Bronze) + Notebook 03 (Gold)

4. âœ… **Partitionnement et buckets**
   - â†’ Notebook 03 (fait_consultation partitionnÃ© par annÃ©e/mois)

5. âœ… **Graphes de performance**
   - â†’ Notebook 04 gÃ©nÃ¨re les PNG

6. âœ… **RequÃªtes de benchmark**
   - â†’ Notebook 04 (6 requÃªtes SQL)

### Outils utilisÃ©s pour Livrable 2 :
- âœ… **PostgreSQL** : Source de donnÃ©es
- âœ… **Spark** : Traitement et transformations
- âœ… **Jupyter** : DÃ©veloppement et documentation
- âœ… **MinIO** : Stockage du lakehouse
- âŒ **Airflow** : Pas obligatoire (mais je vais le rÃ©parer)
- âŒ **Superset** : Pas obligatoire pour L2

---

## ğŸš€ POUR LE LIVRABLE 3 (Suite du projet)

### Objectif probable :
**"PrÃ©sentation des rÃ©sultats et storytelling"**

### Ce qui sera demandÃ© :
1. **Dashboards interactifs** â†’ Superset
2. **Pipelines automatisÃ©s** â†’ Airflow
3. **Analyses complexes** â†’ Jupyter + Spark
4. **Storytelling** â†’ PrÃ©sentation avec graphiques

### Outils qui deviendront importants :
- â­ **Superset** : Dashboards pour la soutenance
- â­ **Airflow** : Montrer l'automatisation
- â­ **Jupyter** : Analyses exploratoires

---

## ğŸ“‹ WORKFLOW COMPLET (Comment Ã§a marche ensemble)

### Workflow Manuel (Livrable 2 - ce qu'on fait maintenant) :

```
1. PostgreSQL (source)
      â†“ Jupyter Notebook 01
2. Spark lit PostgreSQL
      â†“ Spark transforme
3. Spark Ã©crit dans MinIO (Bronze)
      â†“ Jupyter Notebook 03
4. Spark lit Bronze
      â†“ Spark crÃ©e le modÃ¨le Gold
5. Spark Ã©crit dans MinIO (Gold)
      â†“ Jupyter Notebook 04
6. Spark fait des requÃªtes
      â†“ GÃ©nÃ¨re graphiques
7. RÃ©sultats PNG + Rapport
```

### Workflow AutomatisÃ© (Livrable 3 - probable) :

```
1. Airflow DAG dÃ©clenchÃ© (tous les jours 2h)
      â†“
2. Airflow lance Spark Job Bronze
      â†“
3. Airflow lance Spark Job Silver
      â†“
4. Airflow lance Spark Job Gold
      â†“
5. Airflow met Ã  jour Superset
      â†“
6. Dashboards actualisÃ©s automatiquement
```

---

## ğŸ”§ POURQUOI AIRFLOW ET SUPERSET NE MARCHENT PAS ?

**Airflow** : Erreur CSRF (problÃ¨me de sÃ©curitÃ© web)
**Superset** : Probablement mÃªme genre de problÃ¨me

**Pas grave pour le Livrable 2** car :
- Pas obligatoires maintenant
- Je vais les rÃ©parer pour le Livrable 3
- Jupyter suffit largement pour L2

---

## âœ… CE QU'IL FAUT FAIRE MAINTENANT (Livrable 2)

### Ã‰tapes restantes :

1. âœ… **PostgreSQL** : PrÃªt (100K patients + CSV chargÃ©s)

2. â³ **ExÃ©cuter Notebook 01** : Ingestion Bronze
   - Lit PostgreSQL â†’ Ã‰crit MinIO/bronze/
   - DurÃ©e : 5 minutes
   - **RÃ©sultat** : MinIO aura des donnÃ©es !

3. â³ **ExÃ©cuter Notebook 03** : ModÃ¨le Gold
   - Lit Bronze â†’ CrÃ©e Star Schema â†’ Ã‰crit MinIO/gold/
   - DurÃ©e : 15 minutes
   - **C'est le cÅ“ur du Livrable 2** â­

4. â³ **ExÃ©cuter Notebook 04** : Benchmarks
   - Fait des requÃªtes â†’ Mesure temps â†’ Graphiques
   - DurÃ©e : 10 minutes
   - **RÃ©sultat** : PNG + Rapport

5. âœ… **Livrable 2 terminÃ© !**

---

## ğŸ’¡ RÃ‰SUMÃ‰ EN 3 PHRASES

1. **Spark** = cerveau qui calcule (transforme 1M lignes en quelques secondes)
2. **Jupyter** = interface pour coder et voir les rÃ©sultats (pour L2)
3. **MinIO** = disque dur du lakehouse (stocke Bronze/Silver/Gold)
4. **Airflow** = robot qui lance tout automatiquement (pour L3)
5. **Superset** = Ã©cran pour les dÃ©cideurs (dashboards pour L3)

---

**Maintenant je rÃ©pare Airflow et Superset, puis vous pourrez lancer les notebooks !**
