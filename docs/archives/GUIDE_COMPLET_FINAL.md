# ğŸ¯ GUIDE COMPLET FINAL - Livrable 2 CHU Data Lakehouse

**Date** : 2025-10-22
**Statut** : âœ… Tous les scripts corrigÃ©s - PrÃªt pour exÃ©cution

---

## ğŸ“‹ RÃ‰SUMÃ‰ DES CORRECTIONS

### âœ… Corrections appliquÃ©es aux notebooks

1. **Notebook 03 - Cellule 4** : dim_temps SANS partitionnement
2. **Notebook 05 - Cellule 2** : Config `convertMetastoreParquet=false`
3. **Notebook 05 - Cellule 4** : dim_temps SANS `PARTITIONED BY`
4. **Notebook 05 - Cellule 6** : RÃ©paration uniquement fait_consultation
5. **Superset Container** : PyHive installÃ©

### ğŸ”§ ProblÃ¨mes rÃ©solus

- âŒ "Column in converted table has different data type" â†’ âœ… Config Spark corrigÃ©e
- âŒ "Path is a directory, which is not supported" â†’ âœ… Partitionnement supprimÃ©
- âŒ "Could not load database driver" â†’ âœ… PyHive installÃ©
- âŒ Spark SQL multi-statement â†’ âœ… DROP et CREATE sÃ©parÃ©s

---

## ğŸš€ PLAN D'ACTION IMMÃ‰DIAT (12 MINUTES)

### Ã‰TAPE 1 : RÃ©gÃ©nÃ©rer Gold Layer (5 min)

**Objectif** : RecrÃ©er dim_temps SANS partitionnement

1. Ouvrir http://localhost:8888
2. Ouvrir **03_Transform_Gold_STAR_SCHEMA.ipynb**
3. **Kernel** â†’ **Restart Kernel** (important !)
4. **Run** â†’ **Run All Cells**
5. Attendre ~3-4 minutes

**VÃ©rification** : Cellule 4 doit afficher :
```
ğŸ’¾ SauvegardÃ©: /home/jovyan/data/gold/dim_temps (sans partitionnement)
   â„¹ï¸  ChangÃ©: Pas de partitionnement pour compatibilitÃ© Hive/Superset
```

**IMPORTANT** : Le Restart Kernel est crucial pour recrÃ©er les fichiers Parquet correctement.

---

### Ã‰TAPE 2 : CrÃ©er tables Hive (2 min)

**Objectif** : CrÃ©er les 6 tables Spark SQL avec les bons schÃ©mas

1. Ouvrir **05_Setup_Superset.ipynb**
2. **Kernel** â†’ **Restart Kernel** (important !)
3. **Run** â†’ **Run All Cells**
4. Attendre ~1-2 minutes

**VÃ©rification** : Cellule de comptage doit afficher :
```
ğŸ“Š COMPTAGE DES LIGNES
============================================================
  dim_temps                 :      4,748 lignes  âœ…
  dim_patient               :    100,000 lignes  âœ…
  dim_diagnostic            :     15,490 lignes  âœ…
  dim_professionnel         :  1,048,575 lignes  âœ…
  dim_etablissement         :        200 lignes  âœ…
  fait_consultation         :  1,027,157 lignes  âœ…
============================================================
  TOTAL                     :  2,191,422 lignes

âœ… Toutes les tables sont accessibles !
```

**AUCUNE ERREUR** ne doit apparaÃ®tre, notamment pas d'âŒ.

**Tests SQL** : Les 3 tests doivent afficher des rÃ©sultats :
- TEST 1 : 9 lignes (2015-2023)
- TEST 2 : 15 lignes (top diagnostics)
- TEST 3 : 200 lignes (Ã©tablissements)

---

### Ã‰TAPE 3 : Configurer Superset (5 min)

**Objectif** : Connecter Superset Ã  la Gold Layer via Spark Thrift Server

#### 3.1 VÃ©rifier Thrift Server (30 sec)

```bash
docker exec chu_spark_master bash -c "jps | grep -i thrift"
```

**Si rien** : DÃ©marrer le Thrift Server
```bash
docker exec chu_spark_master bash -c '$SPARK_HOME/sbin/start-thriftserver.sh --master spark://spark-master:7077 --hiveconf hive.server2.thrift.port=10000'
```

Attendre 10 secondes.

#### 3.2 VÃ©rifier PyHive (30 sec)

```bash
docker exec chu_superset pip show pyhive
```

**Si erreur** : Installer PyHive
```bash
docker exec chu_superset pip install pyhive
docker restart chu_superset
```

Attendre 20-30 secondes que Superset redÃ©marre.

#### 3.3 Connexion Superset (2 min)

1. Ouvrir http://localhost:8088
2. Login : `admin` / `admin`
3. **Settings** âš™ï¸ â†’ **Database Connections**
4. **+ DATABASE**
5. **SUPPORTED DATABASES** â†’ **Apache Spark SQL**
6. **SQLAlchemy URI** :
   ```
   hive://spark-master:10000/default
   ```
7. **Display Name** : `CHU_Gold_Layer`
8. **ADVANCED** â†’ **Other** â†’ Cocher :
   - âœ… **Expose database in SQL Lab**
   - âœ… **Allow CREATE TABLE AS**
9. **TEST CONNECTION** â†’ âœ… "Connection looks good!"
10. **CONNECT**

---

### Ã‰TAPE 4 : Tester SQL Lab (1 min)

**Objectif** : Valider que Superset peut interroger les tables Hive

1. **SQL Lab** â†’ **SQL Editor**
2. **DATABASE** : `CHU_Gold_Layer`
3. **SCHEMA** : `default`
4. Copier-coller cette requÃªte :

```sql
SELECT
    t.annee,
    COUNT(*) as nb_consultations,
    COUNT(DISTINCT f.id_patient) as patients_uniques
FROM fait_consultation f
JOIN dim_temps t ON f.id_temps = t.id_temps
GROUP BY t.annee
ORDER BY t.annee
```

5. **RUN**

**RÃ©sultat attendu** : 9 lignes (2015-2023)

```
annee | nb_consultations | patients_uniques
------|------------------|------------------
2015  |     33,896       |     28,581
2016  |    184,308       |     85,272
2017  |    119,632       |     69,857
2018  |    113,894       |     66,892
2019  |    113,861       |     66,853
2020  |    113,849       |     66,859
2021  |    113,867       |     66,867
2022  |    113,867       |     66,842
2023  |    113,883       |     66,856
```

âœ… **SI TU VOIS CES RÃ‰SULTATS â†’ WORKFLOW E2E VALIDÃ‰ !**

---

### Ã‰TAPE 5 : CrÃ©er Dashboard (3 min)

**Objectif** : CrÃ©er un dashboard Superset pour dÃ©monstration

#### 5.1 CrÃ©er Dataset (1 min)

1. **Data** â†’ **Datasets** â†’ **+ DATASET**
2. **Database** : `CHU_Gold_Layer`
3. **Schema** : `default`
4. **Table** : `fait_consultation`
5. **ADD**

#### 5.2 CrÃ©er Graphique (1 min)

1. **Charts** â†’ **+ CHART**
2. **Dataset** : `fait_consultation`
3. **Chart Type** : **Big Number**
4. **QUERY** â†’ **Metrics** : `COUNT(*)`
5. **UPDATE CHART**

**RÃ©sultat attendu** : **1,027,157**

#### 5.3 CrÃ©er Dashboard (1 min)

1. **SAVE** (en haut Ã  droite)
2. **Chart Name** : `Total Consultations CHU`
3. **Add to Dashboard** : **Create new dashboard**
   - **Name** : `CHU Overview - Livrable 2`
4. **SAVE & GO TO DASHBOARD**

âœ… **TON DASHBOARD EST CRÃ‰Ã‰ !**

---

## ğŸ“Š ARCHITECTURE E2E COMPLÃˆTE

```
Sources de donnÃ©es
    â”œâ”€ CSV deces.csv (25M lignes)
    â”œâ”€ CSV etablissement_sante.csv (200 lignes)
    â”œâ”€ CSV satisfaction_esatis48h_2019.csv
    â””â”€ PostgreSQL (13 tables)
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    BRONZE LAYER (Notebook 01)
    17 tables | ~29M lignes
    Format: Parquet partitionnÃ©
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    SILVER LAYER (Notebook 02)
    12 tables | NettoyÃ©es + RGPD
    Anonymisation SHA-256
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    GOLD LAYER (Notebook 03) âœ… FIXÃ‰
    8 tables | Star Schema | ~2.2M lignes

    Dimensions (5):
    â”œâ”€ dim_temps (4,748) â† SANS partitionnement
    â”œâ”€ dim_patient (100,000)
    â”œâ”€ dim_diagnostic (15,490) + CIM-10
    â”œâ”€ dim_professionnel (1,048,575)
    â””â”€ dim_etablissement (200) + gÃ©ographie

    Faits (3):
    â”œâ”€ fait_consultation (1,027,157) partitionnÃ© annee/mois
    â”œâ”€ fait_deces (25M)
    â””â”€ fait_satisfaction (3,638)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    SPARK SQL TABLES (Notebook 05) âœ… FIXÃ‰
    6 tables Hive externes

    â”œâ”€ dim_temps (SANS PARTITIONED BY) â† FIX
    â”œâ”€ dim_patient
    â”œâ”€ dim_diagnostic
    â”œâ”€ dim_professionnel
    â”œâ”€ dim_etablissement
    â””â”€ fait_consultation (PARTITIONED BY annee, mois)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    SPARK THRIFT SERVER
    Port 10000 | JDBC/ODBC
    PyHive driver âœ…
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    APACHE SUPERSET
    Connexion: hive://spark-master:10000/default

    â”œâ”€ SQL Lab (requÃªtes ad-hoc)
    â”œâ”€ Datasets (6 tables)
    â”œâ”€ Charts (visualisations)
    â””â”€ Dashboards (analytique mÃ©tier)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

---

## ğŸ¯ CHECKLIST VALIDATION COMPLÃˆTE

### Bronze Layer âœ…
- [x] 17 tables crÃ©Ã©es
- [x] CSV complet deces.csv (25M lignes)
- [x] PostgreSQL (13 tables)
- [x] DÃ©partements franÃ§ais ajoutÃ©s

### Silver Layer âœ…
- [x] 12 tables nettoyÃ©es
- [x] Anonymisation SHA-256 (RGPD)
- [x] Typage strict

### Gold Layer âœ… FIXÃ‰
- [x] 8 tables Star Schema
- [x] dim_diagnostic + catÃ©gorie CIM-10
- [x] dim_etablissement + rÃ©gion/dÃ©partement
- [x] dim_temps SANS partitionnement â† **FIX APPLIQUÃ‰**
- [x] fait_consultation partitionnÃ© (90 partitions)

### Optimisations âœ…
- [x] Format Parquet (~10x compression)
- [x] Partitionnement temporel (annÃ©e/mois)
- [x] Spark AQE activÃ©
- [x] Benchmarks < 16s (Notebook 04)

### Superset âœ… CORRIGÃ‰
- [x] Thrift Server dÃ©marrÃ©
- [x] PyHive installÃ©
- [x] Notebook 05 crÃ©Ã© et corrigÃ©
- [ ] Notebook 03 rÃ©gÃ©nÃ©rÃ© (Ã  faire - 5 min)
- [ ] Notebook 05 validÃ© (Ã  faire - 2 min)
- [ ] Connexion Superset configurÃ©e (Ã  faire - 5 min)
- [ ] Dashboard crÃ©Ã© (Ã  faire - 3 min)

**Total temps restant** : 15 minutes

---

## ğŸ“š DOCUMENTATION CRÃ‰Ã‰E

| Document | Description | Usage |
|----------|-------------|-------|
| [ETAPES_FINALES_SUPERSET.md](ETAPES_FINALES_SUPERSET.md) | Guide complet avec tous les fixes | â­ GUIDE PRINCIPAL |
| [TUTORIEL_SUPERSET.md](TUTORIEL_SUPERSET.md) | Tutorial dÃ©taillÃ© (600 lignes) | Apprentissage approfondi |
| [GUIDE_DEMARRAGE_RAPIDE_SUPERSET.md](GUIDE_DEMARRAGE_RAPIDE_SUPERSET.md) | Quick start 10 min | DÃ©marrage rapide |
| [FIX_RAPIDE_MAINTENANT.md](FIX_RAPIDE_MAINTENANT.md) | Fix erreur metastore | Troubleshooting |
| [README_SUPERSET.md](README_SUPERSET.md) | Vue d'ensemble | Introduction |
| [INDEX_DOCUMENTATION.md](INDEX_DOCUMENTATION.md) | Index de tous les docs | Navigation |

---

## ğŸ“ POUR TON CV / PORTFOLIO

### Description courte (CV)

> **Data Lakehouse CHU - Architecture Medallion (Bronze/Silver/Gold)**
> Pipeline ETLT complet avec Apache Spark (PySpark) : ingestion multi-sources (CSV 25M lignes + PostgreSQL), transformations et anonymisation RGPD (SHA-256), modÃ©lisation dimensionnelle (Star Schema), optimisations (Parquet, partitionnement), et visualisation BI avec Apache Superset via Spark SQL/Thrift Server.
> Stack : Spark 3.5.0, Docker, Jupyter, MinIO, PostgreSQL, Superset

### Points clÃ©s Ã  mentionner

1. **Architecture Medallion** : Bronze (raw) â†’ Silver (cleaned) â†’ Gold (dimensional)
2. **Scale** : 29M lignes sources â†’ 2.2M lignes Gold aprÃ¨s transformations
3. **RGPD** : Anonymisation SHA-256 des donnÃ©es sensibles
4. **Enrichissements** : CatÃ©gories CIM-10, gÃ©olocalisation dÃ©partement/rÃ©gion
5. **Optimisations** : Parquet (compression 10x), partitionnement temporel, Spark AQE
6. **End-to-End** : Sources â†’ Spark â†’ Data Lake â†’ BI (Superset)

### Technologies maÃ®trisÃ©es

- **Big Data** : Apache Spark 3.5.0, PySpark, Spark SQL
- **Storage** : MinIO S3, Parquet columnar format
- **Databases** : PostgreSQL, Hive Metastore
- **BI** : Apache Superset, Spark Thrift Server
- **DevOps** : Docker, Docker Compose
- **Data Quality** : RGPD compliance, data anonymization

---

## ğŸ†˜ PROBLÃˆMES COURANTS

### âŒ Erreur "Column in converted table" persiste aprÃ¨s Restart Kernel

**Solution** : Nettoyer manuellement le metastore

```python
# Dans Notebook 05, crÃ©er une nouvelle cellule au dÃ©but
tables = ["dim_temps", "dim_patient", "dim_diagnostic",
          "dim_professionnel", "dim_etablissement", "fait_consultation"]

for table in tables:
    try:
        spark.sql(f"DROP TABLE IF EXISTS {table}")
        print(f"âœ… {table} supprimÃ©e")
    except:
        pass
```

Puis relancer **Run All Cells**.

---

### âŒ "Connection refused" dans Superset

**Cause** : Thrift Server pas dÃ©marrÃ©

**Fix** :
```bash
docker exec chu_spark_master bash -c '$SPARK_HOME/sbin/start-thriftserver.sh --master spark://spark-master:7077 --hiveconf hive.server2.thrift.port=10000'
```

---

### âŒ Apache Spark SQL n'apparaÃ®t pas dans Superset

**Cause** : PyHive pas installÃ©

**Fix** :
```bash
docker exec chu_superset pip install pyhive
docker restart chu_superset
```

---

### âŒ dim_temps affiche encore "Path is a directory"

**Cause** : Notebook 03 pas rÃ©gÃ©nÃ©rÃ© aprÃ¨s modification

**Fix** : ExÃ©cuter Notebook 03 avec **Kernel â†’ Restart Kernel** puis **Run All**

Cela va recrÃ©er les fichiers Parquet SANS partitionnement.

---

## ğŸ¯ RÃ‰SUMÃ‰ EXÃ‰CUTIF

### OÃ¹ tu en es

âœ… **ComplÃ©tÃ©** :
- Architecture complÃ¨te Bronze/Silver/Gold
- Notebooks 01-04 validÃ©s
- Corrections appliquÃ©es aux notebooks 03 et 05
- Documentation exhaustive crÃ©Ã©e
- PyHive installÃ© dans Superset

â³ **Ã€ faire** (12 minutes) :
1. RÃ©gÃ©nÃ©rer Gold Layer (Notebook 03)
2. CrÃ©er tables Hive (Notebook 05)
3. Configurer Superset
4. CrÃ©er dashboard de dÃ©monstration

### Prochaine action IMMÃ‰DIATE

1. **MAINTENANT** : http://localhost:8888
2. **Ouvrir** : 03_Transform_Gold_STAR_SCHEMA.ipynb
3. **Kernel â†’ Restart Kernel**
4. **Run â†’ Run All Cells**
5. **Attendre** : 3-4 minutes
6. **VÃ©rifier** : "sans partitionnement" dans la sortie de la cellule 4

---

## âœ… VALIDATION FINALE

AprÃ¨s avoir suivi TOUTES les Ã©tapes, tu dois avoir :

1. âœ… Notebook 03 exÃ©cutÃ© â†’ dim_temps sans partitionnement
2. âœ… Notebook 05 exÃ©cutÃ© â†’ 2.2M lignes comptÃ©es sans erreur
3. âœ… Superset connectÃ© â†’ "Connection looks good!"
4. âœ… SQL Lab fonctionnel â†’ RequÃªte retourne 9 lignes
5. âœ… Dashboard crÃ©Ã© â†’ Big Number affiche 1,027,157

**ALORS** : ğŸ‰ **LIVRABLE 2 100% COMPLET ET VALIDÃ‰ !**

---

**ğŸš€ COMMENCE MAINTENANT : Notebook 03 â†’ Kernel Restart â†’ Run All**

**Temps total restant : 12-15 minutes**

**Bon courage ! ğŸ’ª**
