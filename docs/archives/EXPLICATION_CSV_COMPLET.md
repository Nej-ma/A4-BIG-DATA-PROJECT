# ðŸ“„ POURQUOI CHARGER LE CSV `deces.csv` EN ENTIER ?

**Date** : 21 Octobre 2025
**Question** : "Pourquoi on filtre pas deces? On peut pas prendre deces.csv en entier?"

---

## âœ… RÃ‰PONSE : ON CHARGE MAINTENANT LE CSV COMPLET !

### ðŸ”„ Changement appliquÃ© dans Notebook 01

**Avant** :
```python
# Filtrage annÃ©e 2019 uniquement
df_deces_2019 = df_deces_full.filter(year(to_date(col("date_deces"))) == 2019)
# RÃ©sultat : 620K lignes (97.5% de rÃ©duction âŒ)
```

**Maintenant** :
```python
# Chargement INTÃ‰GRAL (toutes les annÃ©es)
df_deces_full = spark.read.csv(f"{DATA_DIR}/DECES EN FRANCE/deces.csv")
df_deces_full = df_deces_full.repartition(20)  # Optimisation
# RÃ©sultat : 25M lignes (100% des donnÃ©es âœ…)
```

---

## ðŸŽ¯ POURQUOI C'EST MIEUX ?

### 1. **Bronze = DonnÃ©es BRUTES (principe Data Lake)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         BRONZE LAYER                 â”‚
â”‚  = "Raw Zone" (zone brute)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âœ… Tout garder tel quel             â”‚
â”‚  âŒ AUCUN filtrage                   â”‚
â”‚  âŒ AUCUNE transformation            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DÃ©finition officielle** (Databricks, AWS, Azure) :
> "Bronze layer contains raw data as-is from the source"

âž¡ï¸ Si le CSV a 25M lignes, Bronze DOIT avoir 25M lignes.

---

### 2. **Ã‰viter CSV â†’ PostgreSQL** (c'Ã©tait ta question)

**Mauvaise approche** :
```
CSV (25M lignes)
  â†’ Charger dans PostgreSQL (CREATE TABLE + COPY)
  â†’ Lire depuis PostgreSQL (JDBC)
  â†’ Ã‰crire dans Bronze
```

**ProblÃ¨mes** :
- âŒ Ã‰tape inutile (CSV â†’ PostgreSQL)
- âŒ PostgreSQL surchargÃ©
- âŒ Plus lent (2 Ã©tapes au lieu de 1)

**Bonne approche** (actuelle) :
```
CSV (25M lignes)
  â†’ Lire directement avec Spark
  â†’ Ã‰crire dans Bronze (Parquet compressÃ©)
```

**Avantages** :
- âœ… 1 seule Ã©tape
- âœ… Spark optimisÃ© pour CSV
- âœ… Plus rapide
- âœ… PostgreSQL reste pour donnÃ©es opÃ©rationnelles

---

### 3. **FlexibilitÃ© analyses futures**

**Si Bronze filtrÃ© (620K lignes 2019)** :
```python
# âŒ Impossible d'analyser 2020
# âŒ Impossible d'analyser tendance 2015-2024
# âŒ Impossible de comparer avant/aprÃ¨s COVID
# âŒ Il faut RE-CHARGER le CSV source
```

**Si Bronze complet (25M lignes)** :
```python
# âœ… Analyse 2019
df_2019 = df.filter(year(col("date_deces")) == 2019)

# âœ… Analyse COVID 2020-2021
df_covid = df.filter(year(col("date_deces")).isin([2020, 2021]))

# âœ… Tendance dÃ©cennale
df_decade = df.groupBy(year(col("date_deces"))).count()

# âœ… Pic canicule 2003
df_canicule = df.filter(
    (year(col("date_deces")) == 2003) & (month(col("date_deces")) == 8)
)
```

---

## ðŸš€ "MAIS 25M LIGNES, Ã‡A VA PAS CRASHER ?"

### âœ… NON, grÃ¢ce Ã  :

#### 1. **Parquet + Compression**
```
CSV brut : ~3 GB
Parquet Snappy : ~300 MB (10x plus compact âœ…)
```

#### 2. **Spark = Lazy Evaluation**
```python
# Ceci ne charge PAS 25M lignes en mÃ©moire :
df = spark.read.parquet("bronze/csv/deces/")

# Seul le filtrage charge en mÃ©moire :
df_2019 = df.filter(year(col("date_deces")) == 2019)  # 620K seulement
```

#### 3. **Repartitionnement**
```python
df.repartition(20)  # 25M Ã· 20 = 1.25M par partition
```

Chaque partition = ~1.25M lignes = gÃ©rable.

#### 4. **MÃ©moire Spark augmentÃ©e**
```python
# Notebook 01, cellule 2
.config("spark.driver.memory", "8g")    # âœ…
.config("spark.executor.memory", "8g")  # âœ…
```

---

## ðŸ“Š COMPARAISON

| Aspect | FiltrÃ© (avant) | Complet (maintenant) |
|--------|----------------|----------------------|
| **Lignes Bronze** | 620K (2019) | 25M (toutes annÃ©es) |
| **Stockage** | ~50 MB | ~300 MB |
| **Conforme Data Lake** | âŒ Non | âœ… Oui |
| **Analyses futures** | âŒ LimitÃ©es | âœ… IllimitÃ©es |
| **Performance** | âš¡ TrÃ¨s rapide | âš¡ Rapide |
| **Crash risquÃ© ?** | Non | Non |

---

## ðŸŽ“ OÃ™ FAIRE LE FILTRAGE ALORS ?

### âŒ PAS dans Bronze (layer brut)

```python
# âŒ MAUVAIS
# Bronze doit rester brut
```

### âš ï¸ POSSIBLE dans Silver (si besoin)

```python
# Notebook 02 - Silver
# Si vraiment besoin de limiter le volume
df_deces_recent = df_deces_bronze.filter(
    year(col("date_deces")) >= 2015  # Garder 2015-2024 seulement
)
```

### âœ… RECOMMANDÃ‰ dans Gold (filtrage mÃ©tier)

```python
# Notebook 03 - Gold
# Filtrage mÃ©tier par annÃ©e
fait_deces_2019 = df_deces_silver \
    .filter(year(col("date_deces")) == 2019) \
    .join(dim_temps, ...) \
    .select(...)
```

---

## ðŸ”„ LE FLUX COMPLET

```
CSV deces.csv (25M lignes, toutes annÃ©es)
         â†“
    âœ… NOTEBOOK 01 - Bronze
    â€¢ Lecture CSV INTÃ‰GRAL (25M lignes)
    â€¢ AUCUN filtrage
    â€¢ Ã‰criture Parquet compressÃ©
         â†“
ðŸ“¦ BRONZE: bronze/csv/deces/ (25M lignes)
         â†“
    âœ… NOTEBOOK 02 - Silver
    â€¢ Anonymisation SHA-256
    â€¢ Formats dates
    â€¢ POSSIBILITÃ‰ de filtrer (ex: >= 2015)
         â†“
ðŸ“¦ SILVER: silver/deces/ (25M ou moins selon filtrage)
         â†“
    âœ… NOTEBOOK 03 - Gold
    â€¢ CrÃ©ation fait_deces
    â€¢ Filtrage mÃ©tier (2019)
    â€¢ Join avec dimensions
         â†“
â­ GOLD: gold/fait_deces/ (620K lignes 2019 partitionnÃ©)
```

---

## âœ… RÃ‰SUMÃ‰ FINAL

### Ta question :
> "On peut pas prendre deces.csv en entier?"

### Ma rÃ©ponse :
**âœ… OUI, on DOIT prendre le CSV en entier !**

**Pourquoi ?**
1. âœ… Bronze = donnÃ©es brutes (principe Data Lake)
2. âœ… FlexibilitÃ© analyses futures
3. âœ… Performance OK (Parquet + Spark)
4. âœ… Pas besoin CSV â†’ PostgreSQL
5. âœ… Filtrage dans Silver/Gold (pas Bronze)

**Changement appliquÃ©** :
- âœ… Notebook 01, cellule 11 modifiÃ©e
- âœ… Charge maintenant 25M lignes (pas 620K)
- âœ… OptimisÃ© avec repartitionnement

**Prochaine exÃ©cution** :
```bash
# Jupyter (http://localhost:8888)
# ExÃ©cuter Notebook 01
# RÃ©sultat : 25M lignes dans bronze/csv/deces/
```

---

**Conclusion** : âœ… **CSV complet = meilleure pratique Data Lake**
