# üìñ Guide d'Onboarding - √âquipe CHU Data Lakehouse

**Bienvenue dans le projet Big Data CHU !**

Ce guide vous accompagne **pas √† pas** pour d√©marrer sur le projet, que vous soyez nouveau sur Docker, Spark ou Delta Lake.

---

## üéØ Objectif du Projet

Construire un **Data Lakehouse** pour le CHU (Cloud Healthcare Unit) avec :

1. **Architecture Bronze/Silver/Gold** (m√©daillons)
2. **Delta Lake** pour le stockage ACID
3. **Pseudonymisation RGPD** des donn√©es patients (T1)
4. **Mod√®le dimensionnel** pour l'analyse BI (T2)
5. **Orchestration** avec Apache Airflow
6. **Visualisation** avec Apache Superset

---

## üìã Pr√©requis

### Logiciels √† installer

1. **Docker Desktop** (Windows/Mac) ou **Docker Engine** (Linux)
   - T√©l√©charge : https://www.docker.com/products/docker-desktop
   - V√©rifie l'installation : `docker --version`

2. **Git**
   - T√©l√©charge : https://git-scm.com/downloads
   - V√©rifie : `git --version`

3. **Un √©diteur de code** (optionnel mais recommand√©)
   - VS Code : https://code.visualstudio.com/
   - PyCharm Community : https://www.jetbrains.com/pycharm/

### Connaissances recommand√©es

- **SQL** : Bases de donn√©es relationnelles
- **Python** : Syntaxe de base, pandas
- **Docker** : Concepts de base (containers, images)
- **Spark** : Notions de DataFrame (similaire √† pandas)

**Pas de panique si vous d√©butez !** Ce guide est fait pour vous accompagner.

---

## üöÄ Premier Lancement (√âtape par √âtape)

### √âtape 1 : Clone le projet

Ouvre un terminal (PowerShell sur Windows, Terminal sur Mac/Linux) :

```bash
cd C:\Users\TON_NOM\Desktop
git clone <url-du-repo>
cd projet_git
```

### √âtape 2 : V√©rifie que Docker tourne

```bash
docker --version
docker ps
```

Si erreur "Cannot connect to Docker daemon" :
- Sur Windows : Lance **Docker Desktop**
- Sur Linux : `sudo systemctl start docker`

### √âtape 3 : Lance la stack compl√®te

```bash
docker-compose up --build -d
```

**Que se passe-t-il ?**
- `--build` : Reconstruit les images Docker personnalis√©es (Airflow, Jupyter)
- `-d` : Mode d√©tach√© (les services tournent en arri√®re-plan)

**Dur√©e** : 5-10 minutes la premi√®re fois (t√©l√©chargement des images).

### √âtape 4 : Attends que tout d√©marre

V√©rifie l'√©tat des services :

```bash
docker-compose ps
```

Tous les services doivent afficher **Up** (sauf `chu_spark_worker` - non utilis√©).

**Si un service est en "Exit" ou "Restarting"** :

```bash
docker logs <nom_du_service>
```

Exemple : `docker logs chu_postgres`

### √âtape 5 : Teste les acc√®s

Ouvre ton navigateur et teste chaque service :

| Service | URL | Login | Mot de passe |
|---------|-----|-------|--------------|
| Jupyter Lab | http://localhost:8888 | - | token: admin123 |
| Airflow | http://localhost:8080 | admin | admin123 |
| MinIO | http://localhost:9001 | minioadmin | minioadmin123 |
| Superset | http://localhost:8088 | admin | admin123 |

**‚úÖ Si tu vois les interfaces : bravo, la stack est op√©rationnelle !**

---

## üóÑÔ∏è D√©couvrir les Donn√©es

### Acc√©der √† PostgreSQL avec pgAdmin

1. Va sur **pgAdmin** : http://localhost:5050
2. Login : `admin@chu.fr` / `admin123`
3. Clique droit sur "Servers" ‚Üí "Create" ‚Üí "Server"
4. Onglet "General" :
   - Name : `CHU PostgreSQL`
5. Onglet "Connection" :
   - Host : `chu_postgres`
   - Port : `5432`
   - Database : `healthcare_data`
   - Username : `admin`
   - Password : `admin123`
6. Sauvegarde

Tu peux maintenant explorer les 8 tables :
- `Patient` (100 patients)
- `Medecin` (20 m√©decins)
- `Service` (8 services)
- `Medicament` (30 m√©dicaments)
- `Consultation` (108 consultations)
- `Hospitalisation` (50 hospitalisations)
- `Prescription` (65 prescriptions)
- `Acte` (94 actes m√©dicaux)

### Requ√™te SQL de test

```sql
-- Nombre de patients par ville
SELECT ville, COUNT(*) as nb_patients
FROM "Patient"
GROUP BY ville
ORDER BY nb_patients DESC
LIMIT 10;
```

---

## üìì Premier Notebook Jupyter

### √âtape 1 : Ouvre Jupyter Lab

1. Va sur http://localhost:8888
2. Token : `admin123`
3. Navigue vers `notebooks/`
4. Cr√©e un nouveau notebook : **New ‚Üí Python 3 (ipykernel)**

### √âtape 2 : Teste la connexion PostgreSQL

Copie ce code dans une cellule :

```python
from pyspark.sql import SparkSession

# Cr√©er session Spark
spark = SparkSession.builder \
    .appName("Test PostgreSQL") \
    .getOrCreate()

# Lire la table Patient
df = spark.read.jdbc(
    url="jdbc:postgresql://chu_postgres:5432/healthcare_data",
    table='"Patient"',
    properties={"user": "admin", "password": "admin123"}
)

# Afficher les 10 premiers patients
df.show(10, truncate=False)

# Statistiques
print(f"Nombre total de patients : {df.count()}")
```

**Ex√©cute la cellule** : `Shift + Enter`

**R√©sultat attendu** : Tu dois voir 10 lignes de patients.

### √âtape 3 : Teste Delta Lake

```python
from delta import *

# Session Spark avec Delta Lake
builder = SparkSession.builder.appName("Test Delta Lake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://chu_minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# Lire depuis PostgreSQL
df_patients = spark.read.jdbc(
    url="jdbc:postgresql://chu_postgres:5432/healthcare_data",
    table='"Patient"',
    properties={"user": "admin", "password": "admin123"}
)

# √âcrire en Delta Lake (zone Bronze)
df_patients.write.format("delta").mode("overwrite").save("s3a://lakehouse/bronze/patient")

print("‚úÖ Donn√©es √©crites en Delta Lake !")

# Relire depuis Delta Lake
df_delta = spark.read.format("delta").load("s3a://lakehouse/bronze/patient")
df_delta.show(5)
```

**Ex√©cute** et v√©rifie que tu vois les donn√©es.

### √âtape 4 : V√©rifie dans MinIO

1. Va sur **MinIO Console** : http://localhost:9001
2. Login : `minioadmin` / `minioadmin123`
3. Clique sur le bucket `lakehouse`
4. Browse : `bronze/patient/`

Tu dois voir :
- Des fichiers `.parquet` (les donn√©es)
- Un dossier `_delta_log/` (le journal de transactions Delta Lake)

**‚úÖ Si tu vois ces fichiers : Delta Lake fonctionne !**

---

## üéØ D√©velopper les Transformations

### Transformation T1 : RGPD (Bronze ‚Üí Silver)

**Objectif** : Pseudonymiser les donn√©es PII pour respecter le RGPD.

**Donn√©es √† pseudonymiser** :
- `nom` ‚Üí SHA-256
- `prenom` ‚Üí SHA-256
- `numero_securite_sociale` ‚Üí SHA-256
- `adresse`, `telephone`, `email` ‚Üí **Supprimer**

**Donn√©es √† conserver** :
- `date_naissance` ‚Üí Calculer l'**√¢ge** √† la place
- `sexe`, `groupe_sanguin`, `code_postal`, `ville`

**Exemple de code** :

```python
from pyspark.sql.functions import sha2, concat_ws, year, current_date, col, lit

# Lire depuis Bronze
df_bronze = spark.read.format("delta").load("s3a://lakehouse/bronze/patient")

# Transformation T1
df_silver = df_bronze \
    .withColumn("nom_hash", sha2(concat_ws("_", col("nom"), lit("secret_salt_chu")), 256)) \
    .withColumn("prenom_hash", sha2(concat_ws("_", col("prenom"), lit("secret_salt_chu")), 256)) \
    .withColumn("nss_hash", sha2(col("numero_securite_sociale"), 256)) \
    .withColumn("age", year(current_date()) - year(col("date_naissance"))) \
    .select(
        "id_patient",
        "nom_hash",
        "prenom_hash",
        "nss_hash",
        "age",
        "sexe",
        "groupe_sanguin",
        "code_postal",
        "ville",
        "date_inscription",
        "created_at"
    )

# √âcrire en Silver
df_silver.write.format("delta").mode("overwrite").save("s3a://lakehouse/silver/patient")

print("‚úÖ Transformation T1 termin√©e (RGPD)")
df_silver.show(5)
```

### Transformation T2 : Mod√®le Dimensionnel (Silver ‚Üí Gold)

**Objectif** : Cr√©er un mod√®le en √©toile pour l'analyse BI.

**Exemple : Fait Consultation enrichi**

```python
# Lire depuis Silver
df_patient_silver = spark.read.format("delta").load("s3a://lakehouse/silver/patient")

# Lire Consultation depuis Bronze
df_consultation = spark.read.jdbc(
    url="jdbc:postgresql://chu_postgres:5432/healthcare_data",
    table='"Consultation"',
    properties={"user": "admin", "password": "admin123"}
)

# Jointure pour enrichir
df_gold_consultation = df_consultation \
    .join(df_patient_silver, "id_patient", "left") \
    .select(
        "id_consultation",
        "id_patient",
        "id_medecin",
        "date_consultation",
        "motif",
        "diagnostic",
        "duree_minutes",
        "tarif",
        "age",
        "sexe",
        "code_postal",
        "ville"
    )

# √âcrire en Gold
df_gold_consultation.write.format("delta").mode("overwrite").save("s3a://lakehouse/gold/consultation")

print("‚úÖ Transformation T2 termin√©e (DWH)")
df_gold_consultation.show(5)
```

---

## üîÑ Orchestration avec Airflow

### Cr√©er un DAG

1. **Cr√©e un fichier** : `airflow/dags/etl_chu_delta_lake.py`

2. **Template de base** :

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'equipe_chu',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'etl_chu_delta_lake',
    default_args=default_args,
    description='Pipeline ETL CHU avec Delta Lake',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['chu', 'delta-lake', 'etl'],
)

def extract_bronze():
    """Extraction PostgreSQL vers Bronze"""
    from pyspark.sql import SparkSession
    from delta import configure_spark_with_delta_pip

    builder = SparkSession.builder.appName("CHU Extract") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://chu_minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    # Extraction Patient
    df = spark.read.jdbc(
        url="jdbc:postgresql://chu_postgres:5432/healthcare_data",
        table='"Patient"',
        properties={"user": "admin", "password": "admin123"}
    )

    df.write.format("delta").mode("overwrite").save("s3a://lakehouse/bronze/patient")
    print("‚úÖ Extraction Bronze termin√©e")

def transform_t1_silver():
    """Transformation T1 : RGPD"""
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import sha2, concat_ws, year, current_date, col, lit
    from delta import configure_spark_with_delta_pip

    builder = SparkSession.builder.appName("CHU T1") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://chu_minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    df_bronze = spark.read.format("delta").load("s3a://lakehouse/bronze/patient")

    df_silver = df_bronze \
        .withColumn("nom_hash", sha2(concat_ws("_", col("nom"), lit("secret_salt_chu")), 256)) \
        .withColumn("age", year(current_date()) - year(col("date_naissance"))) \
        .select("id_patient", "nom_hash", "age", "sexe", "groupe_sanguin", "code_postal", "ville")

    df_silver.write.format("delta").mode("overwrite").save("s3a://lakehouse/silver/patient")
    print("‚úÖ Transformation T1 termin√©e")

def transform_t2_gold():
    """Transformation T2 : DWH"""
    from pyspark.sql import SparkSession
    from delta import configure_spark_with_delta_pip

    builder = SparkSession.builder.appName("CHU T2") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://chu_minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    df_patient = spark.read.format("delta").load("s3a://lakehouse/silver/patient")

    # Ici : ajouter jointures et enrichissements

    df_patient.write.format("delta").mode("overwrite").save("s3a://lakehouse/gold/dim_patient")
    print("‚úÖ Transformation T2 termin√©e")

# D√©finition des t√¢ches
task_extract = PythonOperator(
    task_id='extract_bronze',
    python_callable=extract_bronze,
    dag=dag,
)

task_t1 = PythonOperator(
    task_id='transform_t1_silver',
    python_callable=transform_t1_silver,
    dag=dag,
)

task_t2 = PythonOperator(
    task_id='transform_t2_gold',
    python_callable=transform_t2_gold,
    dag=dag,
)

# Pipeline
task_extract >> task_t1 >> task_t2
```

3. **Active le DAG** :
   - Va sur http://localhost:8080
   - Login : `admin` / `admin123`
   - Cherche ton DAG `etl_chu_delta_lake`
   - Active-le (toggle √† gauche)
   - Clique sur "Trigger DAG" (bouton play)

4. **V√©rifie l'ex√©cution** :
   - Clique sur le DAG ‚Üí Graph View
   - Chaque t√¢che doit √™tre verte
   - Clique sur une t√¢che ‚Üí Logs pour voir les d√©tails

---

## üìä Visualisation avec Superset

### √âtape 1 : Connecte-toi √† PostgreSQL

1. Va sur http://localhost:8088 (admin / admin123)
2. En haut √† droite : **Settings** ‚Üí **Database Connections**
3. **+ Database**
4. S√©lectionne **PostgreSQL**
5. Remplis :
   - **Host** : `chu_postgres`
   - **Port** : `5432`
   - **Database** : `healthcare_data`
   - **Username** : `admin`
   - **Password** : `admin123`
6. **Test Connection** ‚Üí **Connect**

### √âtape 2 : Cr√©e un Dataset

1. **Data** ‚Üí **Datasets** ‚Üí **+ Dataset**
2. S√©lectionne la connexion PostgreSQL
3. Schema : `public`
4. Table : `Consultation`
5. **Add**

### √âtape 3 : Cr√©e un Chart

1. Clique sur le dataset `Consultation`
2. **Create Chart**
3. Choisis **Bar Chart**
4. Dimensions :
   - **X-Axis** : `date_consultation` (group√© par mois)
   - **Metric** : `COUNT(*)`
5. **Update Chart**
6. **Save** ‚Üí Donne un nom

### √âtape 4 : Cr√©e un Dashboard

1. **Dashboards** ‚Üí **+ Dashboard**
2. Glisse-d√©pose ton chart
3. Ajoute d'autres charts
4. **Save**

---

## ‚ùì FAQ - Questions Fr√©quentes

### Pourquoi le Spark Worker ne d√©marre pas ?

**R√©ponse** : C'est normal ! On utilise **PySpark en mode local** dans Jupyter au lieu d'un cluster distribu√©. C'est largement suffisant pour ce projet.

### Comment arr√™ter la stack ?

```bash
docker-compose down
```

Les **volumes persistent** : tes donn√©es PostgreSQL et MinIO sont conserv√©es.

### Comment tout supprimer et recommencer ?

```bash
docker-compose down -v
```

**‚ö†Ô∏è ATTENTION** : Cela supprime les volumes Docker = perte de donn√©es !

### J'ai modifi√© un Dockerfile, comment rebuild ?

```bash
docker-compose up --build -d
```

### Comment voir les logs en temps r√©el ?

```bash
docker logs chu_airflow_webserver -f
```

Appuie sur `Ctrl+C` pour quitter.

### MinIO dit "Access Denied" quand j'√©cris

V√©rifie que tu utilises les bons credentials :
- Access Key : `minioadmin`
- Secret Key : `minioadmin123`

Et le bon endpoint **dans Docker** : `http://chu_minio:9000`

---

## üéì Ressources P√©dagogiques

### Documentation Officielle

- **Delta Lake** : https://docs.delta.io/latest/index.html
- **PySpark** : https://spark.apache.org/docs/latest/api/python/
- **Airflow** : https://airflow.apache.org/docs/
- **MinIO** : https://min.io/docs/minio/linux/index.html

### Tutoriels Recommand√©s

- **Delta Lake Quickstart** : https://docs.delta.io/latest/quick-start.html
- **PySpark Tutorial** : https://spark.apache.org/docs/latest/api/python/getting_started/index.html
- **Airflow Tutorial** : https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html

### Concepts Cl√©s

- **Data Lakehouse** : Fusion Data Lake + Data Warehouse
- **Architecture M√©daillons** : Bronze (raw) ‚Üí Silver (cleaned) ‚Üí Gold (curated)
- **ACID** : Atomicity, Consistency, Isolation, Durability
- **Delta Lake Transaction Log** : `_delta_log/` pour la gestion des versions
- **Time Travel** : Interroger une version ant√©rieure des donn√©es

---

## ‚úÖ Checklist de D√©marrage

- [ ] Docker Desktop install√© et lanc√©
- [ ] Git install√©
- [ ] Projet clon√© : `git clone <url>`
- [ ] Stack lanc√©e : `docker-compose up --build -d`
- [ ] Tous les services UP : `docker-compose ps`
- [ ] Jupyter accessible : http://localhost:8888
- [ ] Airflow accessible : http://localhost:8080
- [ ] MinIO accessible : http://localhost:9001
- [ ] PostgreSQL test√© : 100 patients visibles
- [ ] Premier notebook cr√©√© et test√©
- [ ] Delta Lake test√© : donn√©es √©crites en Bronze
- [ ] Donn√©es visibles dans MinIO (`lakehouse/bronze/patient/`)

**‚úÖ Une fois tout coch√© : tu es pr√™t √† d√©velopper !**

---

## üöÄ Next Steps

1. **D√©veloppe T1** : Pseudonymisation compl√®te dans un notebook
2. **D√©veloppe T2** : Mod√®le dimensionnel (dimensions + faits)
3. **Cr√©e ton DAG Airflow** : Automatise le pipeline complet
4. **Teste le pipeline** : Bronze ‚Üí Silver ‚Üí Gold
5. **V√©rifie dans MinIO** : Les 3 zones doivent √™tre remplies
6. **Cr√©e tes dashboards Superset** : Visualisation BI
7. **Documente** : README du projet, explications des choix techniques
8. **Teste en √©quipe** : Chacun clone et teste

---

**Bon courage ! üí™**

En cas de probl√®me, n'h√©site pas √† :
1. Checker les logs : `docker logs <service>`
2. Red√©marrer le service : `docker-compose restart <service>`
3. Demander de l'aide √† l'√©quipe !
