# üîç GUIDE - Comment v√©rifier que les donn√©es existent ?

**Date** : 22 Octobre 2025
**Probl√®me** : MinIO Console affiche "This location is empty" alors que les notebooks se sont ex√©cut√©s avec succ√®s

---

## ‚ùì POURQUOI MINIO CONSOLE EST VIDE ?

### Probl√®me architectural

**Les notebooks √©crivent dans le syst√®me de fichiers** :
```
Jupyter ‚Üí /home/jovyan/data/bronze/ ‚Üí ./spark/data/bronze/ (host)
```

**MinIO lit depuis son volume mont√©** :
```
MinIO ‚Üí /data/ ‚Üí ./spark/data/ (host)
```

**MinIO Console affiche seulement** :
- Les objets upload√©s via l'API S3
- **PAS** les fichiers qui existent sur le disque

‚û°Ô∏è **C'est normal que MinIO Console soit vide !**

---

## ‚úÖ COMMENT V√âRIFIER QUE LES DONN√âES EXISTENT ?

### M√©thode 1 : V√©rifier sur le disque local (PLUS SIMPLE)

```bash
# Lister les donn√©es Bronze
ls ./spark/data/bronze/

# R√©sultat attendu :
csv/
postgres/

# V√©rifier CSV
ls ./spark/data/bronze/csv/

# R√©sultat attendu :
deces/
deces_2019/
etablissement_sante/
satisfaction_esatis48h_2019/

# V√©rifier taille des donn√©es
du -sh ./spark/data/bronze/
du -sh ./spark/data/silver/
du -sh ./spark/data/gold/
```

**Sur Windows (PowerShell)** :
```powershell
# Lister les donn√©es
dir "C:\Users\littl\Desktop\Big DATA\projet_git\spark\data\bronze"

# Taille des donn√©es
(Get-ChildItem -Path ".\spark\data\bronze" -Recurse | Measure-Object -Property Length -Sum).Sum / 1MB
```

---

### M√©thode 2 : V√©rifier depuis le container Jupyter

```bash
# Se connecter √† Jupyter
docker exec -it chu_jupyter bash

# Lister les donn√©es
ls -lh /home/jovyan/data/bronze/
ls -lh /home/jovyan/data/silver/
ls -lh /home/jovyan/data/gold/

# V√©rifier taille
du -sh /home/jovyan/data/bronze/
du -sh /home/jovyan/data/silver/
du -sh /home/jovyan/data/gold/
```

---

### M√©thode 3 : V√©rifier depuis le container MinIO

```bash
# Se connecter √† MinIO
docker exec -it chu_minio sh

# Lister les donn√©es
ls -lh /data/bronze/
ls -lh /data/silver/
ls -lh /data/gold/

# Compter les fichiers Parquet
find /data/bronze -name "*.parquet" | wc -l
find /data/silver -name "*.parquet" | wc -l
find /data/gold -name "*.parquet" | wc -l
```

---

### M√©thode 4 : V√©rifier avec Spark (MEILLEURE M√âTHODE)

**Dans Jupyter Notebook** :

```python
# Lire les donn√©es Bronze
df_bronze = spark.read.parquet("/home/jovyan/data/bronze/csv/etablissement_sante")
print(f"√âtablissements en Bronze : {df_bronze.count():,} lignes")
df_bronze.show(5)

# Lire les donn√©es Silver
df_silver = spark.read.parquet("/home/jovyan/data/silver/etablissement_sante")
print(f"√âtablissements en Silver : {df_silver.count():,} lignes")

# Lire les donn√©es Gold
df_gold = spark.read.parquet("/home/jovyan/data/gold/dim_etablissement")
print(f"Dimension √âtablissement : {df_gold.count():,} lignes")
```

---

## üìä R√âSULTATS ATTENDUS

### Bronze layer

```bash
$ du -sh ./spark/data/bronze/
~500 MB

$ find ./spark/data/bronze -name "*.parquet" | wc -l
~200 fichiers
```

**Contenu** :
- `bronze/postgres/` : 13 tables PostgreSQL
- `bronze/csv/` : 3 fichiers CSV convertis

---

### Silver layer

```bash
$ du -sh ./spark/data/silver/
~450 MB

$ find ./spark/data/silver -name "*.parquet" | wc -l
~150 fichiers
```

**Contenu** :
- 12 tables nettoy√©es et anonymis√©es

---

### Gold layer

```bash
$ du -sh ./spark/data/gold/
~200 MB

$ find ./spark/data/gold -name "*.parquet" | wc -l
~100 fichiers
```

**Contenu** :
- 5 dimensions
- 3 faits (partitionn√©s par ann√©e/mois)

---

## üîß SI LES DONN√âES N'EXISTENT PAS

### V√©rifier que les notebooks se sont bien ex√©cut√©s

```python
# Dans chaque notebook, v√©rifier la derni√®re cellule

# Notebook 01 - Doit afficher :
‚úÖ Tables extraites: 16/16
üìä Total lignes: ~29M

# Notebook 02 - Doit afficher :
‚úÖ Silver layer : 12 tables
üìä Total lignes: ~29M

# Notebook 03 - Doit afficher :
‚úÖ Gold layer : 8 tables
üìä Total lignes: ~2.8M
```

---

### V√©rifier les logs Spark

```python
# Dans Jupyter Notebook
# Cellule apr√®s l'√©criture Parquet

# Si erreur, Spark affiche :
‚ùå ERROR: ...

# Si succ√®s, Spark affiche :
üíæ Sauvegard√©: /home/jovyan/data/bronze/...
‚è±Ô∏è  Temps: X.XXs
‚úÖ [table] OK
```

---

## üéØ UTILISER LE FICHIER departements-francais.csv

### 1. Ajouter √† Notebook 01 (Extract Bronze)

```python
# PARTIE 2 : Extract CSV
# Ajouter apr√®s satisfaction et deces

# 4. D√âPARTEMENTS FRAN√áAIS
result = ingest_csv_file(
    name="departements",
    file_path="/home/jovyan/departements-francais.csv",
    separator=";"
)
results.append(result)
```

---

### 2. Utiliser dans Notebook 03 (Gold - Enrichissement g√©ographique)

```python
# Enrichir dim_etablissement avec r√©gion/d√©partement

# Lire d√©partements
df_departements = spark.read.parquet("/home/jovyan/data/bronze/csv/departements")

# Enrichir √©tablissements
df_etablissement_enrichi = df_etablissement.join(
    df_departements,
    df_etablissement["code_departement"] == df_departements["num_departement"],
    "left"
).select(
    "id_etablissement",
    "nom_etablissement",
    "commune",
    "num_departement",
    "libelle_departement",  # ‚Üê Nom du d√©partement
    "libelle_region",        # ‚Üê Nom de la r√©gion
    "abv_region"             # ‚Üê Abr√©viation r√©gion (ARA, BFC, etc.)
)

# Sauvegarder
df_etablissement_enrichi.write \
    .mode("overwrite") \
    .parquet("/home/jovyan/data/gold/dim_etablissement")

print(f"‚úÖ dim_etablissement enrichie avec r√©gions/d√©partements")
```

---

## üìä EXEMPLE : Analyses g√©ographiques possibles

### 1. Consultations par r√©gion

```python
# Dans Notebook 03 ou 04
fait_consultation.join(dim_etablissement, "id_etablissement") \
    .groupBy("libelle_region") \
    .count() \
    .orderBy("count", ascending=False) \
    .show()

# R√©sultat :
+------------------------+--------+
|      libelle_region    |  count |
+------------------------+--------+
| √éle-de-France          | 250000 |
| Auvergne-Rh√¥ne-Alpes   | 180000 |
| Provence-Alpes-C√¥te... | 150000 |
+------------------------+--------+
```

---

### 2. D√©c√®s par d√©partement

```python
# Joindre deces avec √©tablissements (via commune)
fait_deces.join(dim_etablissement, "commune") \
    .groupBy("libelle_departement") \
    .count() \
    .orderBy("count", ascending=False) \
    .show(10)
```

---

### 3. Satisfaction par r√©gion

```python
# Moyenne satisfaction par r√©gion
fait_satisfaction.join(dim_etablissement, "id_etablissement") \
    .groupBy("libelle_region") \
    .agg(avg("score_satisfaction").alias("satisfaction_moyenne")) \
    .orderBy("satisfaction_moyenne", ascending=False) \
    .show()
```

---

## ‚úÖ CHECKLIST FINALE

- [ ] V√©rifier que `./spark/data/bronze/` contient des dossiers
- [ ] V√©rifier que `./spark/data/silver/` contient des dossiers
- [ ] V√©rifier que `./spark/data/gold/` contient des dossiers
- [ ] V√©rifier la taille des donn√©es (Bronze ~500MB, Silver ~450MB, Gold ~200MB)
- [ ] Compter les fichiers Parquet (Bronze ~200, Silver ~150, Gold ~100)
- [ ] Tester la lecture avec Spark dans Jupyter
- [ ] Ajouter `departements-francais.csv` au pipeline
- [ ] Enrichir `dim_etablissement` avec r√©gions/d√©partements

---

## üéØ CONCLUSION

### MinIO Console vide = NORMAL

Les notebooks √©crivent dans le **syst√®me de fichiers local** (`/home/jovyan/data/`), pas via l'**API S3 de MinIO**.

‚û°Ô∏è **C'est un choix d'architecture** : plus simple et plus rapide pour le d√©veloppement.

---

### Comment v√©rifier que √ßa marche ?

**‚úÖ M√âTHODE RECOMMAND√âE** :

```python
# Dans Jupyter Notebook
# Cellule de test

# Lire Bronze
df = spark.read.parquet("/home/jovyan/data/bronze/csv/etablissement_sante")
print(f"‚úÖ Bronze OK : {df.count():,} lignes")

# Lire Silver
df = spark.read.parquet("/home/jovyan/data/silver/etablissement_sante")
print(f"‚úÖ Silver OK : {df.count():,} lignes")

# Lire Gold
df = spark.read.parquet("/home/jovyan/data/gold/dim_etablissement")
print(f"‚úÖ Gold OK : {df.count():,} lignes")
```

**Si ces 3 lignes s'affichent, tes donn√©es sont bien l√† !** ‚úÖ

---

**Pour le d√©partement enrichment, veux-tu que je modifie les notebooks pour ajouter `departements-francais.csv` ?**
