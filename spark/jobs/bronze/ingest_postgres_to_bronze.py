"""
BRONZE LAYER - Ingestion PostgreSQL ‚Üí MinIO Delta Lake
====================================================
Extrait toutes les tables PostgreSQL et les charge en format Delta Lake
dans MinIO (couche Bronze - donn√©es brutes)

Auteurs: Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING
Date: 2025
"""

import sys
sys.path.append('/opt/spark-apps/utils')

from spark_utils import (
    create_spark_session,
    get_postgres_connection_properties,
    write_to_delta,
    add_ingestion_metadata,
    log_dataframe_info,
    logger
)

# Tables √† extraire de PostgreSQL
TABLES = [
    "Patient",
    "Consultation",
    "Diagnostic",
    "Professionnel_de_sante",
    "Mutuelle",
    "Adher",
    "Prescription",
    "Medicaments",
    "Laboratoire",
    "Salle",
    "Specialites",
    "date",
    "AAAA"
]

# Chemin base pour la couche Bronze dans MinIO
BRONZE_BASE_PATH = "s3a://lakehouse/bronze/postgres"


def ingest_table_to_bronze(spark, table_name: str, jdbc_props: dict):
    """
    Ing√®re une table PostgreSQL vers la couche Bronze

    Args:
        spark: Session Spark
        table_name: Nom de la table PostgreSQL
        jdbc_props: Propri√©t√©s de connexion JDBC
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"üîÑ Ingestion de la table: {table_name}")
    logger.info(f"{'='*80}")

    try:
        # Lecture depuis PostgreSQL
        logger.info(f"üìñ Lecture depuis PostgreSQL...")
        df = spark.read.jdbc(
            url=jdbc_props["url"],
            table=f'"{table_name}"',  # Guillemets pour les noms en majuscules
            properties=jdbc_props
        )

        log_dataframe_info(df, table_name)

        # Ajout des m√©tadonn√©es d'ingestion
        df_with_metadata = add_ingestion_metadata(df)

        # √âcriture en Delta Lake (Bronze)
        bronze_path = f"{BRONZE_BASE_PATH}/{table_name}"
        write_to_delta(
            df_with_metadata,
            bronze_path,
            mode="overwrite",
            partition_by=["ingestion_date"]
        )

        logger.info(f"‚úÖ Table {table_name} ing√©r√©e avec succ√®s!")
        return True

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ingestion de {table_name}: {str(e)}")
        return False


def main():
    """
    Point d'entr√©e principal
    """
    logger.info("\n" + "="*80)
    logger.info("üöÄ D√âMARRAGE - Ingestion PostgreSQL vers Bronze Layer")
    logger.info("="*80 + "\n")

    # Cr√©ation de la session Spark
    spark = create_spark_session("CHU_Bronze_Postgres_Ingestion")

    # Propri√©t√©s de connexion PostgreSQL
    jdbc_props = get_postgres_connection_properties()

    # Statistiques
    success_count = 0
    failed_count = 0

    # Ingestion de toutes les tables
    for table in TABLES:
        if ingest_table_to_bronze(spark, table, jdbc_props):
            success_count += 1
        else:
            failed_count += 1

    # R√©sum√©
    logger.info("\n" + "="*80)
    logger.info("üìä R√âSUM√â DE L'INGESTION")
    logger.info("="*80)
    logger.info(f"‚úÖ Tables ing√©r√©es avec succ√®s: {success_count}/{len(TABLES)}")
    logger.info(f"‚ùå Tables en erreur: {failed_count}/{len(TABLES)}")
    logger.info(f"üì¶ Localisation Bronze: {BRONZE_BASE_PATH}")
    logger.info("="*80 + "\n")

    # Arr√™t de la session Spark
    spark.stop()
    logger.info("üèÅ Ingestion termin√©e!")


if __name__ == "__main__":
    main()
