"""
Script de test des connexions
VÃ©rifie que Spark peut se connecter Ã  PostgreSQL et MinIO
"""

import sys
sys.path.append('/opt/spark-apps/utils')

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

print("\n" + "="*80)
print("ğŸ” TEST DES CONNEXIONS - CHU Data Lakehouse")
print("="*80 + "\n")

# Configuration Spark
print("ğŸ“¦ Configuration de Spark...")
spark = SparkSession.builder \
    .appName("CHU_Test_Connections") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-core_2.12:2.4.0") \
    .config("spark.hadoop.fs.s3a.endpoint", "chu_minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .getOrCreate()

print("âœ… Session Spark crÃ©Ã©e\n")

# Test 1: Connexion PostgreSQL
print("="*80)
print("TEST 1: Connexion PostgreSQL")
print("="*80)

try:
    print("ğŸ”„ Tentative de connexion Ã  PostgreSQL...")
    jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
    jdbc_props = {
        "user": "admin",
        "password": "admin123",
        "driver": "org.postgresql.Driver"
    }

    # Lire la table Patient (limitÃ© Ã  10 lignes pour le test)
    df_patient = spark.read.jdbc(
        url=jdbc_url,
        table='"Patient"',
        properties=jdbc_props
    )

    count = df_patient.count()
    print(f"âœ… Connexion PostgreSQL OK!")
    print(f"ğŸ“Š Table Patient: {count:,} lignes")
    print("\nAperÃ§u des donnÃ©es:")
    df_patient.show(5, truncate=True)

except Exception as e:
    print(f"âŒ ERREUR PostgreSQL: {str(e)}")
    import traceback
    traceback.print_exc()

# Test 2: Connexion MinIO
print("\n" + "="*80)
print("TEST 2: Ã‰criture/Lecture MinIO")
print("="*80)

try:
    print("ğŸ”„ CrÃ©ation d'un DataFrame de test...")
    test_data = [
        (1, "Test 1", "2025-01-20"),
        (2, "Test 2", "2025-01-20"),
        (3, "Test 3", "2025-01-20")
    ]
    df_test = spark.createDataFrame(test_data, ["id", "nom", "date"])

    print("ğŸ”„ Ã‰criture dans MinIO...")
    test_path = "s3a://lakehouse/test/data"
    df_test.write.mode("overwrite").parquet(test_path)
    print(f"âœ… Ã‰criture OK: {test_path}")

    print("ğŸ”„ Lecture depuis MinIO...")
    df_read = spark.read.parquet(test_path)
    read_count = df_read.count()
    print(f"âœ… Lecture OK: {read_count} lignes")
    print("\nAperÃ§u des donnÃ©es:")
    df_read.show()

    print("âœ… Connexion MinIO OK!")

except Exception as e:
    print(f"âŒ ERREUR MinIO: {str(e)}")
    import traceback
    traceback.print_exc()

# RÃ©sumÃ©
print("\n" + "="*80)
print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS")
print("="*80)
print("âœ… Tous les tests sont passÃ©s!")
print("ğŸ¯ PrÃªt pour l'ingestion Bronze")
print("="*80 + "\n")

spark.stop()
