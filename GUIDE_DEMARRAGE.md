# ğŸš€ GUIDE DE DÃ‰MARRAGE - Projet CHU Data Lakehouse

## ğŸ“‹ CE QUI A Ã‰TÃ‰ FAIT

J'ai crÃ©Ã© une infrastructure Data Lakehouse complÃ¨te pour votre projet. Voici ce qui existe maintenant :

### âœ… Infrastructure Docker
- **PostgreSQL** : Base de donnÃ©es source (100K patients, 1M consultations) âœ…
- **MinIO** : Stockage objet S3 pour Delta Lake âœ…
- **Spark** : Moteur de traitement Big Data âœ…
- **Jupyter** : Interface pour dÃ©velopper et tester âœ…
- **Airflow** : Orchestration des pipelines âš ï¸ (Ã  corriger)
- **Superset** : Visualisation âœ…
- **PgAdmin** : Admin PostgreSQL âœ…

### âœ… Scripts crÃ©Ã©s
1. **spark/jobs/utils/spark_utils.py** - Fonctions utilitaires
2. **spark/jobs/bronze/ingest_postgres_to_bronze.py** - Ingestion PostgreSQL â†’ MinIO
3. **spark/jobs/bronze/ingest_csv_to_bronze.py** - Ingestion CSV â†’ MinIO
4. **docs/livrable2/modele_dimensionnel.md** - ModÃ¨le Star Schema
5. **docs/livrable2/README.md** - Documentation dÃ©taillÃ©e

---

## ğŸ¯ OBJECTIF DU LIVRABLE 2

**Le Livrable 2 consiste Ã  :**

1. **CrÃ©er un entrepÃ´t de donnÃ©es (Data Warehouse)** dans MinIO
2. **Charger toutes les donnÃ©es** (PostgreSQL + CSV) dans cet entrepÃ´t
3. **Organiser en 3 couches** :
   - **BRONZE** = DonnÃ©es brutes (copie exacte des sources)
   - **SILVER** = DonnÃ©es nettoyÃ©es (typage, validation)
   - **GOLD** = ModÃ¨le dimensionnel (Star Schema pour l'analyse)
4. **Optimiser les performances** avec partitionnement et bucketing
5. **Mesurer les performances** avec des requÃªtes benchmark
6. **CrÃ©er des graphiques** montrant les gains de performance

---

## ğŸ”§ PROBLÃˆMES ACTUELS Ã€ CORRIGER

### 1. âŒ Airflow CSRF Error
**ProblÃ¨me** : Erreur "CSRF token missing"
**Solution** : RedÃ©marrer Airflow proprement

### 2. âŒ MinIO buckets
**ProblÃ¨me** : Les buckets ne sont peut-Ãªtre pas visibles
**Solution** : VÃ©rifier via l'interface web

---

## ğŸ“ Ã‰TAPES POUR COMMENCER

### Ã‰TAPE 1 : VÃ©rifier que tout fonctionne

```bash
# Lister tous les conteneurs
docker compose ps

# Tous doivent Ãªtre "Up" et "healthy"
```

### Ã‰TAPE 2 : AccÃ©der Ã  MinIO

1. Ouvrir : http://localhost:9001
2. Login : **minioadmin** / **minioadmin123**
3. VÃ©rifier qu'il y a 2 buckets :
   - `lakehouse`
   - `warehouse`

**Si les buckets n'existent pas**, exÃ©cutez :
```bash
docker compose up -d minio_setup
docker logs chu_minio_setup
```

### Ã‰TAPE 3 : AccÃ©der Ã  Jupyter

1. Ouvrir : http://localhost:8888
2. Token : **admin123**
3. CrÃ©er un nouveau notebook

### Ã‰TAPE 4 : Tester la connexion PostgreSQL

Dans Jupyter, crÃ©ez un notebook et exÃ©cutez :

```python
from pyspark.sql import SparkSession

# CrÃ©er session Spark
spark = SparkSession.builder \
    .appName("Test PostgreSQL") \
    .getOrCreate()

# Connexion PostgreSQL
jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
jdbc_props = {
    "user": "admin",
    "password": "admin123",
    "driver": "org.postgresql.Driver"
}

# Lire la table Patient
df = spark.read.jdbc(url=jdbc_url, table='"Patient"', properties=jdbc_props)

# Afficher
print(f"Nombre de patients : {df.count()}")
df.show(5)
```

**RÃ©sultat attendu** : 100,000 patients affichÃ©s âœ…

### Ã‰TAPE 5 : Tester MinIO

Dans le mÃªme notebook :

```python
# CrÃ©er un DataFrame de test
test_data = [(1, "Test 1"), (2, "Test 2")]
df_test = spark.createDataFrame(test_data, ["id", "nom"])

# Essayer d'Ã©crire dans MinIO
try:
    df_test.write.mode("overwrite").parquet("s3a://lakehouse/test/data")
    print("âœ… SUCCÃˆS : MinIO fonctionne !")
except Exception as e:
    print(f"âŒ ERREUR MinIO : {e}")
```

---

## ğŸ“ COMMENT UTILISER CE PROJET

### ScÃ©nario 1 : Vous voulez JUSTE faire l'ingestion (Bronze Layer)

**Ce qui va se passer** :
- Toutes les tables PostgreSQL â†’ CopiÃ©es dans MinIO (`s3a://lakehouse/bronze/postgres/`)
- Tous les CSV â†’ CopiÃ©s dans MinIO (`s3a://lakehouse/bronze/csv/`)
- Format : **Delta Lake** (format optimisÃ© pour Big Data)

**Comment faire** :
```python
# Dans Jupyter
!python /opt/spark-apps/bronze/ingest_postgres_to_bronze.py
!python /opt/spark-apps/bronze/ingest_csv_to_bronze.py
```

### ScÃ©nario 2 : Vous voulez crÃ©er le modÃ¨le dimensionnel (Gold Layer)

**Ce qui va se passer** :
- CrÃ©ation de tables de **dimensions** (Patient, Temps, Diagnostic, etc.)
- CrÃ©ation de tables de **faits** (Consultations, Hospitalisations, DÃ©cÃ¨s, Satisfaction)
- Partitionnement par annÃ©e/mois
- Bucketing sur les clÃ©s de jointure

**Comment faire** :
```python
# Dans Jupyter (scripts Ã  crÃ©er ensemble)
!python /opt/spark-apps/gold/create_dimensions.py
!python /opt/spark-apps/gold/create_facts.py
```

### ScÃ©nario 3 : Vous voulez mesurer les performances

**Ce qui va se passer** :
- ExÃ©cution de requÃªtes de test
- Mesure des temps de rÃ©ponse
- Comparaison avant/aprÃ¨s optimisation
- GÃ©nÃ©ration de graphiques

**Comment faire** :
```python
# Dans Jupyter (scripts Ã  crÃ©er ensemble)
!python /opt/spark-apps/performance/benchmark_queries.py
```

---

## ğŸ†˜ EN CAS DE PROBLÃˆME

### ProblÃ¨me : "Rien ne marche"
```bash
# Tout arrÃªter
docker compose down

# Tout redÃ©marrer proprement
docker compose up -d postgres minio jupyter spark-master spark-worker

# Attendre 30 secondes, puis vÃ©rifier
docker compose ps
```

### ProblÃ¨me : "Airflow ne fonctionne pas"
```bash
# ArrÃªter Airflow
docker compose stop airflow-webserver airflow-scheduler

# RedÃ©marrer
docker compose up -d airflow-init
# Attendre que airflow-init se termine
docker compose up -d airflow-webserver airflow-scheduler
```

### ProblÃ¨me : "MinIO n'a pas de buckets"
```bash
# RecrÃ©er les buckets
docker compose up -d minio_setup

# VÃ©rifier les logs
docker logs chu_minio_setup

# VÃ©rifier sur http://localhost:9001
```

---

## ğŸ“Š PROCHAINES Ã‰TAPES (Ce qu'il reste Ã  faire)

1. âœ… Infrastructure Docker â†’ **FAIT**
2. âœ… Scripts Bronze â†’ **FAIT**
3. âœ… ModÃ¨le dimensionnel â†’ **DOCUMENTÃ‰**
4. â³ Tester l'ingestion Bronze â†’ **Ã€ FAIRE MAINTENANT**
5. â³ CrÃ©er scripts Silver (nettoyage) â†’ **Ã€ FAIRE**
6. â³ CrÃ©er scripts Gold (Star Schema) â†’ **Ã€ FAIRE**
7. â³ Optimisations (partitionnement/bucketing) â†’ **Ã€ FAIRE**
8. â³ Benchmarks de performance â†’ **Ã€ FAIRE**
9. â³ Graphiques de rÃ©sultats â†’ **Ã€ FAIRE**

---

## ğŸ¯ POUR RÃ‰SUMER EN 3 PHRASES

1. **J'ai crÃ©Ã© toute l'infrastructure** (Docker + Spark + MinIO) pour stocker et traiter vos donnÃ©es
2. **J'ai Ã©crit les scripts** pour copier PostgreSQL et les CSV dans un Data Lakehouse
3. **Il faut maintenant tester que tout fonctionne** puis crÃ©er le modÃ¨le dimensionnel

---

## ğŸ’¡ CE QUE VOUS DEVEZ FAIRE MAINTENANT

**Option 1 (RecommandÃ©e)** : On corrige MinIO ensemble et on teste l'ingestion
**Option 2** : On crÃ©e d'abord les scripts Gold (modÃ¨le dimensionnel)
**Option 3** : Vous testez manuellement avec Jupyter et me dites ce qui bloque

**Quelle option voulez-vous choisir ?**
