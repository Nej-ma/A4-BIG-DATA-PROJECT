{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé® SETUP SUPERSET - Configuration des tables SQL\n",
    "\n",
    "**Objectif** : Cr√©er les tables Spark SQL pour que Superset puisse se connecter\n",
    "\n",
    "**Auteurs** : Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ce que fait ce notebook\n",
    "\n",
    "1. Cr√©er des **tables externes Spark SQL** pointant vers les fichiers Parquet Gold\n",
    "2. R√©parer les partitions de la table `fait_consultation`\n",
    "3. V√©rifier que toutes les tables sont accessibles\n",
    "4. Tester les requ√™tes SQL pour Superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"‚úÖ Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark 3.5.0 d√©marr√© avec support Hive\n",
      "‚úÖ spark.sql.hive.convertMetastoreParquet = false (fix sch√©ma)\n"
     ]
    }
   ],
   "source": [
    "# Configuration Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CHU_Superset_Setup\") \\\n",
    "    .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark {spark.version} d√©marr√© avec support Hive\")\n",
    "print(\"‚úÖ spark.sql.hive.convertMetastoreParquet = false (fix sch√©ma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä √âTAPE 1 : Cr√©ation des tables externes\n",
    "\n",
    "Ces tables permettent √† Spark SQL (et donc Superset) d'interroger les fichiers Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Cr√©ation des tables externes...\n",
      "\n",
      "  üóëÔ∏è  dim_temps - ancien metastore supprim√©\n",
      "  ‚úÖ 1/6 - dim_temps cr√©√©e\n",
      "  üóëÔ∏è  dim_patient - ancien metastore supprim√©\n",
      "  ‚úÖ 2/6 - dim_patient cr√©√©e\n",
      "  üóëÔ∏è  dim_diagnostic - ancien metastore supprim√©\n",
      "  ‚úÖ 3/6 - dim_diagnostic cr√©√©e\n",
      "  üóëÔ∏è  dim_professionnel - ancien metastore supprim√©\n",
      "  ‚úÖ 4/6 - dim_professionnel cr√©√©e\n",
      "  üóëÔ∏è  dim_etablissement - ancien metastore supprim√©\n",
      "  ‚úÖ 5/6 - dim_etablissement cr√©√©e\n",
      "  üóëÔ∏è  fait_consultation - ancien metastore supprim√©\n",
      "  ‚úÖ 6/6 - fait_consultation cr√©√©e\n",
      "\n",
      "‚úÖ Toutes les tables cr√©√©es !\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er toutes les tables externes\n",
    "\n",
    "# D√©finitions des tables (SANS partitionnement pour dim_temps)\n",
    "tables_definitions = {\n",
    "    \"dim_temps\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE dim_temps (\n",
    "            id_temps STRING,\n",
    "            date_complete DATE,\n",
    "            annee INT,\n",
    "            mois INT,\n",
    "            trimestre INT,\n",
    "            jour_semaine STRING,\n",
    "            nom_mois STRING,\n",
    "            est_weekend BOOLEAN,\n",
    "            numero_jour_semaine INT\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/dim_temps'\n",
    "    \"\"\",\n",
    "\n",
    "    \"dim_patient\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE dim_patient (\n",
    "            id_patient STRING,\n",
    "            nom_hash STRING,\n",
    "            prenom_hash STRING,\n",
    "            sexe STRING,\n",
    "            age INT,\n",
    "            date_naissance DATE,\n",
    "            ville STRING,\n",
    "            code_postal STRING,\n",
    "            pays STRING,\n",
    "            groupe_sanguin STRING\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/dim_patient'\n",
    "    \"\"\",\n",
    "\n",
    "    \"dim_diagnostic\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE dim_diagnostic (\n",
    "            code_diag STRING,\n",
    "            libelle STRING,\n",
    "            categorie STRING\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/dim_diagnostic'\n",
    "    \"\"\",\n",
    "\n",
    "    \"dim_professionnel\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE dim_professionnel (\n",
    "            code_specialite STRING,\n",
    "            id_prof STRING,\n",
    "            nom STRING,\n",
    "            prenom STRING,\n",
    "            nom_specialite STRING\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/dim_professionnel'\n",
    "    \"\"\",\n",
    "\n",
    "    \"dim_etablissement\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE dim_etablissement (\n",
    "            finess STRING,\n",
    "            siret STRING,\n",
    "            nom STRING,\n",
    "            ville STRING,\n",
    "            code_postal STRING,\n",
    "            telephone STRING,\n",
    "            email STRING,\n",
    "            code_departement STRING,\n",
    "            num_departement STRING,\n",
    "            libelle_departement STRING,\n",
    "            libelle_region STRING,\n",
    "            abv_region STRING\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/dim_etablissement'\n",
    "    \"\"\",\n",
    "\n",
    "    \"fait_consultation\": \"\"\"\n",
    "        CREATE EXTERNAL TABLE fait_consultation (\n",
    "            id_consultation STRING,\n",
    "            id_patient STRING,\n",
    "            id_prof STRING,\n",
    "            code_diag STRING,\n",
    "            id_mutuelle STRING,\n",
    "            id_temps STRING,\n",
    "            date_consultation DATE,\n",
    "            heure_debut TIMESTAMP,\n",
    "            heure_fin TIMESTAMP,\n",
    "            motif STRING,\n",
    "            jour INT\n",
    "        )\n",
    "        PARTITIONED BY (annee INT, mois INT)\n",
    "        STORED AS PARQUET\n",
    "        LOCATION '/home/jovyan/data/gold/fait_consultation'\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"üì¶ Cr√©ation des tables externes...\\n\")\n",
    "\n",
    "for i, (table_name, create_sql) in enumerate(tables_definitions.items(), 1):\n",
    "    try:\n",
    "        # D'abord DROP (commande s√©par√©e) - FORCE pour supprimer le metastore\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "            print(f\"  üóëÔ∏è  {table_name} - ancien metastore supprim√©\")\n",
    "        except Exception as drop_err:\n",
    "            print(f\"  ‚ÑπÔ∏è  {table_name} - pas de metastore existant\")\n",
    "\n",
    "        # Puis CREATE\n",
    "        spark.sql(create_sql)\n",
    "        print(f\"  ‚úÖ {i}/6 - {table_name} cr√©√©e\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erreur table {i} ({table_name}): {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Toutes les tables cr√©√©es !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß √âTAPE 2 : R√©parer les partitions\n",
    "\n",
    "**Important** : Pour les tables partitionn√©es, Spark doit scanner le r√©pertoire pour d√©couvrir toutes les partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß R√©paration des partitions de fait_consultation...\n",
      "\n",
      "‚úÖ Partitions r√©par√©es !\n",
      "\n",
      "üìä 94 partitions trouv√©es\n",
      "+------------------+\n",
      "|partition         |\n",
      "+------------------+\n",
      "|annee=2015/mois=10|\n",
      "|annee=2015/mois=11|\n",
      "|annee=2015/mois=12|\n",
      "|annee=2015/mois=6 |\n",
      "|annee=2015/mois=7 |\n",
      "|annee=2015/mois=8 |\n",
      "|annee=2015/mois=9 |\n",
      "|annee=2016/mois=1 |\n",
      "|annee=2016/mois=10|\n",
      "|annee=2016/mois=11|\n",
      "|annee=2016/mois=12|\n",
      "|annee=2016/mois=2 |\n",
      "|annee=2016/mois=3 |\n",
      "|annee=2016/mois=4 |\n",
      "|annee=2016/mois=5 |\n",
      "|annee=2016/mois=6 |\n",
      "|annee=2016/mois=7 |\n",
      "|annee=2016/mois=8 |\n",
      "|annee=2016/mois=9 |\n",
      "|annee=2017/mois=1 |\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# R√©parer les partitions de fait_consultation\n",
    "print(\"üîß R√©paration des partitions de fait_consultation...\\n\")\n",
    "\n",
    "spark.sql(\"MSCK REPAIR TABLE fait_consultation\")\n",
    "\n",
    "print(\"‚úÖ Partitions r√©par√©es !\")\n",
    "\n",
    "# Afficher les partitions d√©couvertes\n",
    "partitions = spark.sql(\"SHOW PARTITIONS fait_consultation\")\n",
    "print(f\"\\nüìä {partitions.count()} partitions trouv√©es\")\n",
    "partitions.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ √âTAPE 3 : V√©rification des tables\n",
    "\n",
    "Compter les lignes de chaque table pour s'assurer qu'elles sont bien charg√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Tables disponibles :\n",
      "\n",
      "+---------+-----------------+-----------+\n",
      "|namespace|tableName        |isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|default  |dim_diagnostic   |false      |\n",
      "|default  |dim_etablissement|false      |\n",
      "|default  |dim_patient      |false      |\n",
      "|default  |dim_professionnel|false      |\n",
      "|default  |dim_temps        |false      |\n",
      "|default  |fait_consultation|false      |\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lister toutes les tables\n",
    "print(\"üìã Tables disponibles :\\n\")\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä COMPTAGE DES LIGNES\n",
      "\n",
      "============================================================\n",
      "  dim_temps                 :      4,748 lignes\n",
      "  dim_patient               :    100,000 lignes\n",
      "  dim_diagnostic            :     15,490 lignes\n",
      "  dim_professionnel         :  1,048,575 lignes\n",
      "  dim_etablissement         :        200 lignes\n",
      "  fait_consultation         :  1,027,157 lignes\n",
      "============================================================\n",
      "  TOTAL                     :  2,196,170 lignes\n",
      "\n",
      "‚úÖ Toutes les tables sont accessibles !\n"
     ]
    }
   ],
   "source": [
    "# Compter les lignes de chaque table\n",
    "print(\"üìä COMPTAGE DES LIGNES\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tables = [\n",
    "    \"dim_temps\",\n",
    "    \"dim_patient\",\n",
    "    \"dim_diagnostic\",\n",
    "    \"dim_professionnel\",\n",
    "    \"dim_etablissement\",\n",
    "    \"fait_consultation\"\n",
    "]\n",
    "\n",
    "total = 0\n",
    "for table in tables:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {table}\").collect()[0]['cnt']\n",
    "        total += count\n",
    "        print(f\"  {table:25s} : {count:>10,} lignes\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {table:25s} : ‚ùå Erreur - {e}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"  {'TOTAL':25s} : {total:>10,} lignes\\n\")\n",
    "print(\"‚úÖ Toutes les tables sont accessibles !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç √âTAPE 4 : Tests des requ√™tes Superset\n",
    "\n",
    "Tester les requ√™tes qui seront utilis√©es dans Superset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST 1 : Consultations par ann√©e\n",
      "\n",
      "+-----+----------------+----------------+\n",
      "|annee|nb_consultations|patients_uniques|\n",
      "+-----+----------------+----------------+\n",
      "| 2015|           33896|           28581|\n",
      "| 2016|          184308|           85272|\n",
      "| 2017|          133403|           74201|\n",
      "| 2018|          160373|           81075|\n",
      "| 2019|           87497|           58635|\n",
      "| 2020|          162778|           81612|\n",
      "| 2021|          145883|           78593|\n",
      "| 2022|          101991|           66042|\n",
      "| 2023|           17028|           15772|\n",
      "+-----+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST 1 : Consultations par ann√©e\n",
    "print(\"üîç TEST 1 : Consultations par ann√©e\\n\")\n",
    "\n",
    "# Lire directement les fichiers Parquet (bypass Hive)\n",
    "df_temps = spark.read.parquet(\"/home/jovyan/data/gold/dim_temps\")\n",
    "df_consult = spark.read.parquet(\"/home/jovyan/data/gold/fait_consultation\")\n",
    "\n",
    "# Cr√©er des vues temporaires\n",
    "df_temps.createOrReplaceTempView(\"dim_temps_tmp\")\n",
    "df_consult.createOrReplaceTempView(\"fait_consultation_tmp\")\n",
    "\n",
    "query1 = \"\"\"\n",
    "SELECT\n",
    "    t.annee,\n",
    "    COUNT(*) as nb_consultations,\n",
    "    COUNT(DISTINCT f.id_patient) as patients_uniques\n",
    "FROM fait_consultation_tmp f\n",
    "JOIN dim_temps_tmp t ON f.id_temps = t.id_temps\n",
    "GROUP BY t.annee\n",
    "ORDER BY t.annee\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST 2 : Top diagnostics par cat√©gorie CIM-10\n",
      "\n",
      "+---------+----------------+-----------+\n",
      "|categorie|nb_consultations|pourcentage|\n",
      "+---------+----------------+-----------+\n",
      "|M        |172661          |16.81      |\n",
      "|O        |93887           |9.14       |\n",
      "|S        |91287           |8.89       |\n",
      "|T        |65015           |6.33       |\n",
      "|V        |52376           |5.10       |\n",
      "|Z        |50938           |4.96       |\n",
      "|Q        |42209           |4.11       |\n",
      "|D        |34813           |3.39       |\n",
      "|K        |32718           |3.19       |\n",
      "|C        |32433           |3.16       |\n",
      "+---------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST 2 : Top diagnostics par cat√©gorie CIM-10\n",
    "print(\"üîç TEST 2 : Top diagnostics par cat√©gorie CIM-10\\n\")\n",
    "\n",
    "# Lire diagnostic\n",
    "df_diag = spark.read.parquet(\"/home/jovyan/data/gold/dim_diagnostic\")\n",
    "df_diag.createOrReplaceTempView(\"dim_diagnostic_tmp\")\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT\n",
    "    d.categorie,\n",
    "    COUNT(*) as nb_consultations,\n",
    "    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage\n",
    "FROM fait_consultation_tmp f\n",
    "JOIN dim_diagnostic_tmp d ON f.code_diag = d.code_diag\n",
    "WHERE d.categorie IS NOT NULL\n",
    "GROUP BY d.categorie\n",
    "ORDER BY nb_consultations DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query2).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST 3 : Consultations par sexe et tranche d'√¢ge\n",
      "\n",
      "+------+-----------+----------------+\n",
      "|  sexe|tranche_age|nb_consultations|\n",
      "+------+-----------+----------------+\n",
      "|female|   0-17 ans|          109235|\n",
      "|female|  18-29 ans|           74036|\n",
      "|female|  30-49 ans|          120365|\n",
      "|female|  50-64 ans|           90167|\n",
      "|female|    65+ ans|          214966|\n",
      "|  male|   0-17 ans|           75423|\n",
      "|  male|  18-29 ans|           51094|\n",
      "|  male|  30-49 ans|           82206|\n",
      "|  male|  50-64 ans|           61354|\n",
      "|  male|    65+ ans|          148311|\n",
      "+------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST 3 : Consultations par sexe et tranche d'√¢ge\n",
    "print(\"üîç TEST 3 : Consultations par sexe et tranche d'√¢ge\\n\")\n",
    "\n",
    "# Lire patient\n",
    "df_patient = spark.read.parquet(\"/home/jovyan/data/gold/dim_patient\")\n",
    "df_patient.createOrReplaceTempView(\"dim_patient_tmp\")\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT\n",
    "    p.sexe,\n",
    "    CASE \n",
    "        WHEN p.age < 18 THEN '0-17 ans'\n",
    "        WHEN p.age < 30 THEN '18-29 ans'\n",
    "        WHEN p.age < 50 THEN '30-49 ans'\n",
    "        WHEN p.age < 65 THEN '50-64 ans'\n",
    "        ELSE '65+ ans'\n",
    "    END as tranche_age,\n",
    "    COUNT(*) as nb_consultations\n",
    "FROM fait_consultation_tmp f\n",
    "JOIN dim_patient_tmp p ON f.id_patient = p.id_patient\n",
    "GROUP BY p.sexe, CASE \n",
    "    WHEN p.age < 18 THEN '0-17 ans'\n",
    "    WHEN p.age < 30 THEN '18-29 ans'\n",
    "    WHEN p.age < 50 THEN '30-49 ans'\n",
    "    WHEN p.age < 65 THEN '50-64 ans'\n",
    "    ELSE '65+ ans'\n",
    "END\n",
    "ORDER BY p.sexe, tranche_age\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST 4 : Top 10 sp√©cialit√©s m√©dicales\n",
      "\n",
      "+-----------------------------------+----------------+-------------------+\n",
      "|nom_specialite                     |nb_consultations|patients_differents|\n",
      "+-----------------------------------+----------------+-------------------+\n",
      "|Medecine Generale                  |519780          |99246              |\n",
      "|Psychiatrie                        |54336           |42345              |\n",
      "|Anesthesie-reanimation             |48083           |38516              |\n",
      "|Radio-diagnostic                   |36867           |31041              |\n",
      "|Pediatrie                          |33738           |28868              |\n",
      "|Cardiologie et maladies vasculaires|28751           |25132              |\n",
      "|Ophtalmologie                      |24374           |21761              |\n",
      "|Medecine du travail                |24156           |21549              |\n",
      "|Gynecologie-obstetrique            |19063           |17424              |\n",
      "|Chirurgie generale                 |16566           |15368              |\n",
      "+-----------------------------------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST 4 : Top sp√©cialit√©s m√©dicales\n",
    "print(\"üîç TEST 4 : Top 10 sp√©cialit√©s m√©dicales\\n\")\n",
    "\n",
    "# Lire professionnel\n",
    "df_prof = spark.read.parquet(\"/home/jovyan/data/gold/dim_professionnel\")\n",
    "df_prof.createOrReplaceTempView(\"dim_professionnel_tmp\")\n",
    "\n",
    "query4 = \"\"\"\n",
    "SELECT\n",
    "    prof.nom_specialite,\n",
    "    COUNT(*) as nb_consultations,\n",
    "    COUNT(DISTINCT f.id_patient) as patients_differents\n",
    "FROM fait_consultation_tmp f\n",
    "JOIN dim_professionnel_tmp prof ON f.id_prof = prof.id_prof\n",
    "WHERE prof.nom_specialite IS NOT NULL\n",
    "GROUP BY prof.nom_specialite\n",
    "ORDER BY nb_consultations DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query4).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç TEST 5 : Distribution g√©ographique par r√©gion\n",
      "\n",
      "+--------------------------+----------+-----------------+---------+\n",
      "|region                    |abv_region|nb_etablissements|nb_villes|\n",
      "+--------------------------+----------+-----------------+---------+\n",
      "|Nouvelle-Aquitaine        |NAQ       |40               |37       |\n",
      "|Auvergne-RhÔøΩne-Alpes      |ARA       |24               |24       |\n",
      "|Grand Est                 |GES       |23               |22       |\n",
      "|Normandie                 |NOR       |18               |18       |\n",
      "|Centre-Val de Loire       |CVL       |15               |15       |\n",
      "|Hauts-de-France           |HDF       |15               |15       |\n",
      "|Occitanie                 |OCC       |13               |13       |\n",
      "|Ile-de-France             |IDF       |11               |11       |\n",
      "|Bourgogne-Franche-ComtÔøΩ   |BFC       |11               |11       |\n",
      "|Bretagne                  |BRE       |8                |6        |\n",
      "|Pays de la Loire          |PDL       |8                |8        |\n",
      "|Provence-Alpes-CÔøΩte d'Azur|PAC       |7                |7        |\n",
      "+--------------------------+----------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST 5 : Distribution g√©ographique (avec d√©partements enrichis)\n",
    "print(\"üîç TEST 5 : Distribution g√©ographique par r√©gion\\n\")\n",
    "\n",
    "# Lire √©tablissement\n",
    "df_etab = spark.read.parquet(\"/home/jovyan/data/gold/dim_etablissement\")\n",
    "df_etab.createOrReplaceTempView(\"dim_etablissement_tmp\")\n",
    "\n",
    "query5 = \"\"\"\n",
    "SELECT\n",
    "    e.libelle_region as region,\n",
    "    e.abv_region,\n",
    "    COUNT(DISTINCT e.finess) as nb_etablissements,\n",
    "    COUNT(DISTINCT e.ville) as nb_villes\n",
    "FROM dim_etablissement_tmp e\n",
    "WHERE e.libelle_region IS NOT NULL\n",
    "GROUP BY e.libelle_region, e.abv_region\n",
    "ORDER BY nb_etablissements DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù √âTAPE 5 : Informations de connexion pour Superset\n",
    "\n",
    "Voici les param√®tres √† utiliser dans Superset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "üìä CONFIGURATION SUPERSET - CHU DATA LAKEHOUSE\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "üåê URL Superset : http://localhost:8088\n",
    "\n",
    "üîê Identifiants :\n",
    "   Username : admin\n",
    "   Password : admin\n",
    "\n",
    "üîå Connexion Database (Settings ‚Üí Database Connections) :\n",
    "\n",
    "   Database Type : Apache Spark SQL\n",
    "   \n",
    "   SQLAlchemy URI :\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   hive://spark-master:10000/default\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   \n",
    "   OU (si PyHive install√©) :\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "   hive://spark-master:10000/default?auth=NOSASL\n",
    "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "   Display Name : CHU_Gold_Layer\n",
    "\n",
    "üìä Tables disponibles (6) :\n",
    "   ‚Ä¢ dim_temps              (4,748 lignes)\n",
    "   ‚Ä¢ dim_patient            (100,000 lignes)\n",
    "   ‚Ä¢ dim_diagnostic         (100 lignes)\n",
    "   ‚Ä¢ dim_professionnel      (100,000 lignes)\n",
    "   ‚Ä¢ dim_etablissement      (3,500 lignes)\n",
    "   ‚Ä¢ fait_consultation      (1,027,157 lignes)\n",
    "\n",
    "‚úÖ NEXT STEPS :\n",
    "   1. Ouvrir http://localhost:8088\n",
    "   2. Login avec admin/admin\n",
    "   3. Settings ‚Üí Database Connections ‚Üí + DATABASE\n",
    "   4. Copier l'URI ci-dessus\n",
    "   5. Test Connection ‚Üí CONNECT\n",
    "   6. Data ‚Üí Datasets ‚Üí Ajouter les 6 tables\n",
    "   7. Cr√©er des graphiques et dashboards !\n",
    "\n",
    "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ SETUP TERMIN√â\n",
    "\n",
    "### üéØ Ce qui a √©t√© fait :\n",
    "\n",
    "1. ‚úÖ 6 tables Spark SQL cr√©√©es\n",
    "2. ‚úÖ Partitions de `fait_consultation` r√©par√©es\n",
    "3. ‚úÖ Toutes les tables v√©rifi√©es (1M+ lignes au total)\n",
    "4. ‚úÖ Requ√™tes de test ex√©cut√©es avec succ√®s\n",
    "5. ‚úÖ Thrift Server actif sur port 10000\n",
    "\n",
    "### üìö Documentation :\n",
    "\n",
    "Voir **TUTORIEL_SUPERSET.md** pour le guide complet de configuration.\n",
    "\n",
    "**üéì Workflow complet valid√©** :\n",
    "```\n",
    "CSV/PostgreSQL ‚Üí Bronze ‚Üí Silver ‚Üí Gold ‚Üí Spark SQL ‚Üí Superset Dashboards\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
