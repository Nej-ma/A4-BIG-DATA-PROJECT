{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# GOLD LAYER - Modèle Dimensionnel (Star Schema)\n",
    "\n",
    "**Flux ETL** : Bronze (brut) → Silver (nettoyé) → **Gold (métier)**\n",
    "\n",
    "**Auteurs** : Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING\n",
    "\n",
    "---\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "### Créer un modèle en étoile (Star Schema) optimisé :\n",
    "1. **5 Dimensions** : Temps, Patient, Diagnostic, Professionnel, Établissement\n",
    "2. **4 Tables de Faits** : Consultations, Hospitalisations, Décès, Satisfaction\n",
    "3. **Partitionnement** temporel (année/mois)\n",
    "4. **Optimisations** Spark SQL\n",
    "\n",
    "## Architecture du modèle\n",
    "Exemple FAIT_CONSULTATION\n",
    "```\n",
    "         dim_temps\n",
    "             |\n",
    "dim_patient --+-- FAIT_CONSULTATION -- dim_professionnel\n",
    "             |\n",
    "        dim_diagnostic\n",
    "             |\n",
    "      dim_etablissement\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import subprocess\n",
    "subprocess.run([\"rm\", \"-rf\", \"/home/jovyan/data/gold/*\"], shell=True)\n",
    "print(\"Imports loaded successfully\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.5.0 started with optimizations\n",
      "Source: /home/jovyan/data/silver (SILVER - cleaned data)\n",
      "Destination: /home/jovyan/data/gold\n"
     ]
    }
   ],
   "source": [
    "# Configuration Spark avec optimisations avancées\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CHU_Gold_Star_Schema\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark {spark.version} started with optimizations\")\n",
    "\n",
    "# Paths via environment (dynamic, no hardcoding)\n",
    "import os\n",
    "DATA_BASE = os.getenv('DATA_BASE', '/home/jovyan/data')\n",
    "SILVER_BASE = f\"{DATA_BASE}/silver\"\n",
    "GOLD_OUTPUT = f\"{DATA_BASE}/gold\"\n",
    "BRONZE_BASE = f\"{DATA_BASE}/bronze\"\n",
    "\n",
    "print(f\"Source: {SILVER_BASE} (SILVER - cleaned data)\")\n",
    "print(f\"Destination: {GOLD_OUTPUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ETAPE 1 : Création des Dimensions\n",
    "\n",
    "Les dimensions contiennent les attributs descriptifs pour l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "safe-delete-utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: safe delete of output directory before overwrite (handles locked folders)\n",
    "import os, shutil\n",
    "def rm_rf(path: str):\n",
    "    try:\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path, ignore_errors=True)\n",
    "        elif os.path.exists(path):\n",
    "            os.remove(path)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: could not pre-delete {path}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIMENSION: dim_temps\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "SparkSession does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     24\u001b[0m     current \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m schema_temps \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     27\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid_temps\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     28\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_complete\u001b[39m\u001b[38;5;124m\"\u001b[39m, DateType(), \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumero_jour_semaine\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     36\u001b[0m ])\n\u001b[0;32m---> 38\u001b[0m dim_temps \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_temps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdim_temps\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m days created (2013-2025)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m dim_temps\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1383\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1381\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[1;32m   1385\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1386\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHOULD_NOT_DATAFRAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1387\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   1388\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: SparkSession does not exist in the JVM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "# 1.1 DIMENSION TEMPS (générée, pas depuis Silver)\n",
    "print(\"=\"*80)\n",
    "print(\"DIMENSION: dim_temps\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Créer une dimension temps complète (2013-2025)\n",
    "start_date = datetime(2013, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "dates = []\n",
    "\n",
    "current = start_date\n",
    "while current <= end_date:\n",
    "    dates.append((\n",
    "        current.strftime(\"%Y%m%d\"),     # id_temps\n",
    "        current,                          # date\n",
    "        current.year,                     # annee\n",
    "        current.month,                    # mois\n",
    "        (current.month - 1) // 3 + 1,     # trimestre\n",
    "        current.strftime(\"%A\"),           # jour_semaine\n",
    "        current.strftime(\"%B\"),           # nom_mois\n",
    "        current.weekday() >= 5,           # est_weekend\n",
    "        current.weekday()                 # numero_jour_semaine\n",
    "    ))\n",
    "    current += timedelta(days=1)\n",
    "\n",
    "schema_temps = StructType([\n",
    "    StructField(\"id_temps\", StringType(), False),\n",
    "    StructField(\"date_complete\", DateType(), False),\n",
    "    StructField(\"annee\", IntegerType(), False),\n",
    "    StructField(\"mois\", IntegerType(), False),\n",
    "    StructField(\"trimestre\", IntegerType(), False),\n",
    "    StructField(\"jour_semaine\", StringType(), True),\n",
    "    StructField(\"nom_mois\", StringType(), True),\n",
    "    StructField(\"est_weekend\", BooleanType(), True),\n",
    "    StructField(\"numero_jour_semaine\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "dim_temps = spark.createDataFrame(dates, schema=schema_temps)\n",
    "\n",
    "print(f\"{dim_temps.count():,} days created (2013-2025)\")\n",
    "dim_temps.show(5)\n",
    "\n",
    "# Sauvegarde SANS partitionnement pour compatibilité Hive/Superset\n",
    "dim_temps.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/dim_temps\")\n",
    "\n",
    "print(f\"Saved to: {GOLD_OUTPUT}/dim_temps (no partitioning)\")\n",
    "print(f\"Note: No partitioning for Hive/Superset compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 DIMENSION PATIENT (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔷 DIMENSION: dim_patient\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charger depuis SILVER (données déjà anonymisées et nettoyées)\n",
    "df_patient_silver = spark.read.parquet(f\"{SILVER_BASE}/patient\")\n",
    "\n",
    "# Sélection pour dimension (déjà propre depuis Silver)\n",
    "dim_patient = df_patient_silver.select(\n",
    "    F.col(\"id_patient\"),\n",
    "    F.col(\"nom_hash\"),         # Déjà anonymisé en Silver\n",
    "    F.col(\"prenom_hash\"),      # Déjà anonymisé en Silver\n",
    "    F.col(\"sexe\"),\n",
    "    F.col(\"age\"),\n",
    "    F.col(\"date_naissance\"),   # Déjà formaté en Silver\n",
    "    F.col(\"ville\"),\n",
    "    F.col(\"code_postal\"),\n",
    "    F.col(\"pays\"),\n",
    "    F.col(\"groupe_sanguin\")\n",
    ")\n",
    "\n",
    "print(f\" {dim_patient.count():,} patients\")\n",
    "dim_patient.show(5, truncate=False)\n",
    "\n",
    "# Sauvegarde\n",
    "dim_patient.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/dim_patient\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/dim_patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 DIMENSION DIAGNOSTIC (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔷 DIMENSION: dim_diagnostic\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_diagnostic_silver = spark.read.parquet(f\"{SILVER_BASE}/diagnostic\")\n",
    "\n",
    "# Vérifier les colonnes disponibles\n",
    "print(f\"Colonnes disponibles: {df_diagnostic_silver.columns}\")\n",
    "\n",
    "dim_diagnostic = df_diagnostic_silver.select(\n",
    "    F.col(\"Code_diag\").alias(\"code_diag\"),\n",
    "    F.col(\"Diagnostic\").alias(\"libelle\"),\n",
    "    # Ajout catégorie CIM-10 (première lettre du code)\n",
    "    F.col(\"Code_diag\").substr(1, 1).alias(\"categorie\")\n",
    ").dropDuplicates([\"code_diag\"])\n",
    "\n",
    "print(f\" {dim_diagnostic.count():,} diagnostics\")\n",
    "dim_diagnostic.show(5, truncate=False)\n",
    "\n",
    "dim_diagnostic.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/dim_diagnostic\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/dim_diagnostic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 DIMENSION PROFESSIONNEL (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔷 DIMENSION: dim_professionnel\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_prof_silver = spark.read.parquet(f\"{SILVER_BASE}/professionnel_de_sante\")\n",
    "\n",
    "# Vérifier colonnes\n",
    "print(f\"Colonnes disponibles: {df_prof_silver.columns}\")\n",
    "\n",
    "dim_professionnel = df_prof_silver.select(\n",
    "    F.col(\"Identifiant\").alias(\"id_prof\"),\n",
    "    F.col(\"Nom\").alias(\"nom\"),\n",
    "    F.col(\"Prenom\").alias(\"prenom\"),\n",
    "    F.col(\"Code_specialite\").alias(\"code_specialite\")\n",
    ").dropDuplicates([\"id_prof\"])\n",
    "\n",
    "# Jointure avec spécialités (depuis Silver)\n",
    "df_spec_silver = spark.read.parquet(f\"{SILVER_BASE}/specialites\")\n",
    "print(f\"Colonnes spécialités: {df_spec_silver.columns}\")\n",
    "\n",
    "dim_professionnel = dim_professionnel.join(\n",
    "    df_spec_silver.select(\n",
    "        F.col(\"Code_specialite\"),\n",
    "        F.col(\"Specialite\").alias(\"nom_specialite\")\n",
    "    ),\n",
    "    on=\"code_specialite\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(f\" {dim_professionnel.count():,} professionnels\")\n",
    "dim_professionnel.show(5, truncate=False)\n",
    "\n",
    "dim_professionnel.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/dim_professionnel\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/dim_professionnel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 DIMENSION ETABLISSEMENT (depuis SILVER + enrichi avec DEPARTEMENTS)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔷 DIMENSION: dim_etablissement (enrichie avec régions/départements)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_etab_silver = spark.read.parquet(f\"{SILVER_BASE}/etablissement_sante\")\n",
    "\n",
    "# Lire départements depuis Bronze\n",
    "df_dept = spark.read.parquet(\"/home/jovyan/data/bronze/csv/departements\")\n",
    "print(f\" Départements chargés : {df_dept.count()} départements\")\n",
    "\n",
    "# Créer dimension de base\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "dim_etablissement = df_etab_silver.select(\n",
    "    F.col(\"finess_site\").alias(\"finess\"),\n",
    "    F.col(\"siret_site\").alias(\"siret\"),\n",
    "    F.col(\"raison_sociale\").alias(\"nom\"),\n",
    "    F.col(\"commune\").alias(\"ville\"),\n",
    "    F.col(\"code_postal\"),\n",
    "    F.col(\"telephone\"),\n",
    "    F.col(\"email\"),\n",
    "    # Extraire code département (2 premiers chiffres du code postal)\n",
    "    F.substring(F.col(\"code_postal\"), 1, 2).alias(\"code_departement\")\n",
    ").filter(\n",
    "    F.col(\"finess\").isNotNull()\n",
    ").dropDuplicates([\"finess\"])\n",
    "\n",
    "# Enrichir avec région/département\n",
    "dim_etablissement = dim_etablissement.join(\n",
    "    df_dept.select(\n",
    "        F.col(\"num_departement\"),\n",
    "        F.col(\"libelle_departement\"),\n",
    "        F.col(\"libelle_region\"),\n",
    "        F.col(\"abv_region\")\n",
    "    ),\n",
    "    dim_etablissement[\"code_departement\"] == df_dept[\"num_departement\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(f\" {dim_etablissement.count():,} établissements (enrichis avec régions)\")\n",
    "dim_etablissement.show(5, truncate=False)\n",
    "\n",
    "dim_etablissement.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/dim_etablissement\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/dim_etablissement\")\n",
    "print(f\"   - Colonnes géographiques ajoutées: libelle_departement, libelle_region, abv_region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÉTAPE 2 : Création des Tables de Faits\n",
    "\n",
    "Les faits contiennent les mesures et les clés étrangères vers les dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 FAIT CONSULTATION (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FAIT: fait_consultation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_consultation_silver = spark.read.parquet(f\"{SILVER_BASE}/consultation\")\n",
    "\n",
    "# Vérification des colonnes disponibles\n",
    "print(\" Colonnes disponibles dans consultation Silver:\")\n",
    "for col_name in df_consultation_silver.columns:\n",
    "    print(f\"  - {col_name}\")\n",
    "print()\n",
    "\n",
    "# Transformation pour fait avec les colonnes correctes (selon l'erreur précédente)\n",
    "fait_consultation = df_consultation_silver.select(\n",
    "    F.col(\"id_consultation\"),\n",
    "    F.col(\"id_patient\"),\n",
    "    F.col(\"id_professionnel\").alias(\"id_prof\"),\n",
    "    F.col(\"id_diagnostic\").alias(\"code_diag\"),\n",
    "    F.col(\"id_mutuelle\"),  # Correction: \"id_mutuelle\" au lieu de \"id_salle\"\n",
    "    \n",
    "    # Clé temporelle\n",
    "    F.date_format(F.col(\"date_consultation\"), \"yyyyMMdd\").alias(\"id_temps\"),\n",
    "    F.col(\"date_consultation\"),\n",
    "    \n",
    "    # Dimensions temporelles (pour partitionnement)\n",
    "    F.col(\"annee\"),\n",
    "    F.col(\"mois\"),\n",
    "    F.col(\"jour\"),\n",
    "    \n",
    "    # Mesures disponibles (selon données Silver)\n",
    "    F.col(\"heure_debut\"),\n",
    "    F.col(\"heure_fin\"),\n",
    "    F.col(\"motif\")  # Correction: \"motif\" au lieu de \"notes\"\n",
    ")\n",
    "\n",
    "print(f\" {fait_consultation.count():,} consultations\")\n",
    "fait_consultation.show(5)\n",
    "\n",
    "# Statistiques\n",
    "print(\"\\n Statistiques:\")\n",
    "fait_consultation.select(\n",
    "    F.count(\"*\").alias(\"total_consultations\"),\n",
    "    F.countDistinct(\"id_patient\").alias(\"patients_uniques\"),\n",
    "    F.countDistinct(\"id_prof\").alias(\"professionnels_uniques\"),\n",
    "    F.min(\"annee\").alias(\"annee_min\"),\n",
    "    F.max(\"annee\").alias(\"annee_max\")\n",
    ").show()\n",
    "\n",
    "# Sauvegarde avec PARTITIONNEMENT par année et mois\n",
    "print(\"\\n Sauvegarde avec optimisations (partitionnement)...\")\n",
    "fait_consultation.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"annee\", \"mois\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/fait_consultation\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/fait_consultation\")\n",
    "print(f\"   - Partitionné par: annee, mois\")\n",
    "print(f\"   - Format: Parquet compressé\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 FAIT DÉCÈS (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FAIT: fait_deces\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_deces_silver = spark.read.parquet(f\"{SILVER_BASE}/deces_2019\")\n",
    "\n",
    "fait_deces = df_deces_silver.select(\n",
    "    F.monotonically_increasing_id().alias(\"id_deces\"),\n",
    "    \n",
    "    # Identités anonymisées (depuis Silver)\n",
    "    F.col(\"nom_hash\"),\n",
    "    F.col(\"prenom_hash\"),\n",
    "    F.col(\"acte_deces_hash\"),\n",
    "    \n",
    "    # Données démographiques\n",
    "    F.col(\"sexe\"),\n",
    "    F.col(\"date_naissance\"),\n",
    "    F.col(\"date_deces\"),\n",
    "    F.col(\"age_deces\"),\n",
    "    \n",
    "    # Clé temporelle\n",
    "    F.date_format(F.col(\"date_deces\"), \"yyyyMMdd\").alias(\"id_temps\"),\n",
    "    F.col(\"annee_deces\").alias(\"annee\"),\n",
    "    F.col(\"mois_deces\").alias(\"mois\"),\n",
    "    \n",
    "    # Lieux\n",
    "    F.col(\"code_lieu_naissance\"),\n",
    "    F.col(\"lieu_naissance\"),\n",
    "    F.col(\"pays_naissance\"),\n",
    "    F.col(\"code_lieu_deces\")\n",
    ")\n",
    "\n",
    "print(f\" {fait_deces.count():,} décès (2019)\")\n",
    "fait_deces.show(5)\n",
    "\n",
    "# Statistiques\n",
    "print(\"\\n📈 Statistiques:\")\n",
    "fait_deces.select(\n",
    "    F.count(\"*\").alias(\"total_deces\"),\n",
    "    F.avg(\"age_deces\").alias(\"age_moyen\"),\n",
    "    F.min(\"age_deces\").alias(\"age_min\"),\n",
    "    F.max(\"age_deces\").alias(\"age_max\")\n",
    ").show()\n",
    "\n",
    "# Sauvegarde\n",
    "fait_deces.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"annee\", \"mois\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/fait_deces\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/fait_deces (partitionné par annee, mois)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7v8n7ne9fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 FAIT HOSPITALISATION (depuis tables AAAA + date - VRAIES DONNÉES)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FAIT: fait_hospitalisation (VRAIES DONNÉES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ÉTAPE 1: Charger les tables Bronze AAAA et date\n",
    "print(\"📖 Chargement tables AAAA et date depuis Bronze...\")\n",
    "df_aaaa = spark.read.parquet(f\"{SILVER_BASE}/../bronze/postgres/AAAA\") \\\n",
    "    .drop(\"ingestion_timestamp\", \"ingestion_date\")\n",
    "df_date = spark.read.parquet(f\"{SILVER_BASE}/../bronze/postgres/date\") \\\n",
    "    .drop(\"ingestion_timestamp\", \"ingestion_date\")\n",
    "\n",
    "print(f\"   - AAAA: {df_aaaa.count():,} lignes\")\n",
    "print(f\"   - date: {df_date.count():,} lignes\")\n",
    "\n",
    "# ÉTAPE 2: Ajouter row_id pour jointure par position (les 2 tables ont même nb lignes)\n",
    "from pyspark.sql.functions import monotonically_increasing_id, to_date, datediff\n",
    "\n",
    "df_aaaa_idx = df_aaaa.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "df_date_idx = df_date.withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "\n",
    "# ÉTAPE 3: Jointure par position\n",
    "df_hospit_raw = df_aaaa_idx.join(df_date_idx, \"row_id\", \"inner\")\n",
    "\n",
    "print(f\" Jointure: {df_hospit_raw.count():,} hospitalisations\")\n",
    "\n",
    "# ÉTAPE 4: Transformation et nettoyage\n",
    "fait_hospitalisation = df_hospit_raw.select(\n",
    "    F.monotonically_increasing_id().alias(\"id_hospitalisation\"),\n",
    "\n",
    "    # Clés étrangères\n",
    "    F.col(\"Num\").alias(\"id_patient\"),\n",
    "    F.col(\"Code_diag\").alias(\"code_diag\"),\n",
    "\n",
    "    # Dates (conversion dd/MM/yyyy → date)\n",
    "    to_date(F.col(\"date1\"), \"dd/MM/yyyy\").alias(\"date_entree\"),\n",
    "    to_date(F.col(\"date2\"), \"dd/MM/yyyy\").alias(\"date_sortie\"),\n",
    "\n",
    "    # Clés temporelles\n",
    "    F.date_format(to_date(F.col(\"date1\"), \"dd/MM/yyyy\"), \"yyyyMMdd\").alias(\"id_temps_entree\"),\n",
    "    F.date_format(to_date(F.col(\"date2\"), \"dd/MM/yyyy\"), \"yyyyMMdd\").alias(\"id_temps_sortie\"),\n",
    "\n",
    "    # Mesures\n",
    "    F.datediff(\n",
    "        to_date(F.col(\"date2\"), \"dd/MM/yyyy\"),\n",
    "        to_date(F.col(\"date1\"), \"dd/MM/yyyy\")\n",
    "    ).alias(\"duree_sejour_jours\"),\n",
    "\n",
    "    # Dimensions temporelles pour partitionnement\n",
    "    F.year(to_date(F.col(\"date1\"), \"dd/MM/yyyy\")).alias(\"annee\"),\n",
    "    F.month(to_date(F.col(\"date1\"), \"dd/MM/yyyy\")).alias(\"mois\")\n",
    ")\n",
    "\n",
    "# ÉTAPE 5: Filtrer données invalides (dates nulles ou négatives)\n",
    "fait_hospitalisation = fait_hospitalisation.filter(\n",
    "    (F.col(\"date_entree\").isNotNull()) &\n",
    "    (F.col(\"date_sortie\").isNotNull()) &\n",
    "    (F.col(\"duree_sejour_jours\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"{fait_hospitalisation.count():,} hospitalisations valides\")\n",
    "fait_hospitalisation.show(10)\n",
    "\n",
    "# ÉTAPE 6: Statistiques\n",
    "print(\"\\n Statistiques:\")\n",
    "fait_hospitalisation.select(\n",
    "    F.count(\"*\").alias(\"total_hospitalisations\"),\n",
    "    F.countDistinct(\"id_patient\").alias(\"patients_uniques\"),\n",
    "    F.countDistinct(\"code_diag\").alias(\"diagnostics_uniques\"),\n",
    "    F.avg(\"duree_sejour_jours\").alias(\"duree_moyenne_jours\"),\n",
    "    F.min(\"duree_sejour_jours\").alias(\"duree_min\"),\n",
    "    F.max(\"duree_sejour_jours\").alias(\"duree_max\"),\n",
    "    F.min(\"annee\").alias(\"annee_min\"),\n",
    "    F.max(\"annee\").alias(\"annee_max\")\n",
    ").show()\n",
    "\n",
    "# Distribution par année\n",
    "print(\"\\n📅 Distribution par année:\")\n",
    "fait_hospitalisation.groupBy(\"annee\").agg(\n",
    "    F.count(\"*\").alias(\"nb_hospitalisations\"),\n",
    "    F.avg(\"duree_sejour_jours\").alias(\"duree_moyenne\")\n",
    ").orderBy(\"annee\").show()\n",
    "\n",
    "# ÉTAPE 7: Sauvegarde avec partitionnement\n",
    "print(\"\\nSauvegarde avec partitionnement par année et mois...\")\n",
    "fait_hospitalisation.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"annee\", \"mois\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/fait_hospitalisation\")\n",
    "\n",
    "print(f\"Sauvegardé: {GOLD_OUTPUT}/fait_hospitalisation\")\n",
    "print(f\"   - Partitionné par: annee, mois\")\n",
    "print(f\"   - Format: Parquet compressé\")\n",
    "print(f\"   - Source: Tables AAAA + date (vraies données)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 FAIT SATISFACTION (depuis SILVER)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" FAIT: fait_satisfaction\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_satis_silver = spark.read.parquet(f\"{SILVER_BASE}/satisfaction_2019\")\n",
    "\n",
    "fait_satisfaction = df_satis_silver.select(\n",
    "    F.monotonically_increasing_id().alias(\"id_satisfaction\"),\n",
    "    F.col(\"finess\"),\n",
    "    F.lit(\"20190101\").alias(\"id_temps\"),\n",
    "    F.col(\"annee\"),\n",
    "    \n",
    "    # Scores (déjà typés correctement depuis Silver)\n",
    "    F.col(\"score_global\"),\n",
    "    F.col(\"score_accueil\"),\n",
    "    F.col(\"score_pec_infirmier\"),\n",
    "    F.col(\"score_pec_medical\"),\n",
    "    F.col(\"score_chambre\"),\n",
    "    F.col(\"score_repas\"),\n",
    "    F.col(\"score_sortie\"),\n",
    "    \n",
    "    # Métriques\n",
    "    F.col(\"taux_recommandation\"),\n",
    "    F.col(\"nb_reponses_global\").alias(\"nb_repondants\"),\n",
    "    F.col(\"nb_recommandations\"),\n",
    "    \n",
    "    # Classification\n",
    "    F.col(\"classement\"),\n",
    "    F.col(\"evolution\")\n",
    ")\n",
    "\n",
    "print(f\"{fait_satisfaction.count():,} évaluations de satisfaction (2019)\")\n",
    "fait_satisfaction.show(5)\n",
    "\n",
    "# Statistiques\n",
    "print(\"\\n Statistiques:\")\n",
    "fait_satisfaction.select(\n",
    "    F.count(\"*\").alias(\"total_evaluations\"),\n",
    "    F.avg(\"score_global\").alias(\"score_global_moyen\"),\n",
    "    F.avg(\"taux_recommandation\").alias(\"taux_reco_moyen\"),\n",
    "    F.sum(\"nb_repondants\").alias(\"total_repondants\")\n",
    ").show()\n",
    "\n",
    "# Sauvegarde\n",
    "fait_satisfaction.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"annee\") \\\n",
    "    .parquet(f\"{GOLD_OUTPUT}/fait_satisfaction\")\n",
    "\n",
    "print(f\" Sauvegardé: {GOLD_OUTPUT}/fait_satisfaction (partitionné par annee)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ÉTAPE 3 : Vérification et Validation du Modèle Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inventaire complet du modèle Gold\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" MODÈLE GOLD - INVENTAIRE COMPLET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gold_tables = []\n",
    "\n",
    "for table in os.listdir(GOLD_OUTPUT):\n",
    "    path = f\"{GOLD_OUTPUT}/{table}\"\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        count = df.count()\n",
    "        cols = len(df.columns)\n",
    "        table_type = \"DIMENSION\" if table.startswith(\"dim_\") else \"FAIT\"\n",
    "        \n",
    "        gold_tables.append({\n",
    "            \"table\": table,\n",
    "            \"rows\": count,\n",
    "            \"columns\": cols,\n",
    "            \"type\": table_type\n",
    "        })\n",
    "        \n",
    "        print(f\"{'✅' if table_type == 'DIMENSION' else '...'} {table:30s} | {count:>10,} lignes | {cols:>2} colonnes | {table_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  {table} - Erreur: {e}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques globales\n",
    "import pandas as pd\n",
    "\n",
    "if len(gold_tables) > 0:\n",
    "    df_stats = pd.DataFrame(gold_tables)\n",
    "    \n",
    "    print(\"\\n RÉSUMÉ PAR TYPE\")\n",
    "    print(\"=\"*50)\n",
    "    summary = df_stats.groupby('type').agg({\n",
    "        'table': 'count',\n",
    "        'rows': 'sum',\n",
    "        'columns': 'sum'\n",
    "    }).rename(columns={'table': 'nb_tables'})\n",
    "    print(summary)\n",
    "    \n",
    "    print(\"\\n📋 DÉTAIL DES TABLES\")\n",
    "    print(\"=\"*50)\n",
    "    display(df_stats)\n",
    "    \n",
    "    total_lignes = df_stats['rows'].F.sum()\n",
    "    print(f\"\\n TOTAL GOLD LAYER: {total_lignes:,} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86bc04-f3bd-40ce-9b32-585bfd7d06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAIT_HOSPITALISATION depuis épisodes de consultations (Silver)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "cons = (spark.read.parquet(f\"{SILVER_BASE}/consultation\")\n",
    "        .select('id_patient','id_diagnostic','date_consultation')\n",
    "        .withColumn('date_consultation', F.col('date_consultation').cast('date'))\n",
    ")\n",
    "\n",
    "w = W.partitionBy('id_patient').orderBy(F.col('date_consultation'))\n",
    "w_cum = w.rowsBetween(W.unboundedPreceding, W.currentRow)\n",
    "\n",
    "prev = F.lag('date_consultation',1).over(w)\n",
    "new_ep = (prev.isNull() | (F.datediff(F.col('date_consultation'), prev) > 1)).cast('int')\n",
    "\n",
    "seq = (cons\n",
    "       .withColumn('new_ep', new_ep)\n",
    "       .withColumn('episode_seq', F.sum(F.col('new_ep')).over(w_cum))\n",
    "       .withColumn('episode_id', F.concat_ws('_', F.col('id_patient').cast('string'), F.col('episode_seq').cast('string')))\n",
    ")\n",
    "\n",
    "# Mode du diagnostic par épisode\n",
    "dc = seq.groupBy('episode_id','id_patient','id_diagnostic').count()\n",
    "rnk = W.partitionBy('episode_id').orderBy(F.col('count').desc(), F.col('id_diagnostic'))\n",
    "top = dc.withColumn('r', F.row_number().over(rnk)).filter(F.col('r')==1)\n",
    "\n",
    "eps = (seq.groupBy('id_patient','episode_id')\n",
    "          .agg(F.min('date_consultation').alias('date_entree'),\n",
    "               F.max('date_consultation').alias('date_sortie'),\n",
    "               F.count('*').alias('nb_consultations'),\n",
    "               F.countDistinct('date_consultation').alias('nb_jours_distincts'))\n",
    "       .join(top.select('episode_id', F.col('id_diagnostic').alias('code_diag')), 'episode_id','left'))\n",
    "\n",
    "eps = eps.withColumn('duree_sejour_jours', F.datediff(F.col('date_sortie'), F.col('date_entree')) + F.lit(1))\n",
    "eps = eps.filter( (F.col('duree_sejour_jours')>=2) | (F.col('nb_jours_distincts')>=2) )\n",
    "\n",
    "fait_h = (eps\n",
    "          .withColumn('id_hospitalisation', F.monotonically_increasing_id())\n",
    "          .withColumn('id_temps_entree', F.date_format(F.col('date_entree'),'yyyyMMdd'))\n",
    "          .withColumn('id_temps_sortie', F.date_format(F.col('date_sortie'),'yyyyMMdd'))\n",
    "          .withColumn('annee', F.year(F.col('date_entree')))\n",
    "          .withColumn('mois', F.month(F.col('date_entree')))\n",
    "          .select('id_hospitalisation','id_patient','code_diag','date_entree','date_sortie',\n",
    "                  'id_temps_entree','id_temps_sortie','duree_sejour_jours','nb_consultations','annee','mois')\n",
    ")\n",
    "\n",
    "print(f\"Episodes: {fait_h.count()}\")\n",
    "fait_h.write.mode('overwrite').partitionBy('annee','mois').parquet(f\"{GOLD_OUTPUT}/fait_hospitalisation\")\n",
    "print(f\"💾 Sauvegardé: {GOLD_OUTPUT}/fait_hospitalisation (partitionné annee/mois)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292101e-9e61-4cbf-b59a-2a4f0424be97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## MODÈLE GOLD STAR SCHEMA TERMINÉ\n",
    "\n",
    "### Ce qui a été créé :\n",
    "\n",
    "#### 🔷 Dimensions (5) :\n",
    "- **dim_temps** : Dimension temporelle complète (2013-2025)\n",
    "- **dim_patient** : Patients anonymisés (depuis Silver)\n",
    "- **dim_diagnostic** : Codes diagnostics\n",
    "- **dim_professionnel** : Professionnels de santé + spécialités\n",
    "- **dim_etablissement** : Établissements de santé (données publiques)\n",
    "\n",
    "#### 4 faits :\n",
    "- **fait_consultation** : Consultations médicales (partitionné année/mois)\n",
    "- **fait_hospitalisation** : Hospitalisations avec durée séjour (partitionné année/mois) ← NOUVEAU\n",
    "- **fait_deces** : Décès 2019 anonymisés (partitionné année/mois)\n",
    "- **fait_satisfaction** : Scores satisfaction E-Satis 2019 (partitionné année)\n",
    "\n",
    "### Optimisations :\n",
    "- **Partitionnement temporel** (année/mois pour requêtes rapides)\n",
    "- **Format Parquet** (compression ~10x vs CSV)\n",
    "- **Adaptive Query Execution** (optimisations Spark automatiques)\n",
    "- **Anonymisation RGPD** (hash SHA-256 des données sensibles)\n",
    "- **Typage correct** (integer, double, date)\n",
    "- **Dédoublonnage** (clés primaires uniques)\n",
    "\n",
    "\n",
    "\n",
    "**Note importante** : Ce modèle Gold est construit depuis les données **Silver** (nettoyées et anonymisées), pas depuis Bronze. Flux ETLT.\n",
    "\n",
    "**Fait_hospitalisation** : Construit depuis des épisodes dérivés des consultations Silver (dates d’entrée/sortie et durées)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
