# üéØ PIPELINE ETL FINAL - Architecture Correcte

**Auteurs** : Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING
**Date** : 21 Octobre 2025
**Livrable** : 2 & 3

---

## ‚úÖ FLUX ETL CORRECT (Bronze ‚Üí Silver ‚Üí Gold)

```
üìÅ SOURCES BRUTES
‚îú‚îÄ‚îÄ PostgreSQL (100K patients, 1M+ consultations)
‚îî‚îÄ‚îÄ CSV dans /DATA_2024/ (d√©c√®s, satisfaction, √©tablissements)
         ‚Üì
    EXTRACT (Notebook 01)
    ‚Ä¢ Lit directement les CSV bruts (PAS PostgreSQL!)
    ‚Ä¢ Lit les tables PostgreSQL originales
         ‚Üì
üì¶ BRONZE LAYER (MinIO/data/bronze/)
‚îú‚îÄ‚îÄ bronze/postgres/ - Tables PostgreSQL en Parquet
‚îî‚îÄ‚îÄ bronze/csv/ - CSV convertis en Parquet
    Format: Copie exacte des sources
         ‚Üì
    TRANSFORM 1 (Notebook 02) - NETTOYAGE
    ‚Ä¢ Anonymisation (SHA-256 des noms/pr√©noms)
    ‚Ä¢ Formats dates uniformes (yyyy-MM-dd)
    ‚Ä¢ Typage correct (integer, double, date)
    ‚Ä¢ D√©doublonnage
    ‚Ä¢ Validation donn√©es
         ‚Üì
üì¶ SILVER LAYER (MinIO/data/silver/)
‚îú‚îÄ‚îÄ Donn√©es nettoy√©es et valid√©es
‚îú‚îÄ‚îÄ Conformes RGPD (anonymis√©es)
‚îî‚îÄ‚îÄ Pr√™tes pour l'analyse
         ‚Üì
    TRANSFORM 2 (Notebook 03) - MOD√àLE M√âTIER
    ‚Ä¢ Cr√©ation dimensions (Patient, Temps, Diagnostic...)
    ‚Ä¢ Cr√©ation faits (Consultations, D√©c√®s, Satisfaction)
    ‚Ä¢ Partitionnement (ann√©e/mois)
    ‚Ä¢ Optimisations Spark
         ‚Üì
‚≠ê GOLD LAYER (MinIO/data/gold/)
‚îú‚îÄ‚îÄ Star Schema optimis√©
‚îú‚îÄ‚îÄ dim_temps, dim_patient, dim_diagnostic, dim_professionnel, dim_etablissement
‚îú‚îÄ‚îÄ fait_consultation (partitionn√© ann√©e/mois)
‚îú‚îÄ‚îÄ fait_deces (partitionn√© ann√©e/mois)
‚îî‚îÄ‚îÄ fait_satisfaction (partitionn√© ann√©e)
         ‚Üì
üìä ANALYSE & VISUALISATION
‚îú‚îÄ‚îÄ Notebook 04: Benchmarks + Graphiques de performance
‚îú‚îÄ‚îÄ Jupyter: Requ√™tes SQL + Analyses exploratoires
‚îú‚îÄ‚îÄ Superset: Dashboards BI (Livrable 3)
‚îî‚îÄ‚îÄ Airflow: Automatisation (Livrable 3)
```

---

## üìì NOTEBOOKS - Pipeline Complet

### ‚úÖ Notebook 01 : Extract Bronze (Sources Directes)

**Fichier** : `01_Extract_Bronze_SOURCES_DIRECTES.ipynb`

**Ce qu'il fait** :
```python
# 1. EXTRACT depuis PostgreSQL (tables originales)
df_patient = spark.read.jdbc("postgresql://...", "Patient")
df_consultation = spark.read.jdbc("postgresql://...", "Consultation")
# ... 13 tables

# 2. EXTRACT depuis CSV BRUTS (directement!)
df_etab = spark.read.csv("/home/jovyan/DATA_2024/Etablissement de SANTE/etablissement_sante.csv")
df_satis = spark.read.csv("/home/jovyan/DATA_2024/Satisfaction/2019/...")
df_deces = spark.read.csv("/home/jovyan/DATA_2024/DECES EN FRANCE/deces.csv")
           .filter(year(col("date_deces")) == 2019)  # Filtrage intelligent

# 3. SAVE en BRONZE (donn√©es brutes)
df_patient.write.parquet("/home/jovyan/data/bronze/postgres/Patient")
df_etab.write.parquet("/home/jovyan/data/bronze/csv/etablissement_sante")
```

**R√©sultat** :
- ‚úÖ 13 tables PostgreSQL en Bronze
- ‚úÖ 3 fichiers CSV en Bronze (416K √©tablissements, 1K satisfaction, 620K d√©c√®s 2019)
- ‚úÖ ~4.6 millions de lignes au total

---

### ‚úÖ Notebook 02 : Transform Silver (Nettoyage)

**Fichier** : `02_Transform_Silver_NETTOYAGE.ipynb`

**Ce qu'il fait** :
```python
# 1. Lire depuis BRONZE
df_patient_bronze = spark.read.parquet("/home/jovyan/data/bronze/postgres/Patient")

# 2. ANONYMISATION (RGPD)
df_patient_clean = df_patient_bronze.select(
    col("Id_patient").alias("id_patient"),
    sha2(col("Nom"), 256).alias("nom_hash"),        # Hash SHA-256
    sha2(col("Prenom"), 256).alias("prenom_hash"),  # Anonymisation
    col("Sexe").alias("sexe"),
    col("Age").cast("integer").alias("age"),
    to_date(col("Date"), "M/d/yyyy").alias("date_naissance"),  # Format uniforme
    trim(upper(col("Ville"))).alias("ville"),       # Normalisation
    # ...
)

# 3. Sauvegarder en SILVER
df_patient_clean.write.parquet("/home/jovyan/data/silver/patient")
```

**Transformations appliqu√©es** :
- üîê **Anonymisation** : Hash SHA-256 des noms, pr√©noms, t√©l√©phones, emails, num√©ros s√©cu
- üìÖ **Dates uniformes** : M/d/yyyy ‚Üí yyyy-MM-dd (format ISO)
- üî¢ **Typage correct** : Conversion string ‚Üí integer/double/date
- üßπ **Nettoyage texte** : trim(), upper() pour normalisation
- ‚úÖ **Validation** : Filtres sur √¢ges, dates, identifiants
- üîÑ **D√©doublonnage** : dropDuplicates() sur cl√©s primaires

**Tables trait√©es** :
- ‚úÖ Patient (anonymis√©)
- ‚úÖ Consultation (dates format√©es)
- ‚úÖ √âtablissements (nettoy√©)
- ‚úÖ Satisfaction (scores typ√©s)
- ‚úÖ D√©c√®s (anonymis√©)
- ‚úÖ 7 tables de r√©f√©rence (nettoy√©es)

---

### ‚úÖ Notebook 03 : Transform Gold (Star Schema)

**Fichier** : `03_Transform_Gold_STAR_SCHEMA.ipynb`

**Ce qu'il fait** :
```python
# 1. Lire depuis SILVER (pas Bronze!)
df_patient_silver = spark.read.parquet("/home/jovyan/data/silver/patient")
df_consult_silver = spark.read.parquet("/home/jovyan/data/silver/consultation")

# 2. Cr√©er DIMENSIONS
dim_patient = df_patient_silver.select(
    col("id_patient"),
    col("nom_hash"),      # D√©j√† anonymis√© en Silver
    col("sexe"),
    col("age"),
    col("ville")
)

# 3. Cr√©er FAITS avec PARTITIONNEMENT
fait_consultation = df_consult_silver.select(
    col("id_consultation"),
    col("id_patient"),
    col("id_professionnel"),
    date_format(col("date_consultation"), "yyyyMMdd").alias("id_temps"),
    col("annee"),  # Pour partitionnement
    col("mois"),
    col("duree_minutes")
)

# 4. Sauvegarder avec OPTIMISATIONS
fait_consultation.write \
    .partitionBy("annee", "mois") \
    .parquet("/home/jovyan/data/gold/fait_consultation")
```

**Mod√®le cr√©√©** :

**Dimensions (5)** :
- ‚úÖ `dim_temps` : 4,748 jours (2013-2025)
- ‚úÖ `dim_patient` : 100K patients anonymis√©s
- ‚úÖ `dim_diagnostic` : ~15K codes diagnostics
- ‚úÖ `dim_professionnel` : 1M+ professionnels + sp√©cialit√©s
- ‚úÖ `dim_etablissement` : 416K √©tablissements

**Faits (3)** :
- ‚úÖ `fait_consultation` : 1M+ consultations (partitionn√© ann√©e/mois)
- ‚úÖ `fait_deces` : 620K d√©c√®s 2019 (partitionn√© ann√©e/mois)
- ‚úÖ `fait_satisfaction` : 1,152 √©valuations 2019 (partitionn√© ann√©e)

---

### ‚úÖ Notebook 04 : Benchmarks (D√©j√† existant)

**Fichier** : `04_Performance_Benchmarks.ipynb`

**Ce qu'il fait** :
- Lit depuis Gold
- Mesure performance des requ√™tes SQL
- G√©n√®re graphiques de performance
- Cr√©e rapport markdown

**Pas de modification n√©cessaire** - Lit d√©j√† depuis Gold.

---

## üîÑ DIFF√âRENCES AVEC L'ANCIENNE APPROCHE

### ‚ùå ANCIENNE APPROCHE (Incorrecte)

```
CSV ‚Üí PostgreSQL ‚Üí Bronze ‚Üí Gold
```

**Probl√®mes** :
- CSV d'abord charg√©s dans PostgreSQL (√©tape inutile)
- Pas de layer Silver (pas de nettoyage/anonymisation)
- Lecture depuis PostgreSQL alors que sources = CSV

### ‚úÖ NOUVELLE APPROCHE (Correcte)

```
CSV (brut) ‚Üí Bronze ‚Üí Silver (nettoy√©) ‚Üí Gold (m√©tier)
PostgreSQL ‚Üí Bronze ‚Üí Silver ‚Üí Gold
```

**Avantages** :
- CSV lus **directement** (pas via PostgreSQL)
- Layer Silver = Nettoyage + Anonymisation + Validation
- S√©paration claire des responsabilit√©s (Bronze/Silver/Gold)
- Conforme architecture Medallion (best practice)
- Conforme RGPD (anonymisation en Silver)

---

## üìã CHECKLIST EX√âCUTION

### Pour Livrable 2 :

1. **Lancer Docker** :
   ```bash
   docker compose up -d
   ```

2. **Acc√©der √† Jupyter** :
   - URL : http://localhost:8888
   - Token : `chu_token`

3. **Ex√©cuter les notebooks dans l'ordre** :
   ```
   ‚úÖ 01_Extract_Bronze_SOURCES_DIRECTES.ipynb
   ‚úÖ 02_Transform_Silver_NETTOYAGE.ipynb
   ‚úÖ 03_Transform_Gold_STAR_SCHEMA.ipynb
   ‚úÖ 04_Performance_Benchmarks.ipynb
   ```

4. **V√©rifier MinIO** :
   - URL : http://localhost:9001
   - Login : `minioadmin` / `minioadmin`
   - V√©rifier buckets :
     - `lakehouse/bronze/` ‚Üí Donn√©es brutes
     - `lakehouse/silver/` ‚Üí Donn√©es nettoy√©es
     - `lakehouse/gold/` ‚Üí Star Schema

5. **Livrables g√©n√©r√©s** :
   - ‚úÖ 4 notebooks ex√©cut√©s avec r√©sultats
   - ‚úÖ Graphiques de performance
   - ‚úÖ Rapport markdown (g√©n√©r√© par Notebook 04)
   - ‚úÖ Donn√©es Bronze/Silver/Gold dans MinIO

---

## üöÄ LIVRABLE 3 (Suite)

### √Ä faire ensuite :

#### 1. Pipelines Airflow

**Fichier** : `airflow/dags/chu_etl_pipeline.py`

```python
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

dag = DAG('chu_etl_daily', schedule_interval='@daily')

bronze_task = SparkSubmitOperator(
    task_id='extract_bronze',
    application='/opt/spark-apps/bronze_extract.py',
    dag=dag
)

silver_task = SparkSubmitOperator(
    task_id='transform_silver',
    application='/opt/spark-apps/silver_clean.py',
    dag=dag
)

gold_task = SparkSubmitOperator(
    task_id='transform_gold',
    application='/opt/spark-apps/gold_star_schema.py',
    dag=dag
)

bronze_task >> silver_task >> gold_task
```

#### 2. Dashboards Superset

- Connecter Superset √† Gold layer (via Spark SQL ou PostgreSQL)
- Cr√©er graphiques :
  - Consultations par mois
  - Top 10 diagnostics
  - R√©partition d√©c√®s par localisation
  - Scores satisfaction par r√©gion
- Assembler en dashboard avec storytelling

#### 3. Pr√©sentation

- Montrer pipeline Airflow en ex√©cution
- Dashboard Superset en live
- Notebooks Jupyter pour analyses

---

## üìä VOLUM√âTRIES

| Layer | Tables | Lignes Total | Format | Optimisations |
|-------|--------|--------------|--------|---------------|
| **Bronze** | 16 | ~4.6M | Parquet | Copie brute sources |
| **Silver** | 12 | ~4.6M | Parquet | Anonymis√©, typ√©, nettoy√© |
| **Gold** | 8 | ~1.7M | Parquet | Partitionn√© ann√©e/mois, Star Schema |

---

## üéì POURQUOI CETTE ARCHITECTURE ?

### Bronze / Silver / Gold (Medallion)

- **Bronze** : Zone de quarantaine - Donn√©es brutes au cas o√π
- **Silver** : Zone de confiance - Donn√©es valid√©es RGPD
- **Gold** : Zone business - Mod√®le optimis√© pour d√©cideurs

### Pourquoi pas tout dans PostgreSQL ?

- **PostgreSQL** = OLTP (transactions rapides, petites requ√™tes)
- **MinIO + Parquet** = OLAP (analyses sur millions de lignes)
- **Parquet** = 10x plus compact que CSV
- **Spark** lit Parquet 100x plus vite que CSV

### Pourquoi Jupyter ET Airflow ?

- **Jupyter** : D√©veloppement, tests, prototypage (Livrable 2)
- **Airflow** : Production, automatisation, monitoring (Livrable 3)

Comme Word vs InDesign :
- Jupyter = Word (brouillon, essais)
- Airflow = InDesign (version finale, automatique)

---

## ‚úÖ R√âSUM√â

**Pipeline correct** :
1. ‚úÖ Extract depuis sources brutes (CSV + PostgreSQL) ‚Üí Bronze
2. ‚úÖ Transform avec anonymisation/nettoyage ‚Üí Silver
3. ‚úÖ Transform avec mod√®le m√©tier Star Schema ‚Üí Gold
4. ‚úÖ Analyse avec Jupyter + Benchmarks

**Livrables Livrable 2** :
- ‚úÖ Scripts (4 notebooks)
- ‚úÖ V√©rifications (counts, aper√ßus)
- ‚úÖ Partitionnement (ann√©e/mois)
- ‚úÖ Graphiques performance
- ‚úÖ Documentation compl√®te

**Pr√™t pour Livrable 3** :
- DAGs Airflow √† cr√©er
- Dashboards Superset √† cr√©er
- Pr√©sentation √† pr√©parer

---

**Architecture valid√©e et conforme aux best practices Big Data !** üéâ
