# ğŸ¯ PLAN D'ACTION - Livrable 2 & 3

## âœ… Ã‰TAT ACTUEL (Ce qui fonctionne)

### Infrastructure :
- âœ… PostgreSQL : 100K patients + 1M consultations + CSV (416K Ã©tablissements, 1K satisfaction)
- âœ… MinIO : Buckets crÃ©Ã©s (vides pour l'instant - normal!)
- âœ… Spark Master + Worker : PrÃªts
- âœ… Jupyter : Fonctionnel avec tous les JARs
- âœ… Airflow : **RÃ‰PARÃ‰** (CSRF dÃ©sactivÃ©)
- âœ… Superset : Fonctionnel

---

## ğŸ”„ LE BON FLUX ETL (Ce que tu as compris)

```
ğŸ“ SOURCES BRUTES
â”œâ”€â”€ PostgreSQL (patients, consultations)
â””â”€â”€ CSV dans /data/DATA 2024/ (dÃ©cÃ¨s, satisfaction, Ã©tablissements)
         â†“
    EXTRACT avec Spark
         â†“
ğŸ“¦ BRONZE (MinIO) - DonnÃ©es brutes en Parquet
â”œâ”€â”€ Copie exacte des sources
â””â”€â”€ Format: Parquet (compression)
         â†“
    TRANSFORM 1: Nettoyage
    â€¢ Anonymisation (supprimer noms/prÃ©noms)
    â€¢ Formats dates cohÃ©rents
    â€¢ Typage correct
    â€¢ DÃ©doublonnage
         â†“
ğŸ“¦ SILVER (MinIO) - DonnÃ©es nettoyÃ©es
â”œâ”€â”€ DonnÃ©es validÃ©es et propres
â””â”€â”€ PrÃªtes pour l'analyse
         â†“
    TRANSFORM 2: ModÃ¨le mÃ©tier
    â€¢ CrÃ©er dimensions (Patient, Temps, Diagnostic...)
    â€¢ CrÃ©er faits (Consultations, DÃ©cÃ¨s...)
    â€¢ Partitionnement (annÃ©e/mois)
    â€¢ Bucketing (optimisation)
         â†“
â­ GOLD (MinIO) - Star Schema
â”œâ”€â”€ dim_temps, dim_patient, dim_diagnostic...
â”œâ”€â”€ fait_consultation (partitionnÃ©)
â””â”€â”€ fait_deces, fait_satisfaction...
         â†“
ğŸ“Š ANALYSE & VISUALISATION
â”œâ”€â”€ Jupyter: RequÃªtes SQL + Graphiques
â”œâ”€â”€ Superset: Dashboards BI
â””â”€â”€ Airflow: Automatisation
```

---

## ğŸ““ NOTEBOOKS Ã€ CRÃ‰ER/MODIFIER

### Notebook 01 : EXTRACT + BRONZE âœ… (Ã€ modifier)

**Ce qu'il doit faire** :
```python
# 1. Lire CSV BRUTS depuis /DATA_2024/
df_deces = spark.read.csv("/home/jovyan/DATA_2024/DECES EN FRANCE/deces.csv")
df_etab = spark.read.csv("/home/jovyan/DATA_2024/Etablissement.../etablissement_sante.csv")
df_satis = spark.read.csv("/home/jovyan/DATA_2024/Satisfaction/.../...")

# 2. Lire PostgreSQL
df_patient = spark.read.jdbc("postgresql://...", "Patient")
df_consult = spark.read.jdbc("postgresql://...", "Consultation")

# 3. Sauvegarder EN BRONZE (donnÃ©es brutes)
df_deces.write.parquet("s3a://lakehouse/bronze/csv/deces")
df_patient.write.parquet("s3a://lakehouse/bronze/postgres/patient")
```

**RÃ©sultat** : MinIO/lakehouse/bronze/ sera REMPLI

---

### Notebook 02 : SILVER (Nettoyage) - NOUVEAU

**Ce qu'il doit faire** :
```python
# 1. Lire depuis BRONZE
df_patient_bronze = spark.read.parquet("s3a://lakehouse/bronze/postgres/Patient")

# 2. ANONYMISER
df_patient_clean = df_patient_bronze.select(
    col("Id_patient"),
    # ANONYMISATION: On supprime nom/prÃ©nom ou on hash
    sha2(col("Nom"), 256).alias("nom_hash"),
    col("Sexe"),
    col("Age"),
    to_date(col("Date"), "M/d/yyyy").alias("date_naissance"),  # FORMAT DATE
    col("Ville"),
    col("Code_postal")
)

# 3. Sauvegarder en SILVER
df_patient_clean.write.parquet("s3a://lakehouse/silver/patient")
```

**RÃ©sultat** : MinIO/lakehouse/silver/ avec donnÃ©es propres

---

### Notebook 03 : GOLD (Star Schema) â­ (DÃ©jÃ  crÃ©Ã©, Ã  adapter)

**Ce qu'il doit faire** :
```python
# 1. Lire depuis SILVER (pas Bronze!)
df_patient = spark.read.parquet("s3a://lakehouse/silver/patient")
df_consult = spark.read.parquet("s3a://lakehouse/silver/consultation")

# 2. CrÃ©er dimensions
dim_patient = df_patient.select("id_patient", "sexe", "age", "ville")

# 3. CrÃ©er faits avec PARTITIONNEMENT
fait_consultation = df_consult \
    .withColumn("annee", year(col("date"))) \
    .withColumn("mois", month(col("date")))

# 4. Sauvegarder avec OPTIMISATIONS
fait_consultation.write \
    .partitionBy("annee", "mois") \  # PARTITION
    .parquet("s3a://lakehouse/gold/fait_consultation")
```

**RÃ©sultat** : MinIO/lakehouse/gold/ avec Star Schema

---

### Notebook 04 : BENCHMARKS (DÃ©jÃ  crÃ©Ã©) âœ…

Pas de changement, il lit depuis Gold et mesure les performances.

---

## ğŸ¯ LIVRABLE 2 - Ce qu'il faut livrer

### 1. Scripts (= Notebooks)
- âœ… Notebook 01: Extract + Bronze
- ğŸ†• Notebook 02: Transform Silver (anonymisation, formats)
- âœ… Notebook 03: Transform Gold (Star Schema + partitionnement)
- âœ… Notebook 04: Benchmarks

### 2. VÃ©rifications
Chaque notebook affiche :
- Nombre de lignes lues
- Nombre de lignes Ã©crites
- AperÃ§u des donnÃ©es

### 3. Partitionnement
- `fait_consultation` partitionnÃ© par annÃ©e/mois
- DÃ©montrÃ© dans Notebook 03

### 4. Graphiques de performance
- GÃ©nÃ©rÃ©s par Notebook 04
- Temps d'exÃ©cution des requÃªtes
- Heatmap des partitions

### 5. Rapport
- Markdown gÃ©nÃ©rÃ© automatiquement par Notebook 04

---

## ğŸš€ LIVRABLE 3 - Ce qu'il faudra faire aprÃ¨s

### 1. Pipelines Airflow (DAGs)

**CrÃ©er un DAG** :
```python
# airflow/dags/chu_etl_pipeline.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

dag = DAG('chu_etl_daily', schedule_interval='@daily')

bronze_task = SparkSubmitOperator(
    task_id='extract_bronze',
    application='/opt/spark-apps/bronze/ingest.py',
    dag=dag
)

silver_task = SparkSubmitOperator(
    task_id='transform_silver',
    application='/opt/spark-apps/silver/clean.py',
    dag=dag
)

gold_task = SparkSubmitOperator(
    task_id='transform_gold',
    application='/opt/spark-apps/gold/star_schema.py',
    dag=dag
)

bronze_task >> silver_task >> gold_task  # Ordre d'exÃ©cution
```

### 2. Dashboards Superset

- Se connecter Ã  PostgreSQL OU Spark SQL
- CrÃ©er des graphiques (consultations par mois, top diagnostics, etc.)
- Assembler en dashboard
- **Storytelling** : Raconter une histoire avec les donnÃ©es

### 3. PrÃ©sentation

- Montrer le pipeline Airflow en action
- Dashboard Superset en live
- Notebooks Jupyter pour analyses exploratoires

---

## âœ… ACTIONS IMMÃ‰DIATES (Pour finir L2)

### Action 1 : Modifier Notebook 01

Faire qu'il lise les **CSV bruts** (pas PostgreSQL pour les CSV) :

```python
# Au lieu de :
df_etab = spark.read.jdbc(..., "Etablissement_sante")

# Faire :
df_etab = spark.read.csv("/home/jovyan/DATA_2024/Etablissement de SANTE/etablissement_sante.csv", sep=";")
```

### Action 2 : CrÃ©er Notebook 02 (Silver)

Avec anonymisation + formats dates

### Action 3 : Adapter Notebook 03

Qu'il lise depuis Silver (pas Bronze)

### Action 4 : ExÃ©cuter tout

01 â†’ 02 â†’ 03 â†’ 04

---

## ğŸ“ POURQUOI CETTE ARCHITECTURE ?

### Pourquoi Bronze/Silver/Gold ?

- **Bronze** : "Zone de quarantaine" - donnÃ©es brutes, au cas oÃ¹
- **Silver** : "Zone de confiance" - donnÃ©es validÃ©es, prÃªtes Ã  Ãªtre utilisÃ©es
- **Gold** : "Zone business" - modÃ¨le optimisÃ© pour les dÃ©cideurs

### Pourquoi pas tout mettre dans PostgreSQL ?

- PostgreSQL = OLTP (transactions rapides, petites requÃªtes)
- MinIO + Parquet = OLAP (analyses sur millions de lignes)
- Parquet = 10x plus compact que CSV
- Spark lit Parquet 100x plus vite que CSV

### Pourquoi Jupyter ET Airflow ?

- **Jupyter** : DÃ©veloppement, tests, prototypage
- **Airflow** : Production, automatisation, monitoring

C'est comme Word vs InDesign :
- Jupyter = Word (brouillon, essais)
- Airflow = InDesign (version finale, automatique)

---

## ğŸ”§ SERVICES - RÃ©sumÃ©

| Service | RÃ´le | Livrable 2 | Livrable 3 |
|---------|------|------------|------------|
| PostgreSQL | Source donnÃ©es | âœ… Essentiel | âœ… Essentiel |
| Spark | Moteur ETL | âœ… Essentiel | âœ… Essentiel |
| MinIO | Stockage Lakehouse | âœ… Essentiel | âœ… Essentiel |
| Jupyter | DÃ©veloppement | âœ… Essentiel | âš ï¸ Optionnel |
| Airflow | Orchestration | âŒ Bonus | âœ… Essentiel |
| Superset | Dashboards BI | âŒ Bonus | âœ… Essentiel |

---

**Maintenant Airflow et Superset sont rÃ©parÃ©s. PrÃªt pour le L3 !**

**Pour finir le L2 : ExÃ©cute les notebooks 01 â†’ 03 â†’ 04 dans Jupyter.**
