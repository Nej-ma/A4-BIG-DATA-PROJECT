# üìò Guide Complet - Projet Big Data CHU

## üéØ Vue d'ensemble du projet

Vous devez r√©aliser un syst√®me d√©cisionnel Big Data pour le groupe hospitalier CHU, avec 3 livrables sur le semestre.

### Les 3 livrables

| Livrable | Contenu | √âtat | Outil principal |
|----------|---------|------|-----------------|
| **Livrable 1** | R√©f√©rentiel de donn√©es (mod√®le conceptuel, architecture) | ‚úÖ **FAIT** | LaTeX |
| **Livrable 2** | Impl√©mentation physique, jobs ETL, optimisation | üöß **EN COURS** | Airflow + Spark |
| **Livrable 3** | Pr√©sentation, dashboards, storytelling | ‚è≥ **√Ä VENIR** | Superset + PowerPoint |

---

## üì¶ Livrable 1 - R√©f√©rentiel de donn√©es (‚úÖ TERMIN√â)

### Ce qui a √©t√© fait

Vous avez d√©j√† r√©alis√© le **Livrable 1** (`Livrable1.tex` compil√© en PDF) contenant :

‚úÖ Architecture ETLT avec zones Bronze/Silver/Gold
‚úÖ Mod√®le en constellation (4 faits + 8 dimensions)
‚úÖ Identification des sources de donn√©es
‚úÖ Description des jobs d'alimentation
‚úÖ Respect RGPD/HDS

**Fichier** : `Livrable1.pdf`

**Note** : Dans votre livrable, vous mentionnez Talend et Cloudera. Avec cette nouvelle stack, vous devez **mettre √† jour** ces r√©f√©rences par :
- Talend ‚Üí **Apache Airflow**
- Cloudera/HDFS ‚Üí **MinIO (S3)**
- Hive ‚Üí **Apache Spark**
- Power BI ‚Üí **Apache Superset** (ou vous pouvez garder Power BI si vous y avez acc√®s)

---

## üîß Livrable 2 - Impl√©mentation physique (üöß EN COURS)

### Objectif

Passer de la conception (Livrable 1) √† l'impl√©mentation r√©elle avec :
1. Jobs ETL fonctionnels
2. Mod√®le physique cr√©√©
3. Donn√©es charg√©es
4. Tests de performance
5. Optimisations (partitionnement, bucketing)

### Votre stack moderne (remplace Talend + Cloudera)

| Fonction | Outil | Port | Acc√®s |
|----------|-------|------|-------|
| Base de donn√©es | PostgreSQL 15 | 5432 | `admin` / `admin123` |
| Interface DB | pgAdmin 4 | 5050 | http://localhost:5050 |
| Data Lake | MinIO (S3) | 9000/9001 | http://localhost:9001 |
| Orchestration ETL | Apache Airflow | 8080 | http://localhost:8080 |
| Traitement distribu√© | Apache Spark | 8081 | http://localhost:8081 |
| Analyse | Jupyter Lab | 8888 | http://localhost:8888 |
| Visualisation | Apache Superset | 8088 | http://localhost:8088 |

### Plan d'action pour le Livrable 2

#### Phase 1 : Pr√©paration (1-2 jours)

**1.1 D√©marrer la stack**
```bash
# Double-clic sur START_BIGDATA_STACK.bat
# Ou :
docker-compose -f docker-compose.bigdata.yml up -d
```

**1.2 Charger les donn√©es dans PostgreSQL**
- Via pgAdmin (http://localhost:5050)
- Importer votre dump `DATA 2024`
- V√©rifier que les tables sont pr√©sentes :
  - `patient` (~100K lignes)
  - `consultation` (~1M lignes)
  - `professionnel` (~1M lignes)
  - `diagnostic` (~15K lignes)
  - `medicaments`
  - `mutuelle`

**1.3 Pr√©parer les fichiers CSV**
- Placer tous les CSV dans `DATA 2024/`
- √âtablissements (FINESS)
- Satisfaction (2014-2020)
- D√©c√®s
- Hospitalisation

#### Phase 2 : D√©veloppement des jobs ETL (3-5 jours)

**2.1 Jobs d'extraction (E)**

Le DAG fourni (`airflow/dags/chu_etl_pipeline.py`) contient d√©j√† :
- `extract_from_postgres` : Extraction PostgreSQL ‚Üí Bronze
- `extract_csv_files` : Extraction CSV ‚Üí Bronze

**Actions** :
1. V√©rifier que les chemins de fichiers sont corrects
2. Tester le DAG dans Airflow
3. V√©rifier que les donn√©es arrivent dans MinIO (bucket `bronze`)

**2.2 Jobs de transformation T1 (Conformit√© RGPD)**

**T√¢ches √† d√©velopper** :

```python
# Dans chu_etl_pipeline.py, compl√©ter :

def transform_t1_patient():
    """
    - Pseudonymiser : nom, prenom, email, telephone, num_secu_social
    - Cr√©er tranches d'√¢ge
    - Supprimer colonnes PII
    """

def transform_t1_consultation():
    """
    - Normaliser dates
    - Valider FK (id_patient, id_professionnel, id_diagnostic)
    """

def transform_t1_etablissement():
    """
    - Nettoyer adresses
    - Valider codes FINESS
    - D√©tecter doublons
    """

def transform_t1_satisfaction():
    """
    - Harmoniser les 27 fichiers CSV (2014-2020)
    - Uniformiser les colonnes
    - Agr√©ger par √©tablissement/ann√©e
    """

def transform_t1_deces():
    """
    - Pseudonymiser identifiants
    - Normaliser localisation (r√©gion/d√©partement)
    """
```

**2.3 Jobs de transformation T2 (Mod√®le dimensionnel)**

**Dimensions √† cr√©er** :

```python
# D√©j√† fourni :
- dim_temps : Calendrier 2014-2025
- dim_patient : √Ä partir de patient_clean (Silver)

# √Ä d√©velopper :
- dim_etablissement : Jointure PostgreSQL + CSV FINESS
- dim_diagnostic : √Ä partir de table diagnostic
- dim_professionnel : √Ä partir de table professionnel
- dim_specialite : R√©f√©rentiel des sp√©cialit√©s m√©dicales
- dim_mutuelle : √Ä partir de table mutuelle
- dim_type_enquete : Typologie des enqu√™tes satisfaction
```

**Tables de faits √† cr√©er** :

```python
# 1. FAIT_CONSULTATION
def create_fait_consultation():
    """
    Jointure :
    - consultation (PostgreSQL)
    - dim_temps
    - dim_patient
    - dim_professionnel
    - dim_diagnostic
    - dim_etablissement
    - dim_mutuelle

    Mesures :
    - nb_consultations (count)
    - duree_consultation_moyenne
    """

# 2. FAIT_HOSPITALISATION
def create_fait_hospitalisation():
    """
    Jointure :
    - hospitalisation (CSV)
    - dim_temps (date_entree, date_sortie)
    - dim_patient
    - dim_etablissement
    - dim_diagnostic

    Mesures :
    - nb_hospitalisations
    - duree_sejour (date_sortie - date_entree)
    - duree_sejour_moyenne
    """

# 3. FAIT_DECES
def create_fait_deces():
    """
    Jointure :
    - deces (CSV)
    - dim_temps (date_deces)
    - dim_patient
    - dim_localisation (r√©gion/d√©partement)

    Mesures :
    - nb_deces
    - age_moyen_deces
    """

# 4. FAIT_SATISFACTION
def create_fait_satisfaction():
    """
    Jointure :
    - satisfaction (CSV agr√©g√©)
    - dim_temps (annee)
    - dim_etablissement
    - dim_type_enquete

    Mesures :
    - note_satisfaction_moyenne
    - nb_reponses
    - taux_satisfaction (note >= 8)
    """
```

#### Phase 3 : Optimisation (2-3 jours)

**3.1 Partitionnement Spark**

Dans `spark/apps/chu_spark_processing.py` :

```python
# Partitionner par ann√©e pour les consultations
df_fait_consultation.write \
    .partitionBy("annee") \
    .mode("overwrite") \
    .parquet("s3a://gold/faits/fait_consultation")

# Partitionner par r√©gion pour les d√©c√®s
df_fait_deces.write \
    .partitionBy("region") \
    .mode("overwrite") \
    .parquet("s3a://gold/faits/fait_deces")
```

**3.2 Bucketing (optionnel)**

```python
# Bucketing sur id_patient pour optimiser les jointures
df.write \
    .bucketBy(50, "id_patient") \
    .sortBy("date_consultation") \
    .saveAsTable("fait_consultation_bucketed")
```

**3.3 Format Parquet vs CSV**

- **Bronze** : CSV (tel quel)
- **Silver** : CSV ou Parquet
- **Gold** : **Parquet** (compression Snappy)

Avantages Parquet :
- Stockage colonnaire (requ√™tes plus rapides)
- Compression efficace (~10x moins d'espace)
- M√©tadonn√©es int√©gr√©es

#### Phase 4 : Tests de performance (1-2 jours)

**4.1 Requ√™tes de test**

Dans Jupyter (`jupyter/notebooks/`), cr√©er un notebook `02_Tests_Performance.ipynb` :

```python
import time
import pandas as pd
from sqlalchemy import create_engine

engine = create_engine('postgresql://admin:admin123@postgres:5432/healthcare_data')

# Test 1 : Taux de consultation par ann√©e
start = time.time()
df = pd.read_sql("""
    SELECT
        EXTRACT(YEAR FROM date_consultation) as annee,
        COUNT(*) as nb_consultations
    FROM consultation
    GROUP BY annee
    ORDER BY annee
""", engine)
duration_1 = time.time() - start
print(f"Requ√™te 1 : {duration_1:.2f}s")

# Test 2 : Taux d'hospitalisation par diagnostic
start = time.time()
df = pd.read_sql("""
    SELECT
        d.libelle_diagnostic,
        COUNT(*) as nb_hospitalisations,
        AVG(h.duree_sejour) as duree_moyenne
    FROM hospitalisation h
    JOIN diagnostic d ON h.id_diagnostic = d.id_diagnostic
    GROUP BY d.libelle_diagnostic
    ORDER BY nb_hospitalisations DESC
    LIMIT 20
""", engine)
duration_2 = time.time() - start
print(f"Requ√™te 2 : {duration_2:.2f}s")

# ... autres requ√™tes
```

**4.2 Comparer avec/sans optimisation**

Mesurer :
- Temps de r√©ponse des requ√™tes
- Taille des fichiers (CSV vs Parquet)
- Utilisation m√©moire Spark

**Graphes de performance** :
```python
import matplotlib.pyplot as plt

tests = ['Req 1', 'Req 2', 'Req 3', 'Req 4']
temps_sans_optim = [12.5, 23.1, 45.3, 18.7]
temps_avec_optim = [2.3, 5.4, 8.1, 3.2]

plt.figure(figsize=(10, 6))
x = range(len(tests))
plt.bar([i-0.2 for i in x], temps_sans_optim, width=0.4, label='Sans optimisation', color='coral')
plt.bar([i+0.2 for i in x], temps_avec_optim, width=0.4, label='Avec optimisation', color='steelblue')
plt.xlabel('Requ√™tes')
plt.ylabel('Temps (secondes)')
plt.title('Impact de l\'optimisation sur les temps de r√©ponse')
plt.xticks(x, tests)
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
```

#### Phase 5 : R√©daction du Livrable 2 (2-3 jours)

**Structure du rapport** :

```
1. Introduction
   - Rappel de l'architecture (Livrable 1)
   - Objectifs du Livrable 2

2. Impl√©mentation des jobs ETL
   2.1 Jobs d'extraction (E)
       - PostgreSQL ‚Üí Bronze
       - CSV ‚Üí Bronze
       - Capture d'√©cran Airflow DAG

   2.2 Jobs de transformation T1 (Conformit√©)
       - Pseudonymisation (code + r√©sultats)
       - Normalisation
       - Validation qualit√©

   2.3 Jobs de transformation T2 (Mod√®le dimensionnel)
       - Cr√©ation des 8 dimensions
       - Cr√©ation des 4 tables de faits
       - Captures d'√©cran MinIO (buckets)

3. Mod√®le physique
   3.1 Sch√©mas des tables (DDL SQL)
   3.2 Volum√©trie r√©elle
   3.3 Organisation du Data Lake (Bronze/Silver/Gold)

4. Optimisations
   4.1 Partitionnement (code + r√©sultats)
   4.2 Bucketing (si impl√©ment√©)
   4.3 Format Parquet (comparaison tailles)

5. Tests de performance
   5.1 Requ√™tes de test
   5.2 Temps de r√©ponse (tableaux + graphes)
   5.3 Analyse des r√©sultats

6. Conclusion
   - Bilan des performances
   - Difficult√©s rencontr√©es
   - Am√©liorations possibles

Annexes :
- Code complet des jobs Airflow
- Scripts Spark
- Requ√™tes SQL de test
```

**√âl√©ments √† inclure** :

üì∏ **Captures d'√©cran** :
- Airflow : DAG en ex√©cution (Graph View)
- MinIO : Buckets bronze/silver/gold avec fichiers
- Spark UI : Jobs en cours
- Jupyter : Notebook avec r√©sultats
- pgAdmin : Tables cr√©√©es

üìä **Graphiques** :
- √âvolution des temps de r√©ponse
- Comparaison taille CSV vs Parquet
- Distribution des donn√©es par partition

üìù **Code** :
- Jobs Airflow complets (annexe)
- Jobs Spark (annexe)
- Requ√™tes SQL de validation

---

## üé® Livrable 3 - Pr√©sentation et storytelling (‚è≥ √Ä VENIR)

### Objectif

Pr√©senter les r√©sultats d'analyse √† travers des dashboards et un storytelling convaincant.

### Pr√©paration

**3.1 Cr√©er les dashboards dans Superset**

Acc√©der √† http://localhost:8088

**Dashboards √† cr√©er** :

1. **Vue d'ensemble CHU**
   - KPI : Total patients, consultations, hospitalisations
   - √âvolution mensuelle des consultations
   - Distribution sexe/√¢ge des patients

2. **Analyse m√©dicale**
   - Top 10 diagnostics
   - Taux d'hospitalisation par diagnostic
   - Dur√©e moyenne de s√©jour par pathologie

3. **Performance √©tablissements**
   - Consultations par √©tablissement
   - Taux de satisfaction par r√©gion
   - Cartographie des √©tablissements

4. **Qualit√© des soins**
   - √âvolution satisfaction 2014-2020
   - Corr√©lation satisfaction / dur√©e s√©jour
   - Indicateurs qualit√©

**3.2 Storytelling**

**Structure de la pr√©sentation (20 min)** :

```
1. Introduction (2 min)
   - Contexte du projet CHU
   - Probl√©matique Big Data dans la sant√©
   - Objectifs du syst√®me d√©cisionnel

2. M√©thodologie (5 min)
   - Architecture ETLT choisie
   - Stack technologique moderne
   - Conformit√© RGPD/HDS

3. Mod√®le d√©cisionnel (3 min)
   - Constellation (4 faits + 8 dimensions)
   - Volum√©trie r√©elle
   - Optimisations mises en place

4. R√©sultats et analyses (7 min)
   ‚≠ê STORYTELLING avec dashboards
   - Insight 1 : "La consultation augmente de 15% chaque ann√©e"
   - Insight 2 : "Les diagnostics cardiaques repr√©sentent 25% des hospitalisations"
   - Insight 3 : "La satisfaction moyenne a progress√© de 12% depuis 2014"
   - Recommandations pour le CHU

5. D√©monstration technique (3 min)
   - Live demo : Pipeline Airflow
   - Live demo : Dashboard interactif Superset
   - Architecture compl√®te

6. Questions/R√©ponses (10 min)
```

**3.3 Support de pr√©sentation**

- PowerPoint ou Google Slides
- Inclure captures d'√©cran des dashboards
- Graphiques clairs et impactants
- Palette de couleurs coh√©rente
- Animations sobres

---

## üìä Correspondance objectifs p√©dagogiques

Votre stack couvre **TOUS** les objectifs :

| Code | Objectif | Solution dans votre stack |
|------|----------|---------------------------|
| [1] | Contraintes RGPD | Pseudonymisation SHA-256 dans T1 |
| [2] | 5V du Big Data | Volume (1M+ consultations), Vari√©t√© (PostgreSQL+CSV), V√©locit√© (Airflow daily), V√©racit√© (validation T1), Valeur (KPIs) |
| [2] | Architecture BigData | Architecture compl√®te fournie (Bronze/Silver/Gold) |
| [2] | Infrastructure distribu√©e | MinIO (stockage S3) + Spark (traitement distribu√©) |
| [2] | Cloud storage | MinIO = S3-compatible (AWS/Azure/GCP) |
| [3] | MapReduce | Spark = MapReduce am√©lior√© (jobs fournis) |
| [2] | Syst√®mes fichiers distribu√©s | MinIO = Object Storage distribu√© |
| [2] | Entreposage de donn√©es | Data Warehouse en zone Gold |
| [3] | Mod√®le dimensionnel | Constellation 4 faits + 8 dimensions |
| [1] | Difficult√©s traitements distribu√©s | Gestion erreurs Airflow, partitionnement Spark |
| [3] | Alimenter Data Mart | Pipeline Airflow complet |
| [3] | Requ√™tes SQL multi-tables | Jupyter + PostgreSQL |
| [3] | Traduire demande en requ√™tes | Requ√™tes m√©tier fournies |
| [3] | ETL | Airflow (remplace Talend) |
| [4] | Qualifier donn√©es | Validation qualit√© en T1 |
| [4] | Strat√©gie d'analyse | M√©thodologie ETLT document√©e |
| [3] | Anonymisation RGPD | Pseudonymisation syst√©matique |
| [3] | Organisation Data Lake | Bronze/Silver/Gold |
| [2] | Types bases NoSQL | MinIO = Object Storage (cl√©-valeur) |
| [2] | ACID vs BASE | PostgreSQL (ACID) vs MinIO (BASE) |
| [3] | Int√©grer donn√©es NoSQL | Airflow ‚Üí MinIO |
| [4] | S√©lectionner stockage | PostgreSQL (OLTP) + MinIO (OLAP) |
| [3] | Exploiter environnement Big Data | Stack compl√®te op√©rationnelle |
| [3] | MapReduce | Spark jobs fournis |
| [4] | Optimiser mod√®le physique | Partitionnement + Parquet |
| [4] | Graphes KPI | Superset + Jupyter (graphiques fournis) |
| [4] | Visualisation UX | Dashboards Superset interactifs |

‚úÖ **100% des objectifs couverts !**

---

## üéØ Planning sugg√©r√©

### Semaine 1-2 : Livrable 1
‚úÖ **FAIT** - Vous l'avez d√©j√† termin√©

### Semaine 3-5 : Livrable 2
- **Semaine 3** : D√©marrage stack + Jobs extraction (E)
- **Semaine 4** : Jobs T1 (conformit√©) + T2 (dimensions)
- **Semaine 5** : Tables de faits + Optimisations + Tests performance

### Semaine 6-7 : Livrable 2 (suite)
- **Semaine 6** : R√©daction rapport + Captures d'√©cran
- **Semaine 7** : Relecture + Finalisation

### Semaine 8-10 : Livrable 3
- **Semaine 8** : Dashboards Superset
- **Semaine 9** : Storytelling + Pr√©sentation PowerPoint
- **Semaine 10** : R√©p√©tition + Ajustements

---

## üõ†Ô∏è Ressources et aide

### Documentation fournie
- `README_STACK.md` : Documentation compl√®te de la stack
- `QUICK_START.md` : Guide de d√©marrage rapide
- `docker-compose.bigdata.yml` : Configuration des services
- `airflow/dags/chu_etl_pipeline.py` : DAG ETL de base
- `spark/apps/chu_spark_processing.py` : Jobs Spark
- `jupyter/notebooks/01_Analyse_Exploratoire_CHU.ipynb` : Analyse exploratoire

### Liens utiles
- **Airflow** : https://airflow.apache.org/docs/
- **Spark** : https://spark.apache.org/docs/latest/
- **MinIO** : https://min.io/docs/
- **Superset** : https://superset.apache.org/docs/intro
- **Pandas** : https://pandas.pydata.org/docs/
- **PySpark** : https://spark.apache.org/docs/latest/api/python/

### Commandes de secours

```bash
# Tout r√©initialiser
docker-compose -f docker-compose.bigdata.yml down -v
docker system prune -a

# Voir les logs
docker-compose -f docker-compose.bigdata.yml logs -f [service]

# Red√©marrer un service
docker-compose -f docker-compose.bigdata.yml restart [service]

# Acc√©der √† PostgreSQL
docker exec -it chu_postgres psql -U admin -d healthcare_data
```

---

## ‚úÖ Checklist finale

### Livrable 2

- [ ] Stack Docker d√©marr√©e et fonctionnelle
- [ ] Donn√©es charg√©es dans PostgreSQL (1M+ consultations)
- [ ] Fichiers CSV accessibles dans `DATA 2024/`
- [ ] DAG Airflow `chu_etl_pipeline` ex√©cut√© avec succ√®s
- [ ] Buckets MinIO bronze/silver/gold remplis
- [ ] 8 dimensions cr√©√©es dans Gold
- [ ] 4 tables de faits cr√©√©es dans Gold
- [ ] Partitionnement Spark impl√©ment√©
- [ ] Tests de performance r√©alis√©s (graphes)
- [ ] Rapport Livrable 2 r√©dig√©
- [ ] Captures d'√©cran incluses
- [ ] Code en annexe
- [ ] Relecture compl√®te

### Livrable 3

- [ ] Dashboards Superset cr√©√©s (4 minimum)
- [ ] KPIs calcul√©s et valid√©s
- [ ] Storytelling r√©dig√©
- [ ] Pr√©sentation PowerPoint cr√©√©e
- [ ] D√©monstration technique pr√©par√©e
- [ ] R√©p√©tition effectu√©e (timing 20 min)
- [ ] Questions/r√©ponses anticip√©es

---

## üéâ Conclusion

Vous avez maintenant :
‚úÖ Un **stack moderne** qui remplace Talend + Cloudera
‚úÖ Une **architecture compl√®te** Big Data op√©rationnelle
‚úÖ Des **outils performants** (Airflow, Spark, Superset)
‚úÖ Une **base solide** pour les livrables 2 et 3
‚úÖ Une **documentation compl√®te** pour vous guider

**N'oubliez pas** :
- Travaillez en √©quipe (3-4 personnes)
- Testez r√©guli√®rement
- Documentez au fur et √† mesure
- Demandez de l'aide si besoin
- Amusez-vous avec les donn√©es ! üòä

**Bonne chance pour votre projet ! üöÄ**

---

**Contacts** :
- √âquipe : Nejma MOUALHI, Brieuc OLIVIERI, Nicolas TAING
- Formation : CESI FISA A4
- Ann√©e : 2025-2026
