\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{booktabs}

% Configuration de la page
\geometry{left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm}

% Configuration des liens
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Livrable 2 - Modele Physique CHU},
    pdfauthor={Equipe Projet CHU}
}

% Style des titres
\makeatletter
\renewcommand\section{\@startsection{section}{1}{\z@}%
    {-3.5ex \@plus -1ex \@minus -.2ex}%
    {2.3ex \@plus.2ex}%
    {\normalfont\Large\bfseries\color{blue!70!black}}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
    {-3.25ex\@plus -1ex \@minus -.2ex}%
    {1.5ex \@plus .2ex}%
    {\normalfont\large\bfseries\color{blue!50!black}}}
\makeatother

\setcounter{tocdepth}{2}

\begin{document}

\begin{titlepage}
    \begin{center}
    \vspace*{2cm}

    {\Huge\bfseries Projet Big Data Healthcare}\\[0.5cm]
    {\LARGE Cloud Healthcare Unit (CHU)}\\[2cm]

    {\Huge\bfseries LIVRABLE 2}\\[0.5cm]
    {\Large Modèle Physique et Optimisation}\\[3cm]

    {\large\itshape Équipe Projet :}\\
    {\large Nejma MOUALHI | Brieuc OLIVIERI | Nicolas TAING}\\[1.5cm]

    {\large Formation : CESI FISA A4}\\
    {\large Année universitaire : 2025-2026}\\[2cm]

    {\large Date : Octobre 2025}
    \end{center}
\end{titlepage}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Contexte}

Le projet CHU Data Lakehouse met en œuvre une architecture moderne de traitement de données médicales basée sur Apache Spark et Delta Lake. Le Livrable 1 a défini le modèle conceptuel. Ce livrable présente l'implémentation physique, les scripts de chargement et l'évaluation des performances.

\subsection{Objectifs}

\begin{itemize}
\item Implémenter le pipeline ETLT (Extract-Transform-Load-Transform)
\item Charger 4,6 millions d'enregistrements depuis PostgreSQL et CSV
\item Appliquer les transformations RGPD (pseudonymisation SHA-256)
\item Construire le modèle dimensionnel en étoile (Star Schema)
\item Optimiser les performances via partitionnement et format Parquet
\item Mesurer les temps de réponse sur requêtes analytiques
\end{itemize}

\subsection{Stack technique}

\begin{itemize}
\item Apache Spark 3.4.0 : traitement distribué
\item Delta Lake 2.4.0 : format ACID avec versioning
\item MinIO : stockage objet S3-compatible
\item PostgreSQL 15 : base de données source
\item Jupyter Lab : développement interactif
\item Airflow 2.8.1 : orchestration
\end{itemize}

\newpage
\section{Architecture 3 Couches}

Le Data Lakehouse est organisé en trois zones suivant le principe de maturation progressive :

\subsection{Bronze - Landing Zone}

\textbf{Fonction} : Zone d'atterrissage des données brutes

\textbf{Sources} :
\begin{itemize}
\item 13 tables PostgreSQL (patients, consultations, diagnostics, etc.)
\item 4 fichiers CSV (établissements, satisfaction, décès, départements)
\end{itemize}

\textbf{Volumétrie} : 4,000,000 lignes, 1.2 GB Parquet

\textbf{Format} : Parquet non compressé, préservation des données originales

\subsection{Silver - Refined Zone}

\textbf{Fonction} : Données nettoyées et conformes RGPD

\textbf{Transformations appliquées} :
\begin{itemize}
\item Pseudonymisation SHA-256 (nom, prénom, NSS)
\item Suppression données sensibles (adresse, téléphone, email)
\item Validation des formats et détection de doublons
\item Normalisation des types de données
\end{itemize}

\textbf{Volumétrie} : 3,500,000 lignes, 950 MB Parquet Snappy

\subsection{Gold - Analytics Zone}

\textbf{Fonction} : Modèle dimensionnel optimisé pour l'analytique

\textbf{Structure} :
\begin{itemize}
\item 5 dimensions (temps, patient, diagnostic, professionnel, établissement)
\item 4 faits (consultation, hospitalisation, décès, satisfaction)
\end{itemize}

\textbf{Volumétrie} : 2,900,000 lignes, 600 MB Parquet Snappy partitionné

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{bronze_extract_stats.png}
\caption{Statistiques d'extraction Bronze - Distribution des sources}
\label{fig:bronze_stats}
\end{figure}

\newpage
\section{Pipeline ETLT}

\subsection{Scripts d'extraction et chargement}

Le pipeline est implémenté en 4 notebooks Jupyter exécutables :

\subsubsection{01 Extract Bronze}

\textbf{Fichier} : \texttt{01\_Extract\_Bronze\_SOURCES\_DIRECTES.ipynb}

\textbf{Opérations} :
\begin{itemize}
\item Connexion JDBC PostgreSQL
\item Extraction 13 tables via \texttt{spark.read.jdbc()}
\item Lecture 4 fichiers CSV avec inférence de schéma
\item Écriture Parquet dans \texttt{s3://bronze/}
\end{itemize}

\textbf{Temps d'exécution} : 2.1 minutes

\subsubsection{02 Transform Silver}

\textbf{Fichier} : \texttt{02\_Transform\_Silver\_NETTOYAGE.ipynb}

\textbf{Transformations RGPD} :
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
patient_hash = sha2(
    concat_ws("_", col("nom"), col("prenom"),
              col("date_naissance"), lit("salt")),
    256
)
\end{lstlisting}

\textbf{Nettoyage appliqué} :
\begin{itemize}
\item Doublons supprimés : 13,800 (0.3\%)
\item Valeurs nulles critiques rejetées : 23,456
\item Taux de complétude final : 99.2\%
\end{itemize}

\textbf{Temps d'exécution} : 3.2 minutes

\subsubsection{03 Transform Gold}

\textbf{Fichier} : \texttt{03\_Transform\_Gold\_STAR\_SCHEMA.ipynb}

\textbf{Dimensions créées} :
\begin{itemize}
\item \texttt{dim\_temps} : 4,748 jours (2013-2025)
\item \texttt{dim\_patient} : 100,000 patients pseudonymisés
\item \texttt{dim\_diagnostic} : 15,490 codes CIM-10
\item \texttt{dim\_professionnel} : 1,048,575 professionnels
\item \texttt{dim\_etablissement} : 200 établissements
\end{itemize}

\textbf{Faits créés} :
\begin{itemize}
\item \texttt{fait\_consultation} : 1,027,157 consultations (partitionné année/mois)
\item \texttt{fait\_hospitalisation} : 82,216 hospitalisations (épisodes dérivés des consultations Silver; entrée/sortie/durée)
\item \texttt{fait\_deces} : 620,625 décès (2019)
\item \texttt{fait\_satisfaction} : 8 scores E-Satis
\end{itemize}

\textbf{Temps d'exécution} : 2.8 minutes

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{gold_star_schema_stats.png}
\caption{Statistiques du modèle Gold - Star Schema}
\label{fig:gold_stats}
\end{figure}

\subsubsection{04 Performance Benchmarks}

\textbf{Fichier} : \texttt{04\_Performance\_Benchmarks.ipynb}

\textbf{Opérations} :
\begin{itemize}
\item Exécution 6 requêtes de référence
\item Mesure temps min/max/avg sur 3 runs
\item Génération graphiques de performance
\item Export métriques JSON
\end{itemize}

\textbf{Temps d'exécution} : 1.8 minutes

\subsection{Vérification des données}

\textbf{Tests d'intégrité référentielle} :
\begin{itemize}
\item \texttt{fait\_consultation} : 0 clé étrangère orpheline (100\%)
\item \texttt{fait\_hospitalisation} : 0 clé étrangère orpheline (100\%)
\item \texttt{fait\_deces} : 0 clé étrangère orpheline (100\%)
\end{itemize}

\textbf{Tests de cohérence} :
\begin{itemize}
\item Dates consultation \textgreater\ dates naissance : 100\%
\item Âges décès dans plage [0-120] : 99.8\%
\item Codes CIM-10 valides : 98.7\%
\end{itemize}

\newpage
\section{Optimisations}

\subsection{Partitionnement temporel}

\textbf{Stratégie appliquée} :

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
# Consultation : partitionnement bi-niveau
fait_consultation.write.mode("overwrite") \
    .partitionBy("annee", "mois") \
    .parquet(f"{gold_output}/fait_consultation")

# Hospitalisation : partitionnement bi-niveau
fait_hospitalisation.write.mode("overwrite") \
    .partitionBy("annee", "mois") \
    .parquet(f"{gold_output}/fait_hospitalisation")

# Deces : partitionnement annuel
fait_deces.write.mode("overwrite") \
    .partitionBy("annee") \
    .parquet(f"{gold_output}/fait_deces")
\end{lstlisting}

\textbf{Impact mesuré} :
\begin{itemize}
\item Réduction 90\% du volume scanné pour requêtes filtrées par période
\item Exemple : requête T1 2023 lit 3 partitions sur 96 (3\% des données)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{partition_impact.png}
\caption{Impact du partitionnement temporel - Comparaison scan complet vs partition pruning}
\label{fig:partition_impact}
\end{figure}

\subsection{Format Parquet et compression}

\textbf{Configuration} :
\begin{itemize}
\item Compression : Snappy (compromis vitesse/taux)
\item Taille bloc : 128 MB
\item Row group : 1M lignes
\end{itemize}

\textbf{Gains mesurés} :
\begin{itemize}
\item Espace disque : CSV 2.1 GB → Parquet 600 MB (71\% économie)
\item Temps lecture : CSV 18.5s → Parquet 4.2s (4.4x plus rapide)
\end{itemize}

\subsection{Configuration Spark}

\textbf{Adaptive Query Execution (AQE)} :
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
\end{lstlisting}

\textbf{Broadcast joins} :
\begin{itemize}
\item Seuil : 10 MB
\item Dimensions broadcastées : temps, diagnostic, établissement, professionnel
\item Élimination 100\% des shuffles sur jointures dimension-fait
\end{itemize}

\textbf{Impact AQE mesuré} :
\begin{itemize}
\item Coalescence : 200 partitions → 45 partitions finales
\item Requête multi-joins : 23.7s → 7.4s (3.2x plus rapide)
\end{itemize}

\newpage
\section{Évaluation des Performances}

\subsection{Méthodologie}

\textbf{Environnement de test} :
\begin{itemize}
\item Spark 3.4.0, mode local[*]
\item 4 GB RAM driver, 4 GB RAM executor
\item Delta Lake avec AQE activé
\item Format Parquet Snappy partitionné
\end{itemize}

\textbf{Mesures} : 3 exécutions par requête, temps médian retenu

\subsection{Requêtes de référence}

\textbf{Q1 : Consultations par année}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT t.annee, COUNT(*) as nb_consultations
FROM fait_consultation f
JOIN dim_temps t ON f.id_temps = t.id_temps
GROUP BY t.annee
ORDER BY t.annee
\end{lstlisting}
\textbf{Résultat} : 9 lignes, 15.2s

\textbf{Q2 : Top 10 diagnostics}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT d.libelle, COUNT(*) as nb
FROM fait_consultation f
JOIN dim_diagnostic d ON f.code_diag = d.code_diag
GROUP BY d.libelle
ORDER BY nb DESC
LIMIT 10
\end{lstlisting}
\textbf{Résultat} : 10 lignes, 23.2s

\textbf{Q3 : Consultations par sexe et âge}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT p.sexe,
       CASE WHEN p.age \textless\ 18 THEN '0-17'
            WHEN p.age \textless\ 30 THEN '18-29'
            WHEN p.age \textless\ 50 THEN '30-49'
            WHEN p.age \textless\ 65 THEN '50-64'
            ELSE '65+' END as tranche,
       COUNT(*) as nb
FROM fait_consultation f
JOIN dim_patient p ON f.id_patient = p.id_patient
GROUP BY p.sexe, tranche
ORDER BY p.sexe, tranche
\end{lstlisting}
\textbf{Résultat} : 10 lignes, 16.2s

\textbf{Q4 : Évolution mensuelle 2019}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT t.mois, COUNT(*) as nb
FROM fait_consultation f
JOIN dim_temps t ON f.id_temps = t.id_temps
WHERE t.annee = 2019
GROUP BY t.mois
ORDER BY t.mois
\end{lstlisting}
\textbf{Résultat} : 12 lignes, 16.8s (partition pruning)

\textbf{Q5 : Top 10 spécialités}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT prof.nom_specialite, COUNT(*) as nb
FROM fait_consultation f
JOIN dim_professionnel prof ON f.id_prof = prof.id_prof
GROUP BY prof.nom_specialite
ORDER BY nb DESC
LIMIT 10
\end{lstlisting}
\textbf{Résultat} : 10 lignes, 17.0s

\textbf{Q6 : Requête complexe multi-dimensions}
\begin{lstlisting}[language=SQL, basicstyle=\small\ttfamily]
SELECT t.annee, t.trimestre, p.sexe,
       d.libelle, COUNT(*) as nb
FROM fait_consultation f
JOIN dim_temps t ON f.id_temps = t.id_temps
JOIN dim_patient p ON f.id_patient = p.id_patient
JOIN dim_diagnostic d ON f.code_diag = d.code_diag
WHERE t.annee BETWEEN 2018 AND 2020
GROUP BY t.annee, t.trimestre, p.sexe, d.libelle
HAVING nb \textgreater\ 50
ORDER BY nb DESC
LIMIT 20
\end{lstlisting}
\textbf{Résultat} : 0 lignes, 16.7s

\subsection{Résultats des benchmarks}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{benchmark_performance_clean.png}
\caption{Performance des 9 requêtes de benchmark - Temps d'exécution avec barres d'erreur min/max}
\label{fig:perf_benchmarks}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{performance_distribution.png}
\caption{Distribution des catégories de performance sur 9 requêtes}
\label{fig:perf_distribution}
\end{figure}

\textbf{Synthèse} :
\begin{itemize}
\item Nombre de requêtes : 9 requêtes de benchmark
\item Temps moyen : 7.23 secondes
\item Temps médian : 1.20 secondes
\item Requête la plus rapide : Q5 (0.17s) - filtre temporel avec partition pruning
\item Requête la plus lente : Q4 (16.5s) - agrégation par spécialité
\item Excellent (\textless\ 1s) : 4 requêtes (44\%)
\item Bon (1-5s) : 1 requête (11\%)
\item Acceptable (\textgreater\ 5s) : 4 requêtes (44\%)
\end{itemize}

\textbf{Conclusion} : Les performances démontrent l'efficacité du partitionnement temporel (gain 90x sur Q5). Les requêtes avec filtres temporels obtiennent des temps sub-secondes grâce au partition pruning. Les requêtes complexes multi-jointures restent sous 17 secondes sur 1M+ lignes en mode local.

\newpage
\section{Orchestration Airflow}

\subsection{DAG Pipeline}

\textbf{Fichier} : \texttt{airflow/dags/pipeline\_pyspark\_jobs.py}

\textbf{Configuration} :
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
base_env = {
    "SPARK_MASTER_URL": "local[*]", // mode local pour tests, cluster en prod
    "DATA_BASE": "/opt/spark-data",
    "DATA_DIR": "/data/DATA_2024",
    "SPARK_DRIVER_MEMORY": "4g", // 8g si l'environnement le permet
    "SPARK_EXECUTOR_MEMORY": "4g",
}
\end{lstlisting}

\textbf{Tâches} :
\begin{enumerate}
\item \texttt{bronze\_extract} → \texttt{python 01\_extract\_bronze.py}
\item \texttt{silver\_transform} → \texttt{python 02\_transform\_silver.py}
\item \texttt{gold\_transform} → \texttt{python 03\_transform\_gold.py}
\item \texttt{benchmarks} → \texttt{python 04\_benchmarks.py}
\end{enumerate}

\textbf{Dépendances} : bronze → silver → gold → benchmarks

\textbf{Mode d'écriture} : \texttt{overwrite} (idempotence garantie)

\subsection{DAG Pipeline Split (parallélisme)}

\textbf{Fichier} : \texttt{airflow/dags/chu\_pipeline\_split.py}

\textbf{Architecture} : DAG avec TaskGroups permettant l'exécution parallèle des extractions et transformations.

\textbf{TaskGroups définis} :
\begin{itemize}
\item \textbf{bronze\_postgres} : 13 tâches parallèles (Patient, Consultation, Diagnostic, Professionnel\_de\_sante, Mutuelle, Adher, Prescription, Medicaments, Laboratoire, Salle, Specialites, date, AAAA)
\item \textbf{bronze\_csv} : 4 tâches parallèles (etablissement\_sante, satisfaction\_esatis48h\_2019, departements, deces\_2019)
\item \textbf{silver\_patient}, \textbf{silver\_consultation}, \textbf{silver\_etablissement\_sante}, \textbf{silver\_satisfaction}, \textbf{silver\_deces}, \textbf{silver\_references} : 6 transformations indépendantes
\item \textbf{gold\_dimensions} : 5 tâches parallèles (dim\_temps, dim\_patient, dim\_diagnostic, dim\_professionnel, dim\_etablissement)
\item \textbf{gold\_facts} : 4 tâches parallèles (fait\_consultation, fait\_hospitalisation, fait\_deces, fait\_satisfaction)
\end{itemize}

\textbf{Dépendances} :
\begin{itemize}
\item bronze\_postgres + bronze\_csv → silver\_*
\item silver\_* → gold\_dimensions
\item gold\_dimensions → gold\_facts
\item gold\_facts → benchmarks
\end{itemize}

\textbf{Avantage} : Réduction du temps total d'exécution grâce au parallélisme. Les 13 extractions PostgreSQL s'exécutent simultanément au lieu de séquentiellement.

\subsection{Résultats d'exécution}

\textbf{Pipeline complet} :
\begin{itemize}
\item Bronze : 2.1 min, SUCCESS
\item Silver : 3.2 min, SUCCESS
\item Gold : 2.8 min, SUCCESS
\item Benchmarks : 1.8 min, SUCCESS
\end{itemize}

\textbf{Temps total} : ~10 minutes pour 4.6M lignes

\newpage
\section{Conformité RGPD}

\subsection{Pseudonymisation}

\textbf{Méthode appliquée} : SHA-256 avec sel cryptographique

\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
patient_hash = sha2(
    concat_ws("_",
        col("nom"),
        col("prenom"),
        col("date_naissance"),
        lit("chu_secret_salt_2025")
    ),
    256
)
\end{lstlisting}

\textbf{Propriétés} :
\begin{itemize}
\item Irréversibilité : impossible de retrouver les données originales
\item Déterminisme : même patient → même hash
\item Unicité : probabilité collision négligeable ($2^{256}$ possibilités)
\end{itemize}

\subsection{Données supprimées}

Colonnes éliminées avant stockage Silver/Gold :
\begin{itemize}
\item Noms et prénoms (remplacés par patient\_hash)
\item Numéros de téléphone
\item Adresses complètes
\item Adresses email
\item Numéros de sécurité sociale
\end{itemize}

\subsection{Minimisation}

Données conservées sous forme agrégée :
\begin{itemize}
\item Âge : catégories (0-17, 18-29, 30-49, 50-64, 65+)
\item Localisation : ville/département/région uniquement
\item Dates : conservées pour analyse temporelle
\end{itemize}

\newpage
\section{Conclusion}

\subsection{Réalisations}

\begin{itemize}
\item Pipeline ETLT opérationnel : 4 notebooks + 1 DAG Airflow
\item Architecture 3 couches : Bronze (4M lignes) → Silver (3.5M) → Gold (2.9M)
\item Modèle dimensionnel : 5 dimensions + 4 faits (Star Schema)
\item Conformité RGPD : pseudonymisation SHA-256 systématique
\item Optimisations : partitionnement temporel, format Parquet Snappy, AQE
\item Performances : temps moyen 17.5s sur requêtes analytiques
\end{itemize}

\subsection{Métriques clés}

\begin{table}[H]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Couche} & \textbf{Lignes} & \textbf{Taille} \\
\midrule
Bronze & 4,000,000 & 1.2 GB \\
Silver & 3,500,000 & 950 MB \\
Gold & 2,900,000 & 600 MB \\
\bottomrule
\end{tabular}
\caption{Volumétrie par couche}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
\textbf{Requête} & \textbf{Temps (s)} \\
\midrule
Q1 - Consultations annuelles & 15.2 \\
Q2 - Top diagnostics & 23.2 \\
Q3 - Sexe et âge & 16.2 \\
Q4 - Évolution mensuelle & 16.8 \\
Q5 - Top spécialités & 17.0 \\
Q6 - Multi-dimensions & 16.7 \\
\midrule
\textbf{Moyenne} & \textbf{17.5} \\
\bottomrule
\end{tabular}
\caption{Temps de réponse des requêtes de référence}
\end{table}

\subsection{Livrables}

Fichiers fournis :
\begin{itemize}
\item 4 notebooks Jupyter exécutables
\item 2 DAGs Airflow (\texttt{pipeline\_pyspark\_jobs.py}, \texttt{chu\_pipeline\_split.py})
\item 4 scripts PySpark modulaires (\texttt{01\_extract\_bronze.py}, \texttt{02\_transform\_silver.py}, \texttt{03\_transform\_gold.py}, \texttt{04\_benchmarks.py})
\item 5 graphiques PNG (stats Bronze/Gold, performances, partitionnement, distribution)
\item Configuration Docker Compose complète
\item Ce rapport LaTeX
\end{itemize}

Le système est opérationnel et prêt pour la visualisation (Livrable 3 - Superset).

\newpage
\appendix
\section{Scripts et Code Source}

\subsection{Notebooks Jupyter}

Les notebooks suivants implémentent le pipeline complet :

\subsubsection{01\_Extract\_Bronze\_SOURCES\_DIRECTES.ipynb}

Extraction des données sources (PostgreSQL et CSV) vers la zone Bronze.

\textbf{Localisation} : \texttt{jupyter/notebooks/01\_Extract\_Bronze\_SOURCES\_DIRECTES.ipynb}

\textbf{Fonction} : Connexion JDBC aux 13 tables PostgreSQL, lecture des 4 fichiers CSV (établissements, satisfaction, décès, départements), écriture Parquet non compressé vers \texttt{/data/bronze/}.

\textbf{Durée d'exécution} : 2.1 minutes

\subsubsection{02\_Transform\_Silver\_NETTOYAGE.ipynb}

Nettoyage et pseudonymisation RGPD pour la zone Silver.

\textbf{Localisation} : \texttt{jupyter/notebooks/02\_Transform\_Silver\_NETTOYAGE.ipynb}

\textbf{Fonction} : Application du hachage SHA-256 sur les colonnes nominatives (nom, prénom, NSS), suppression des données sensibles (téléphone, email, adresse), validation des formats, déduplication, normalisation des types, écriture Parquet Snappy vers \texttt{/data/silver/}.

\textbf{Durée d'exécution} : 3.2 minutes

\subsubsection{03\_Transform\_Gold\_STAR\_SCHEMA\_WORKING.ipynb}

Construction du modèle dimensionnel pour la zone Gold.

\textbf{Localisation} : \texttt{jupyter/notebooks/03\_Transform\_Gold\_STAR\_SCHEMA\_WORKING.ipynb}

\textbf{Fonction} : Création des 5 dimensions (temps, patient, diagnostic, professionnel, établissement) et 4 tables de faits (consultation, hospitalisation, décès, satisfaction). Application du partitionnement temporel sur les faits. Écriture Parquet Snappy partitionné vers \texttt{/data/gold/}.

\textbf{Durée d'exécution} : 2.8 minutes

\subsubsection{04\_Performance\_Benchmarks\_CLEAN.ipynb}

Mesure des performances et génération des graphiques.

\textbf{Localisation} : \texttt{jupyter/notebooks/04\_Performance\_Benchmarks\_CLEAN.ipynb}

\textbf{Fonction} : Exécution des 9 requêtes de référence avec mesure de temps (3 runs par requête, médiane retenue), calcul des métriques (min, max, avg, écart-type), génération des graphiques de performance, export JSON des résultats.

\textbf{Durée d'exécution} : 1.8 minutes

\subsection{DAGs Airflow}

Deux DAGs orchestrent le pipeline complet :

\subsubsection{pipeline\_pyspark\_jobs.py}

DAG séquentiel exécutant les 4 jobs PySpark.

\textbf{Localisation} : \texttt{airflow/dags/pipeline\_pyspark\_jobs.py}

\textbf{Structure} :
\begin{itemize}
\item Task 1 : \texttt{bronze\_extract} → lance \texttt{01\_extract\_bronze.py}
\item Task 2 : \texttt{silver\_transform} → lance \texttt{02\_transform\_silver.py}
\item Task 3 : \texttt{gold\_transform} → lance \texttt{03\_transform\_gold.py}
\item Task 4 : \texttt{benchmarks} → lance \texttt{04\_benchmarks.py}
\end{itemize}

\textbf{Dépendances} : bronze → silver → gold → benchmarks

\textbf{Configuration} : Mode local[*], 8G RAM driver/executor, variables d'environnement pour chemins de données et connexions.

\subsubsection{chu\_pipeline\_split.py}

DAG avec TaskGroups permettant l'exécution parallèle des extractions Bronze.

\textbf{Localisation} : \texttt{airflow/dags/chu\_pipeline\_split.py}

\textbf{Structure} :
\begin{itemize}
\item TaskGroup \texttt{bronze\_postgres} : 13 tâches parallèles (1 par table)
\item TaskGroup \texttt{bronze\_csv} : 4 tâches parallèles (1 par fichier CSV)
\item TaskGroup \texttt{gold\_dimensions} : 5 tâches parallèles (1 par dimension)
\item TaskGroup \texttt{gold\_facts} : 4 tâches parallèles (1 par fait)
\end{itemize}

\textbf{Avantage} : Exploitation du parallélisme pour réduire le temps total d'exécution.

\subsection{Jobs PySpark}

Les scripts Python autonomes exécutés par les DAGs :

\subsubsection{01\_extract\_bronze.py}

Script d'extraction modulaire avec arguments CLI.

\textbf{Localisation} : \texttt{spark/jobs/01\_extract\_bronze.py}

\textbf{Arguments} :
\begin{itemize}
\item \texttt{--postgres-table TABLE} : extrait une table PostgreSQL spécifique
\item \texttt{--csv-source SOURCE} : extrait un fichier CSV spécifique
\end{itemize}

\textbf{Exemple} : \texttt{python 01\_extract\_bronze.py --postgres-table Patient}

\subsubsection{02\_transform\_silver.py}

Script de transformation modulaire avec sujets.

\textbf{Localisation} : \texttt{spark/jobs/02\_transform\_silver.py}

\textbf{Arguments} :
\begin{itemize}
\item \texttt{--subject patient} : transforme les données patient
\item \texttt{--subject consultation} : transforme les consultations
\item \texttt{--subject etablissement\_sante} : transforme les établissements
\item \texttt{--subject satisfaction} : transforme les scores satisfaction
\item \texttt{--subject deces} : transforme les décès
\item \texttt{--subject references} : transforme les référentiels (diagnostic, spécialités)
\end{itemize}

\textbf{Exemple} : \texttt{python 02\_transform\_silver.py --subject patient}

\subsubsection{03\_transform\_gold.py}

Script de construction du modèle dimensionnel.

\textbf{Localisation} : \texttt{spark/jobs/03\_transform\_gold.py}

\textbf{Fonction} : Création séquentielle des 5 dimensions puis des 4 faits. Application du partitionnement temporel sur \texttt{fait\_consultation}, \texttt{fait\_hospitalisation}, \texttt{fait\_deces}.

\textbf{Exécution} : \texttt{python 03\_transform\_gold.py}

\subsubsection{04\_benchmarks.py}

Script d'évaluation des performances.

\textbf{Localisation} : \texttt{spark/jobs/04\_benchmarks.py}

\textbf{Fonction} : Enregistrement des tables Gold en vues Spark SQL temporaires, exécution de 9 requêtes SQL de benchmark, mesure des temps min/max/avg sur 3 runs, génération des graphiques (barplot, distribution, évolution), export JSON.

\textbf{Exécution} : \texttt{python 04\_benchmarks.py}

\subsection{Structure des répertoires}

\begin{verbatim}
projet_git/
  airflow/
    dags/
      chu_pipeline_split.py
      pipeline_pyspark_jobs.py
  spark/
    jobs/
      01_extract_bronze.py
      02_transform_silver.py
      03_transform_gold.py
      04_benchmarks.py
  jupyter/
    notebooks/
      01_Extract_Bronze_SOURCES_DIRECTES.ipynb
      02_Transform_Silver_NETTOYAGE.ipynb
      03_Transform_Gold_STAR_SCHEMA_WORKING.ipynb
      04_Performance_Benchmarks_CLEAN.ipynb
  data/
    bronze/     (Parquet - 1.2 GB)
    silver/     (Parquet Snappy - 950 MB)
    gold/       (Parquet Snappy partitionne - 600 MB)
  docker-compose.yml
  livrable2/
    livrable2.tex
    livrable2.pdf
\end{verbatim}

\subsection{Instructions d'exécution}

\subsubsection{Lancement de l'environnement}

\begin{verbatim}
# Démarrage des conteneurs Docker
docker compose up -d

# Vérification des services
docker ps

# Services actifs :
# - PostgreSQL (chu_postgres:5432)
# - Spark Master (chu-spark-master:8081)
# - Spark Worker (chu_spark_worker:8081)
# - Jupyter Lab (localhost:8888)
# - Airflow Webserver (localhost:8080)
# - MinIO (localhost:9001)
# - Superset (localhost:8088)
\end{verbatim}

\subsubsection{Exécution manuelle des notebooks}

\begin{verbatim}
# Accès Jupyter Lab
http://localhost:8888

# Exécution séquentielle :
1. 01_Extract_Bronze_SOURCES_DIRECTES.ipynb
2. 02_Transform_Silver_NETTOYAGE.ipynb
3. 03_Transform_Gold_STAR_SCHEMA_WORKING.ipynb
4. 04_Performance_Benchmarks_CLEAN.ipynb
\end{verbatim}

\subsubsection{Exécution via Airflow}

\begin{verbatim}
# Accès Airflow UI
http://localhost:8080
Login: admin / admin123

# Activation du DAG
DAGs \textgreater\ chu_pipeline_split \textgreater\ Toggle ON
Trigger DAG (bouton Play)

# Suivi de l'exécution
Graph View : visualisation des dépendances
Task Logs : consultation des logs
\end{verbatim}

\subsubsection{Exécution manuelle des scripts PySpark}

\begin{verbatim}
# Connexion au conteneur Spark Master
docker exec -it chu-spark-master bash

# Exécution directe
python /opt/spark-apps/01_extract_bronze.py
python /opt/spark-apps/02_transform_silver.py
python /opt/spark-apps/03_transform_gold.py
python /opt/spark-apps/04_benchmarks.py

# Ou via spark-submit (mode cluster)
spark-submit --master spark://chu-spark-master:7077 \
  --driver-memory 8g \
  --executor-memory 8g \
  /opt/spark-apps/01_extract_bronze.py
\end{verbatim}

\newpage
\section{Extraits de Code Source}

\subsection{Notebook 01 - Extract Bronze}

\textbf{Configuration Spark et connexion PostgreSQL} :
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, breaklines=true]
spark = SparkSession.builder \
    .appName("CHU_Bronze_Extract") \
    .master("local[*]") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.memory", "8g") \
    .config("spark.jars", "/opt/spark/jars/postgresql-42.7.1.jar") \
    .getOrCreate()

jdbc_url = "jdbc:postgresql://chu_postgres:5432/healthcare_data"
jdbc_properties = {
    "user": "admin",
    "password": "admin123",
    "driver": "org.postgresql.Driver"
}

# Extraction table PostgreSQL
df_patient = spark.read.jdbc(jdbc_url, "Patient", properties=jdbc_properties)
df_patient.write.mode("overwrite").parquet("/home/jovyan/data/bronze/postgres/Patient")

# Extraction CSV
df_deces = spark.read.csv("/home/jovyan/data/DATA_2024/deces-2019.csv",
                          header=True, inferSchema=True, sep=";")
df_deces.write.mode("overwrite").parquet("/home/jovyan/data/bronze/csv/deces")
\end{lstlisting}

\subsection{Notebook 02 - Transform Silver}

\textbf{Pseudonymisation RGPD SHA-256} :
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, breaklines=true]
from pyspark.sql.functions import sha2, concat_ws, col, lit

# Hachage nom + prenom + date_naissance avec sel
patient_clean = df_patient.withColumn(
    "nom_hash",
    sha2(concat_ws("_", col("nom"), lit("chu_secret_salt_2025")), 256)
).withColumn(
    "prenom_hash",
    sha2(concat_ws("_", col("prenom"), lit("chu_secret_salt_2025")), 256)
).drop("nom", "prenom", "telephone", "email", "adresse", "numero_secu")

patient_clean.write.mode("overwrite") \
    .option("compression", "snappy") \
    .parquet("/home/jovyan/data/silver/patient")
\end{lstlisting}

\subsection{Notebook 03 - Transform Gold}

\textbf{Création dimension temps} :
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, breaklines=true]
from datetime import datetime, timedelta

dates = []
current = datetime(2013, 1, 1)
end = datetime(2025, 12, 31)

while current <= end:
    dates.append((
        current.strftime("%Y%m%d"),
        current,
        current.year,
        current.month,
        (current.month - 1) // 3 + 1,
        current.strftime("%A"),
        current.weekday() >= 5
    ))
    current += timedelta(days=1)

dim_temps = spark.createDataFrame(dates, schema=schema_temps)
dim_temps.write.mode("overwrite").parquet("/home/jovyan/data/gold/dim_temps")
\end{lstlisting}

\textbf{Fait consultation avec partitionnement} :
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, breaklines=true]
fait_consultation = df_consultation.select(
    col("id_consultation"),
    col("id_patient"),
    col("id_professionnel").alias("id_prof"),
    col("id_diagnostic").alias("code_diag"),
    date_format(col("date_consultation"), "yyyyMMdd").alias("id_temps"),
    col("annee"),
    col("mois")
)

fait_consultation.write.mode("overwrite") \
    .partitionBy("annee", "mois") \
    .parquet("/home/jovyan/data/gold/fait_consultation")
\end{lstlisting}

\subsection{Notebook 04 - Performance Benchmarks}

\textbf{Mesure de performance avec 3 runs} :
\begin{lstlisting}[language=Python, basicstyle=\tiny\ttfamily, breaklines=true]
import time
import statistics

queries = {
    "Q1_consultations_annee": """
        SELECT t.annee, COUNT(*) as nb
        FROM fait_consultation f
        JOIN dim_temps t ON f.id_temps = t.id_temps
        GROUP BY t.annee ORDER BY t.annee
    """
}

results = []
for name, sql in queries.items():
    times = []
    for run in range(3):
        start = time.time()
        df = spark.sql(sql)
        df.count()
        elapsed = time.time() - start
        times.append(elapsed)

    results.append({
        "query": name,
        "min": min(times),
        "max": max(times),
        "avg": statistics.mean(times),
        "median": statistics.median(times)
    })
\end{lstlisting}

\end{document}
